{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "loading annotations into memory...\n",
      "Done (t=0.33s)\n",
      "creating index...\n",
      "index created!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[Epoch 0/5] [Batch 0/988] [loss: 0.002188]\n",
      "[Epoch 0/5] [Batch 1/988] [loss: 0.002053]\n",
      "[Epoch 0/5] [Batch 2/988] [loss: 0.002039]\n",
      "[Epoch 0/5] [Batch 3/988] [loss: 0.002236]\n",
      "[Epoch 0/5] [Batch 4/988] [loss: 0.002075]\n",
      "[Epoch 0/5] [Batch 5/988] [loss: 0.002102]\n",
      "[Epoch 0/5] [Batch 6/988] [loss: 0.002259]\n",
      "[Epoch 0/5] [Batch 7/988] [loss: 0.002200]\n",
      "[Epoch 0/5] [Batch 8/988] [loss: 0.002199]\n",
      "[Epoch 0/5] [Batch 9/988] [loss: 0.001944]\n",
      "[Epoch 0/5] [Batch 10/988] [loss: 0.002274]\n",
      "[Epoch 0/5] [Batch 11/988] [loss: 0.002102]\n",
      "[Epoch 0/5] [Batch 12/988] [loss: 0.002003]\n",
      "[Epoch 0/5] [Batch 13/988] [loss: 0.001908]\n",
      "[Epoch 0/5] [Batch 14/988] [loss: 0.002443]\n",
      "[Epoch 0/5] [Batch 15/988] [loss: 0.002176]\n",
      "[Epoch 0/5] [Batch 16/988] [loss: 0.002230]\n",
      "[Epoch 0/5] [Batch 17/988] [loss: 0.002096]\n",
      "[Epoch 0/5] [Batch 18/988] [loss: 0.002245]\n",
      "[Epoch 0/5] [Batch 19/988] [loss: 0.002075]\n",
      "[Epoch 0/5] [Batch 20/988] [loss: 0.002006]\n",
      "[Epoch 0/5] [Batch 21/988] [loss: 0.002175]\n",
      "[Epoch 0/5] [Batch 22/988] [loss: 0.002078]\n",
      "[Epoch 0/5] [Batch 23/988] [loss: 0.002243]\n",
      "[Epoch 0/5] [Batch 24/988] [loss: 0.002016]\n",
      "[Epoch 0/5] [Batch 25/988] [loss: 0.002340]\n",
      "[Epoch 0/5] [Batch 26/988] [loss: 0.002217]\n",
      "[Epoch 0/5] [Batch 27/988] [loss: 0.002185]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_loop_conv import *\n",
    "\n",
    "model = torch.load('model_state.pt')\n",
    "model.eval()\n",
    "\n",
    "#After first round of training model made it to epoch 1 iteration ~800 loss ~.002\n",
    "\n",
    "\n",
    "train_model(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CIFAR10', 'CIFAR100', 'Cityscapes', 'CocoCaptions', 'CocoDetection', 'DatasetFolder', 'EMNIST', 'FakeData', 'FashionMNIST', 'Flickr30k', 'Flickr8k', 'ImageFolder', 'KMNIST', 'LSUN', 'LSUNClass', 'MNIST', 'Omniglot', 'PhotoTour', 'SBU', 'SEMEION', 'STL10', 'SVHN', 'VOCDetection', 'VOCSegmentation', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'cifar', 'cityscapes', 'coco', 'fakedata', 'flickr', 'folder', 'lsun', 'mnist', 'omniglot', 'phototour', 'sbu', 'semeion', 'stl10', 'svhn', 'utils', 'voc']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.append('../dataprocessing')\n",
    "from dataloader import BlurDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F0C70666DA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F0C70666710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path='~/cs231n-proj/data/cifar'\n",
    "\n",
    "data = BlurDataset.from_single_dataset(path)\n",
    "blurred, tgt = data.train[0]\n",
    "display(blurred)\n",
    "display(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-92391269448a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_state'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "model = torch.load('model_state')\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.31s)\n",
      "creating index...\n",
      "index created!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "train [Epoch 0/10] [Batch 0/1173] [Iter 0] [G_Loss 0.493833] [D_Loss 2.203641]\n",
      "train [Epoch 0/10] [Batch 1/1173] [Iter 1] [G_Loss 674.883240] [D_Loss 125.520073]\n",
      "train [Epoch 0/10] [Batch 2/1173] [Iter 2] [G_Loss 301.344299] [D_Loss 46.836433]\n",
      "train [Epoch 0/10] [Batch 3/1173] [Iter 3] [G_Loss 69.886955] [D_Loss 0.052057]\n",
      "train [Epoch 0/10] [Batch 4/1173] [Iter 4] [G_Loss 0.510172] [D_Loss 16.354734]\n",
      "train [Epoch 0/10] [Batch 5/1173] [Iter 5] [G_Loss 113.438980] [D_Loss 2.373063]\n",
      "train [Epoch 0/10] [Batch 6/1173] [Iter 6] [G_Loss 134.948532] [D_Loss 5.490053]\n",
      "train [Epoch 0/10] [Batch 7/1173] [Iter 7] [G_Loss 114.567993] [D_Loss 0.466100]\n",
      "train [Epoch 0/10] [Batch 8/1173] [Iter 8] [G_Loss 88.668175] [D_Loss 0.152591]\n",
      "train [Epoch 0/10] [Batch 9/1173] [Iter 9] [G_Loss 74.638626] [D_Loss 0.009849]\n",
      "train [Epoch 0/10] [Batch 10/1173] [Iter 10] [G_Loss 66.055412] [D_Loss 0.001357]\n",
      "train [Epoch 0/10] [Batch 11/1173] [Iter 11] [G_Loss 54.559677] [D_Loss 0.000671]\n",
      "train [Epoch 0/10] [Batch 12/1173] [Iter 12] [G_Loss 49.660896] [D_Loss 0.000611]\n",
      "train [Epoch 0/10] [Batch 13/1173] [Iter 13] [G_Loss 42.606277] [D_Loss 0.001275]\n",
      "train [Epoch 0/10] [Batch 14/1173] [Iter 14] [G_Loss 36.368149] [D_Loss 0.000927]\n",
      "train [Epoch 0/10] [Batch 15/1173] [Iter 15] [G_Loss 29.939405] [D_Loss 0.194802]\n",
      "train [Epoch 0/10] [Batch 16/1173] [Iter 16] [G_Loss 28.472900] [D_Loss 0.395836]\n",
      "train [Epoch 0/10] [Batch 17/1173] [Iter 17] [G_Loss 36.078133] [D_Loss 0.005085]\n",
      "train [Epoch 0/10] [Batch 18/1173] [Iter 18] [G_Loss 40.660179] [D_Loss 0.011676]\n",
      "train [Epoch 0/10] [Batch 19/1173] [Iter 19] [G_Loss 39.008965] [D_Loss 0.009847]\n",
      "train [Epoch 0/10] [Batch 20/1173] [Iter 20] [G_Loss 35.704945] [D_Loss 0.030245]\n",
      "train [Epoch 0/10] [Batch 21/1173] [Iter 21] [G_Loss 31.137524] [D_Loss 0.109420]\n",
      "train [Epoch 0/10] [Batch 22/1173] [Iter 22] [G_Loss 28.655024] [D_Loss 0.144223]\n",
      "train [Epoch 0/10] [Batch 23/1173] [Iter 23] [G_Loss 29.362431] [D_Loss 0.095637]\n",
      "train [Epoch 0/10] [Batch 24/1173] [Iter 24] [G_Loss 32.539139] [D_Loss 0.022792]\n",
      "train [Epoch 0/10] [Batch 28/1173] [Iter 28] [G_Loss 34.021927] [D_Loss 0.045531]\n",
      "train [Epoch 0/10] [Batch 29/1173] [Iter 29] [G_Loss 26.682611] [D_Loss 0.066918]\n",
      "train [Epoch 0/10] [Batch 30/1173] [Iter 30] [G_Loss 24.352364] [D_Loss 0.064115]\n",
      "train [Epoch 0/10] [Batch 31/1173] [Iter 31] [G_Loss 29.502714] [D_Loss 0.013755]\n",
      "train [Epoch 0/10] [Batch 32/1173] [Iter 32] [G_Loss 25.093300] [D_Loss 0.137542]\n",
      "train [Epoch 0/10] [Batch 33/1173] [Iter 33] [G_Loss 34.945335] [D_Loss 0.061793]\n",
      "train [Epoch 0/10] [Batch 34/1173] [Iter 34] [G_Loss 35.885639] [D_Loss 0.133826]\n",
      "train [Epoch 0/10] [Batch 35/1173] [Iter 35] [G_Loss 31.989681] [D_Loss 0.014515]\n",
      "train [Epoch 0/10] [Batch 36/1173] [Iter 36] [G_Loss 27.577915] [D_Loss 0.003049]\n",
      "train [Epoch 0/10] [Batch 37/1173] [Iter 37] [G_Loss 25.722031] [D_Loss 0.024684]\n",
      "train [Epoch 0/10] [Batch 38/1173] [Iter 38] [G_Loss 23.078606] [D_Loss 0.015526]\n",
      "train [Epoch 0/10] [Batch 39/1173] [Iter 39] [G_Loss 18.107021] [D_Loss 0.078417]\n",
      "train [Epoch 0/10] [Batch 40/1173] [Iter 40] [G_Loss 28.436308] [D_Loss 0.056261]\n",
      "train [Epoch 0/10] [Batch 41/1173] [Iter 41] [G_Loss 29.550442] [D_Loss 0.003768]\n",
      "train [Epoch 0/10] [Batch 42/1173] [Iter 42] [G_Loss 26.042814] [D_Loss 0.041510]\n",
      "train [Epoch 0/10] [Batch 43/1173] [Iter 43] [G_Loss 27.387913] [D_Loss 0.000886]\n",
      "train [Epoch 0/10] [Batch 44/1173] [Iter 44] [G_Loss 22.257515] [D_Loss 0.001380]\n",
      "train [Epoch 0/10] [Batch 45/1173] [Iter 45] [G_Loss 18.639555] [D_Loss 0.026396]\n",
      "train [Epoch 0/10] [Batch 46/1173] [Iter 46] [G_Loss 24.987617] [D_Loss 0.033011]\n",
      "train [Epoch 0/10] [Batch 47/1173] [Iter 47] [G_Loss 24.382910] [D_Loss 0.007350]\n",
      "train [Epoch 0/10] [Batch 48/1173] [Iter 48] [G_Loss 23.873701] [D_Loss 0.002454]\n",
      "train [Epoch 0/10] [Batch 49/1173] [Iter 49] [G_Loss 18.070724] [D_Loss 0.009465]\n",
      "train [Epoch 0/10] [Batch 50/1173] [Iter 50] [G_Loss 19.772228] [D_Loss 0.003191]\n",
      "train [Epoch 0/10] [Batch 51/1173] [Iter 51] [G_Loss 21.349613] [D_Loss 0.005034]\n",
      "train [Epoch 0/10] [Batch 52/1173] [Iter 52] [G_Loss 19.993479] [D_Loss 0.017909]\n",
      "train [Epoch 0/10] [Batch 53/1173] [Iter 53] [G_Loss 18.164583] [D_Loss 0.106479]\n",
      "train [Epoch 0/10] [Batch 54/1173] [Iter 54] [G_Loss 16.030895] [D_Loss 0.003659]\n",
      "train [Epoch 0/10] [Batch 55/1173] [Iter 55] [G_Loss 13.778596] [D_Loss 0.015373]\n",
      "train [Epoch 0/10] [Batch 56/1173] [Iter 56] [G_Loss 20.206188] [D_Loss 0.000832]\n",
      "train [Epoch 0/10] [Batch 57/1173] [Iter 57] [G_Loss 20.257839] [D_Loss 0.148618]\n",
      "train [Epoch 0/10] [Batch 58/1173] [Iter 58] [G_Loss 16.247236] [D_Loss 0.001240]\n",
      "train [Epoch 0/10] [Batch 59/1173] [Iter 59] [G_Loss 13.493039] [D_Loss 0.003348]\n",
      "train [Epoch 0/10] [Batch 60/1173] [Iter 60] [G_Loss 14.656669] [D_Loss 0.003408]\n",
      "train [Epoch 0/10] [Batch 61/1173] [Iter 61] [G_Loss 17.072779] [D_Loss 0.000207]\n",
      "train [Epoch 0/10] [Batch 62/1173] [Iter 62] [G_Loss 16.065317] [D_Loss 0.003428]\n",
      "train [Epoch 0/10] [Batch 63/1173] [Iter 63] [G_Loss 15.763718] [D_Loss 0.000872]\n",
      "train [Epoch 0/10] [Batch 64/1173] [Iter 64] [G_Loss 17.874569] [D_Loss 0.032297]\n",
      "train [Epoch 0/10] [Batch 65/1173] [Iter 65] [G_Loss 13.217429] [D_Loss 0.032919]\n",
      "train [Epoch 0/10] [Batch 66/1173] [Iter 66] [G_Loss 11.362195] [D_Loss 0.026790]\n",
      "train [Epoch 0/10] [Batch 67/1173] [Iter 67] [G_Loss 22.966343] [D_Loss 0.040338]\n",
      "train [Epoch 0/10] [Batch 68/1173] [Iter 68] [G_Loss 25.222891] [D_Loss 0.008066]\n",
      "train [Epoch 0/10] [Batch 69/1173] [Iter 69] [G_Loss 25.036684] [D_Loss 0.025094]\n",
      "train [Epoch 0/10] [Batch 70/1173] [Iter 70] [G_Loss 21.496077] [D_Loss 0.000153]\n",
      "train [Epoch 0/10] [Batch 71/1173] [Iter 71] [G_Loss 20.432549] [D_Loss 0.004430]\n",
      "train [Epoch 0/10] [Batch 72/1173] [Iter 72] [G_Loss 18.047041] [D_Loss 0.000236]\n",
      "train [Epoch 0/10] [Batch 73/1173] [Iter 73] [G_Loss 16.177620] [D_Loss 0.002426]\n",
      "train [Epoch 0/10] [Batch 74/1173] [Iter 74] [G_Loss 14.616003] [D_Loss 0.000393]\n",
      "train [Epoch 0/10] [Batch 75/1173] [Iter 75] [G_Loss 14.968057] [D_Loss 0.003701]\n",
      "train [Epoch 0/10] [Batch 76/1173] [Iter 76] [G_Loss 14.490449] [D_Loss 0.000301]\n",
      "train [Epoch 0/10] [Batch 77/1173] [Iter 77] [G_Loss 15.657353] [D_Loss 0.000157]\n",
      "train [Epoch 0/10] [Batch 78/1173] [Iter 78] [G_Loss 12.047318] [D_Loss 0.036272]\n",
      "train [Epoch 0/10] [Batch 79/1173] [Iter 79] [G_Loss 9.778085] [D_Loss 0.014007]\n",
      "train [Epoch 0/10] [Batch 80/1173] [Iter 80] [G_Loss 14.650275] [D_Loss 0.000534]\n",
      "train [Epoch 0/10] [Batch 81/1173] [Iter 81] [G_Loss 14.464670] [D_Loss 0.000640]\n",
      "train [Epoch 0/10] [Batch 82/1173] [Iter 82] [G_Loss 14.118337] [D_Loss 0.000740]\n",
      "train [Epoch 0/10] [Batch 83/1173] [Iter 83] [G_Loss 17.569084] [D_Loss 0.000047]\n",
      "train [Epoch 0/10] [Batch 84/1173] [Iter 84] [G_Loss 18.577702] [D_Loss 0.000181]\n",
      "train [Epoch 0/10] [Batch 85/1173] [Iter 85] [G_Loss 15.094489] [D_Loss 0.002835]\n",
      "train [Epoch 0/10] [Batch 86/1173] [Iter 86] [G_Loss 13.071340] [D_Loss 0.003116]\n",
      "train [Epoch 0/10] [Batch 87/1173] [Iter 87] [G_Loss 12.351211] [D_Loss 0.004100]\n",
      "train [Epoch 0/10] [Batch 88/1173] [Iter 88] [G_Loss 11.266454] [D_Loss 0.039223]\n",
      "train [Epoch 0/10] [Batch 89/1173] [Iter 89] [G_Loss 10.516628] [D_Loss 0.013534]\n",
      "train [Epoch 0/10] [Batch 90/1173] [Iter 90] [G_Loss 13.629787] [D_Loss 0.008700]\n",
      "train [Epoch 0/10] [Batch 91/1173] [Iter 91] [G_Loss 14.345196] [D_Loss 0.000948]\n",
      "train [Epoch 0/10] [Batch 92/1173] [Iter 92] [G_Loss 13.522466] [D_Loss 0.000323]\n",
      "train [Epoch 0/10] [Batch 93/1173] [Iter 93] [G_Loss 14.671781] [D_Loss 0.001108]\n",
      "train [Epoch 0/10] [Batch 94/1173] [Iter 94] [G_Loss 17.679117] [D_Loss 0.001330]\n",
      "train [Epoch 0/10] [Batch 95/1173] [Iter 95] [G_Loss 12.952890] [D_Loss 0.000248]\n",
      "train [Epoch 0/10] [Batch 96/1173] [Iter 96] [G_Loss 12.541500] [D_Loss 0.000717]\n",
      "train [Epoch 0/10] [Batch 97/1173] [Iter 97] [G_Loss 11.131610] [D_Loss 0.050323]\n",
      "train [Epoch 0/10] [Batch 98/1173] [Iter 98] [G_Loss 23.852516] [D_Loss 0.075292]\n",
      "train [Epoch 0/10] [Batch 99/1173] [Iter 99] [G_Loss 25.773151] [D_Loss 0.158360]\n",
      "train [Epoch 0/10] [Batch 100/1173] [Iter 100] [G_Loss 19.147488] [D_Loss 0.011909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 101/1173] [Iter 101] [G_Loss 16.375139] [D_Loss 0.002723]\n",
      "train [Epoch 0/10] [Batch 102/1173] [Iter 102] [G_Loss 14.346645] [D_Loss 0.001791]\n",
      "train [Epoch 0/10] [Batch 103/1173] [Iter 103] [G_Loss 12.451850] [D_Loss 0.008482]\n",
      "train [Epoch 0/10] [Batch 104/1173] [Iter 104] [G_Loss 14.753071] [D_Loss 0.002745]\n",
      "train [Epoch 0/10] [Batch 105/1173] [Iter 105] [G_Loss 14.870546] [D_Loss 0.001495]\n",
      "train [Epoch 0/10] [Batch 106/1173] [Iter 106] [G_Loss 15.203244] [D_Loss 0.003486]\n",
      "train [Epoch 0/10] [Batch 107/1173] [Iter 107] [G_Loss 14.650886] [D_Loss 0.076077]\n",
      "train [Epoch 0/10] [Batch 108/1173] [Iter 108] [G_Loss 11.966388] [D_Loss 0.098818]\n",
      "train [Epoch 0/10] [Batch 109/1173] [Iter 109] [G_Loss 18.054235] [D_Loss 0.042267]\n",
      "train [Epoch 0/10] [Batch 110/1173] [Iter 110] [G_Loss 18.117941] [D_Loss 0.020115]\n",
      "train [Epoch 0/10] [Batch 111/1173] [Iter 111] [G_Loss 14.929398] [D_Loss 0.041182]\n",
      "train [Epoch 0/10] [Batch 112/1173] [Iter 112] [G_Loss 11.562938] [D_Loss 0.032316]\n",
      "train [Epoch 0/10] [Batch 113/1173] [Iter 113] [G_Loss 7.095781] [D_Loss 0.054444]\n",
      "train [Epoch 0/10] [Batch 114/1173] [Iter 114] [G_Loss 9.650575] [D_Loss 0.002023]\n",
      "train [Epoch 0/10] [Batch 115/1173] [Iter 115] [G_Loss 10.173660] [D_Loss 0.005581]\n",
      "train [Epoch 0/10] [Batch 116/1173] [Iter 116] [G_Loss 10.939243] [D_Loss 0.009973]\n",
      "train [Epoch 0/10] [Batch 117/1173] [Iter 117] [G_Loss 11.572419] [D_Loss 0.000350]\n",
      "train [Epoch 0/10] [Batch 118/1173] [Iter 118] [G_Loss 11.049182] [D_Loss 0.011739]\n",
      "train [Epoch 0/10] [Batch 119/1173] [Iter 119] [G_Loss 11.329189] [D_Loss 0.001319]\n",
      "train [Epoch 0/10] [Batch 120/1173] [Iter 120] [G_Loss 9.751713] [D_Loss 0.003699]\n",
      "train [Epoch 0/10] [Batch 121/1173] [Iter 121] [G_Loss 9.929332] [D_Loss 0.001983]\n",
      "train [Epoch 0/10] [Batch 122/1173] [Iter 122] [G_Loss 9.019070] [D_Loss 0.004732]\n",
      "train [Epoch 0/10] [Batch 123/1173] [Iter 123] [G_Loss 9.140446] [D_Loss 0.005532]\n",
      "train [Epoch 0/10] [Batch 124/1173] [Iter 124] [G_Loss 11.251077] [D_Loss 0.006005]\n",
      "train [Epoch 0/10] [Batch 125/1173] [Iter 125] [G_Loss 10.323828] [D_Loss 0.001198]\n",
      "train [Epoch 0/10] [Batch 126/1173] [Iter 126] [G_Loss 8.547989] [D_Loss 0.002235]\n",
      "train [Epoch 0/10] [Batch 127/1173] [Iter 127] [G_Loss 8.954485] [D_Loss 0.003243]\n",
      "train [Epoch 0/10] [Batch 128/1173] [Iter 128] [G_Loss 9.470411] [D_Loss 0.002493]\n",
      "train [Epoch 0/10] [Batch 129/1173] [Iter 129] [G_Loss 9.441149] [D_Loss 0.002052]\n",
      "train [Epoch 0/10] [Batch 130/1173] [Iter 130] [G_Loss 10.538243] [D_Loss 0.001739]\n",
      "train [Epoch 0/10] [Batch 131/1173] [Iter 131] [G_Loss 9.722578] [D_Loss 0.001767]\n",
      "train [Epoch 0/10] [Batch 132/1173] [Iter 132] [G_Loss 11.558187] [D_Loss 0.057035]\n",
      "train [Epoch 0/10] [Batch 133/1173] [Iter 133] [G_Loss 8.720489] [D_Loss 0.018122]\n",
      "train [Epoch 0/10] [Batch 134/1173] [Iter 134] [G_Loss 4.587414] [D_Loss 0.122100]\n",
      "train [Epoch 0/10] [Batch 135/1173] [Iter 135] [G_Loss 23.354462] [D_Loss 0.129157]\n",
      "train [Epoch 0/10] [Batch 136/1173] [Iter 136] [G_Loss 23.544418] [D_Loss 0.025291]\n",
      "train [Epoch 0/10] [Batch 137/1173] [Iter 137] [G_Loss 19.900881] [D_Loss 0.053758]\n",
      "train [Epoch 0/10] [Batch 138/1173] [Iter 138] [G_Loss 17.547894] [D_Loss 0.000161]\n",
      "train [Epoch 0/10] [Batch 139/1173] [Iter 139] [G_Loss 15.983893] [D_Loss 0.000279]\n",
      "train [Epoch 0/10] [Batch 140/1173] [Iter 140] [G_Loss 14.920711] [D_Loss 0.000896]\n",
      "train [Epoch 0/10] [Batch 141/1173] [Iter 141] [G_Loss 14.783335] [D_Loss 0.001013]\n",
      "train [Epoch 0/10] [Batch 142/1173] [Iter 142] [G_Loss 13.951438] [D_Loss 0.002031]\n",
      "train [Epoch 0/10] [Batch 143/1173] [Iter 143] [G_Loss 12.710536] [D_Loss 0.140441]\n",
      "train [Epoch 0/10] [Batch 144/1173] [Iter 144] [G_Loss 27.626455] [D_Loss 0.048194]\n",
      "train [Epoch 0/10] [Batch 145/1173] [Iter 145] [G_Loss 24.766972] [D_Loss 0.018040]\n",
      "train [Epoch 0/10] [Batch 146/1173] [Iter 146] [G_Loss 22.849161] [D_Loss 0.180218]\n",
      "train [Epoch 0/10] [Batch 147/1173] [Iter 147] [G_Loss 14.725278] [D_Loss 0.009956]\n",
      "train [Epoch 0/10] [Batch 148/1173] [Iter 148] [G_Loss 15.547597] [D_Loss 0.000048]\n",
      "train [Epoch 0/10] [Batch 149/1173] [Iter 149] [G_Loss 13.307736] [D_Loss 0.000444]\n",
      "train [Epoch 0/10] [Batch 150/1173] [Iter 150] [G_Loss 8.689032] [D_Loss 0.001370]\n",
      "train [Epoch 0/10] [Batch 151/1173] [Iter 151] [G_Loss 4.541499] [D_Loss 0.251056]\n",
      "train [Epoch 0/10] [Batch 152/1173] [Iter 152] [G_Loss 36.671566] [D_Loss 0.782954]\n",
      "train [Epoch 0/10] [Batch 153/1173] [Iter 153] [G_Loss 23.469625] [D_Loss 0.017858]\n",
      "train [Epoch 0/10] [Batch 154/1173] [Iter 154] [G_Loss 14.384545] [D_Loss 0.000218]\n",
      "train [Epoch 0/10] [Batch 155/1173] [Iter 155] [G_Loss 11.082197] [D_Loss 0.001278]\n",
      "train [Epoch 0/10] [Batch 156/1173] [Iter 156] [G_Loss 8.993929] [D_Loss 0.002480]\n",
      "train [Epoch 0/10] [Batch 157/1173] [Iter 157] [G_Loss 8.368836] [D_Loss 0.006689]\n",
      "train [Epoch 0/10] [Batch 158/1173] [Iter 158] [G_Loss 7.063937] [D_Loss 0.006850]\n",
      "train [Epoch 0/10] [Batch 159/1173] [Iter 159] [G_Loss 7.416465] [D_Loss 0.004096]\n",
      "train [Epoch 0/10] [Batch 160/1173] [Iter 160] [G_Loss 7.047956] [D_Loss 0.006275]\n",
      "train [Epoch 0/10] [Batch 161/1173] [Iter 161] [G_Loss 7.063539] [D_Loss 0.004743]\n",
      "train [Epoch 0/10] [Batch 162/1173] [Iter 162] [G_Loss 8.029637] [D_Loss 0.001738]\n",
      "train [Epoch 0/10] [Batch 163/1173] [Iter 163] [G_Loss 8.300643] [D_Loss 0.001456]\n",
      "train [Epoch 0/10] [Batch 164/1173] [Iter 164] [G_Loss 8.586360] [D_Loss 0.011565]\n",
      "train [Epoch 0/10] [Batch 165/1173] [Iter 165] [G_Loss 8.254032] [D_Loss 0.002510]\n",
      "train [Epoch 0/10] [Batch 166/1173] [Iter 166] [G_Loss 7.605236] [D_Loss 0.004342]\n",
      "train [Epoch 0/10] [Batch 167/1173] [Iter 167] [G_Loss 7.426723] [D_Loss 0.003894]\n",
      "train [Epoch 0/10] [Batch 168/1173] [Iter 168] [G_Loss 7.821157] [D_Loss 0.004920]\n",
      "train [Epoch 0/10] [Batch 169/1173] [Iter 169] [G_Loss 7.779600] [D_Loss 0.002421]\n",
      "train [Epoch 0/10] [Batch 170/1173] [Iter 170] [G_Loss 8.532163] [D_Loss 0.001457]\n",
      "train [Epoch 0/10] [Batch 171/1173] [Iter 171] [G_Loss 8.488072] [D_Loss 0.000763]\n",
      "train [Epoch 0/10] [Batch 172/1173] [Iter 172] [G_Loss 8.001223] [D_Loss 0.001864]\n",
      "train [Epoch 0/10] [Batch 173/1173] [Iter 173] [G_Loss 10.213850] [D_Loss 0.001696]\n",
      "train [Epoch 0/10] [Batch 174/1173] [Iter 174] [G_Loss 9.037488] [D_Loss 0.001564]\n",
      "train [Epoch 0/10] [Batch 175/1173] [Iter 175] [G_Loss 8.527789] [D_Loss 0.000868]\n",
      "train [Epoch 0/10] [Batch 176/1173] [Iter 176] [G_Loss 8.745566] [D_Loss 0.003820]\n",
      "train [Epoch 0/10] [Batch 177/1173] [Iter 177] [G_Loss 8.352148] [D_Loss 0.003789]\n",
      "train [Epoch 0/10] [Batch 178/1173] [Iter 178] [G_Loss 8.347116] [D_Loss 0.007135]\n",
      "train [Epoch 0/10] [Batch 179/1173] [Iter 179] [G_Loss 7.564906] [D_Loss 0.005523]\n",
      "train [Epoch 0/10] [Batch 180/1173] [Iter 180] [G_Loss 7.606180] [D_Loss 0.018389]\n",
      "train [Epoch 0/10] [Batch 181/1173] [Iter 181] [G_Loss 8.756359] [D_Loss 0.006386]\n",
      "train [Epoch 0/10] [Batch 182/1173] [Iter 182] [G_Loss 9.360872] [D_Loss 0.000533]\n",
      "train [Epoch 0/10] [Batch 183/1173] [Iter 183] [G_Loss 9.338165] [D_Loss 0.001431]\n",
      "train [Epoch 0/10] [Batch 184/1173] [Iter 184] [G_Loss 8.816184] [D_Loss 0.014826]\n",
      "train [Epoch 0/10] [Batch 185/1173] [Iter 185] [G_Loss 8.852843] [D_Loss 0.005287]\n",
      "train [Epoch 0/10] [Batch 186/1173] [Iter 186] [G_Loss 8.602194] [D_Loss 0.005255]\n",
      "train [Epoch 0/10] [Batch 187/1173] [Iter 187] [G_Loss 8.959701] [D_Loss 0.008198]\n",
      "train [Epoch 0/10] [Batch 188/1173] [Iter 188] [G_Loss 7.688233] [D_Loss 0.023519]\n",
      "train [Epoch 0/10] [Batch 189/1173] [Iter 189] [G_Loss 9.632919] [D_Loss 0.008579]\n",
      "train [Epoch 0/10] [Batch 190/1173] [Iter 190] [G_Loss 10.457003] [D_Loss 0.000278]\n",
      "train [Epoch 0/10] [Batch 191/1173] [Iter 191] [G_Loss 10.312341] [D_Loss 0.001663]\n",
      "train [Epoch 0/10] [Batch 192/1173] [Iter 192] [G_Loss 11.405070] [D_Loss 0.000613]\n",
      "train [Epoch 0/10] [Batch 193/1173] [Iter 193] [G_Loss 9.424427] [D_Loss 0.011041]\n",
      "train [Epoch 0/10] [Batch 194/1173] [Iter 194] [G_Loss 9.989724] [D_Loss 0.000774]\n",
      "train [Epoch 0/10] [Batch 195/1173] [Iter 195] [G_Loss 7.847937] [D_Loss 0.005975]\n",
      "train [Epoch 0/10] [Batch 196/1173] [Iter 196] [G_Loss 10.096628] [D_Loss 0.001083]\n",
      "train [Epoch 0/10] [Batch 197/1173] [Iter 197] [G_Loss 8.318398] [D_Loss 0.022647]\n",
      "train [Epoch 0/10] [Batch 198/1173] [Iter 198] [G_Loss 11.396534] [D_Loss 0.000821]\n",
      "train [Epoch 0/10] [Batch 199/1173] [Iter 199] [G_Loss 12.103160] [D_Loss 0.039311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 200/1173] [Iter 200] [G_Loss 8.911354] [D_Loss 0.004652]\n",
      "train [Epoch 0/10] [Batch 201/1173] [Iter 201] [G_Loss 8.534307] [D_Loss 0.020572]\n",
      "train [Epoch 0/10] [Batch 202/1173] [Iter 202] [G_Loss 11.020671] [D_Loss 0.011239]\n",
      "train [Epoch 0/10] [Batch 203/1173] [Iter 203] [G_Loss 12.005119] [D_Loss 0.088761]\n",
      "train [Epoch 0/10] [Batch 204/1173] [Iter 204] [G_Loss 11.104124] [D_Loss 0.070004]\n",
      "train [Epoch 0/10] [Batch 205/1173] [Iter 205] [G_Loss 7.012040] [D_Loss 0.028299]\n",
      "train [Epoch 0/10] [Batch 206/1173] [Iter 206] [G_Loss 8.310213] [D_Loss 0.006347]\n",
      "train [Epoch 0/10] [Batch 207/1173] [Iter 207] [G_Loss 8.296386] [D_Loss 0.010993]\n",
      "train [Epoch 0/10] [Batch 208/1173] [Iter 208] [G_Loss 9.059128] [D_Loss 0.004006]\n",
      "train [Epoch 0/10] [Batch 209/1173] [Iter 209] [G_Loss 10.039147] [D_Loss 0.050091]\n",
      "train [Epoch 0/10] [Batch 210/1173] [Iter 210] [G_Loss 8.151641] [D_Loss 0.019703]\n",
      "train [Epoch 0/10] [Batch 211/1173] [Iter 211] [G_Loss 7.921334] [D_Loss 0.020384]\n",
      "train [Epoch 0/10] [Batch 212/1173] [Iter 212] [G_Loss 9.618491] [D_Loss 0.022278]\n",
      "train [Epoch 0/10] [Batch 213/1173] [Iter 213] [G_Loss 8.047652] [D_Loss 0.008687]\n",
      "train [Epoch 0/10] [Batch 214/1173] [Iter 214] [G_Loss 8.444828] [D_Loss 0.031409]\n",
      "train [Epoch 0/10] [Batch 215/1173] [Iter 215] [G_Loss 13.997474] [D_Loss 0.052856]\n",
      "train [Epoch 0/10] [Batch 216/1173] [Iter 216] [G_Loss 8.589205] [D_Loss 0.099298]\n",
      "train [Epoch 0/10] [Batch 217/1173] [Iter 217] [G_Loss 8.398984] [D_Loss 0.008595]\n",
      "train [Epoch 0/10] [Batch 218/1173] [Iter 218] [G_Loss 10.215121] [D_Loss 0.002610]\n",
      "train [Epoch 0/10] [Batch 219/1173] [Iter 219] [G_Loss 11.570611] [D_Loss 0.001842]\n",
      "train [Epoch 0/10] [Batch 220/1173] [Iter 220] [G_Loss 9.461491] [D_Loss 0.001372]\n",
      "train [Epoch 0/10] [Batch 221/1173] [Iter 221] [G_Loss 10.545447] [D_Loss 0.022304]\n",
      "train [Epoch 0/10] [Batch 222/1173] [Iter 222] [G_Loss 10.032074] [D_Loss 0.001641]\n",
      "train [Epoch 0/10] [Batch 223/1173] [Iter 223] [G_Loss 7.103053] [D_Loss 0.007719]\n",
      "train [Epoch 0/10] [Batch 224/1173] [Iter 224] [G_Loss 7.127667] [D_Loss 0.025745]\n",
      "train [Epoch 0/10] [Batch 225/1173] [Iter 225] [G_Loss 8.841865] [D_Loss 0.006519]\n",
      "train [Epoch 0/10] [Batch 226/1173] [Iter 226] [G_Loss 11.281540] [D_Loss 0.035532]\n",
      "train [Epoch 0/10] [Batch 227/1173] [Iter 227] [G_Loss 5.690552] [D_Loss 0.060660]\n",
      "train [Epoch 0/10] [Batch 228/1173] [Iter 228] [G_Loss 15.081691] [D_Loss 0.353646]\n",
      "train [Epoch 0/10] [Batch 229/1173] [Iter 229] [G_Loss 0.324092] [D_Loss 10.870398]\n",
      "train [Epoch 0/10] [Batch 230/1173] [Iter 230] [G_Loss 40.790760] [D_Loss 34.687046]\n",
      "train [Epoch 0/10] [Batch 231/1173] [Iter 231] [G_Loss 4.401018] [D_Loss 0.527458]\n",
      "train [Epoch 0/10] [Batch 232/1173] [Iter 232] [G_Loss 0.806033] [D_Loss 0.711339]\n",
      "train [Epoch 0/10] [Batch 233/1173] [Iter 233] [G_Loss 0.756982] [D_Loss 0.717826]\n",
      "train [Epoch 0/10] [Batch 234/1173] [Iter 234] [G_Loss 2.101019] [D_Loss 0.324669]\n",
      "train [Epoch 0/10] [Batch 235/1173] [Iter 235] [G_Loss 3.598639] [D_Loss 0.129267]\n",
      "train [Epoch 0/10] [Batch 236/1173] [Iter 236] [G_Loss 6.550705] [D_Loss 0.224899]\n",
      "train [Epoch 0/10] [Batch 237/1173] [Iter 237] [G_Loss 2.758888] [D_Loss 0.146867]\n",
      "train [Epoch 0/10] [Batch 238/1173] [Iter 238] [G_Loss 4.217233] [D_Loss 0.038590]\n",
      "train [Epoch 0/10] [Batch 239/1173] [Iter 239] [G_Loss 5.158587] [D_Loss 0.016336]\n",
      "train [Epoch 0/10] [Batch 240/1173] [Iter 240] [G_Loss 6.207601] [D_Loss 0.011111]\n",
      "train [Epoch 0/10] [Batch 241/1173] [Iter 241] [G_Loss 6.316996] [D_Loss 0.010707]\n",
      "train [Epoch 0/10] [Batch 242/1173] [Iter 242] [G_Loss 6.125718] [D_Loss 0.010042]\n",
      "train [Epoch 0/10] [Batch 243/1173] [Iter 243] [G_Loss 6.048086] [D_Loss 0.015811]\n",
      "train [Epoch 0/10] [Batch 244/1173] [Iter 244] [G_Loss 6.489137] [D_Loss 0.051853]\n",
      "train [Epoch 0/10] [Batch 245/1173] [Iter 245] [G_Loss 5.822195] [D_Loss 0.025339]\n",
      "train [Epoch 0/10] [Batch 246/1173] [Iter 246] [G_Loss 6.074644] [D_Loss 0.009531]\n",
      "train [Epoch 0/10] [Batch 247/1173] [Iter 247] [G_Loss 6.841683] [D_Loss 0.046710]\n",
      "train [Epoch 0/10] [Batch 248/1173] [Iter 248] [G_Loss 5.896709] [D_Loss 0.133819]\n",
      "train [Epoch 0/10] [Batch 249/1173] [Iter 249] [G_Loss 9.778298] [D_Loss 0.587230]\n",
      "train [Epoch 0/10] [Batch 250/1173] [Iter 250] [G_Loss 0.748266] [D_Loss 3.590557]\n",
      "train [Epoch 0/10] [Batch 251/1173] [Iter 251] [G_Loss 6.466605] [D_Loss 0.276128]\n",
      "train [Epoch 0/10] [Batch 252/1173] [Iter 252] [G_Loss 6.318599] [D_Loss 0.031061]\n",
      "train [Epoch 0/10] [Batch 253/1173] [Iter 253] [G_Loss 5.580228] [D_Loss 0.023717]\n",
      "train [Epoch 0/10] [Batch 254/1173] [Iter 254] [G_Loss 5.163952] [D_Loss 0.072343]\n",
      "train [Epoch 0/10] [Batch 255/1173] [Iter 255] [G_Loss 6.151568] [D_Loss 0.090317]\n",
      "train [Epoch 0/10] [Batch 256/1173] [Iter 256] [G_Loss 5.958211] [D_Loss 0.070266]\n",
      "train [Epoch 0/10] [Batch 257/1173] [Iter 257] [G_Loss 6.580637] [D_Loss 0.187940]\n",
      "train [Epoch 0/10] [Batch 258/1173] [Iter 258] [G_Loss 4.793533] [D_Loss 0.041494]\n",
      "train [Epoch 0/10] [Batch 259/1173] [Iter 259] [G_Loss 6.470911] [D_Loss 0.007906]\n",
      "train [Epoch 0/10] [Batch 260/1173] [Iter 260] [G_Loss 6.577343] [D_Loss 0.064390]\n",
      "train [Epoch 0/10] [Batch 261/1173] [Iter 261] [G_Loss 5.793898] [D_Loss 0.017784]\n",
      "train [Epoch 0/10] [Batch 262/1173] [Iter 262] [G_Loss 4.779802] [D_Loss 0.023890]\n",
      "train [Epoch 0/10] [Batch 263/1173] [Iter 263] [G_Loss 5.235600] [D_Loss 0.017182]\n",
      "train [Epoch 0/10] [Batch 264/1173] [Iter 264] [G_Loss 6.151830] [D_Loss 0.026031]\n",
      "train [Epoch 0/10] [Batch 265/1173] [Iter 265] [G_Loss 5.533805] [D_Loss 0.026060]\n",
      "train [Epoch 0/10] [Batch 266/1173] [Iter 266] [G_Loss 6.055663] [D_Loss 0.041182]\n",
      "train [Epoch 0/10] [Batch 267/1173] [Iter 267] [G_Loss 5.109761] [D_Loss 0.146697]\n",
      "train [Epoch 0/10] [Batch 268/1173] [Iter 268] [G_Loss 9.550879] [D_Loss 0.360478]\n",
      "train [Epoch 0/10] [Batch 269/1173] [Iter 269] [G_Loss 6.736756] [D_Loss 0.177347]\n",
      "train [Epoch 0/10] [Batch 270/1173] [Iter 270] [G_Loss 5.689977] [D_Loss 0.257571]\n",
      "train [Epoch 0/10] [Batch 271/1173] [Iter 271] [G_Loss 5.850863] [D_Loss 0.136126]\n",
      "train [Epoch 0/10] [Batch 272/1173] [Iter 272] [G_Loss 6.717287] [D_Loss 0.100340]\n",
      "train [Epoch 0/10] [Batch 273/1173] [Iter 273] [G_Loss 5.615702] [D_Loss 0.153206]\n",
      "train [Epoch 0/10] [Batch 274/1173] [Iter 274] [G_Loss 4.098744] [D_Loss 0.184468]\n",
      "train [Epoch 0/10] [Batch 275/1173] [Iter 275] [G_Loss 5.852423] [D_Loss 0.060816]\n",
      "train [Epoch 0/10] [Batch 276/1173] [Iter 276] [G_Loss 5.195735] [D_Loss 0.136752]\n",
      "train [Epoch 0/10] [Batch 277/1173] [Iter 277] [G_Loss 5.799579] [D_Loss 0.076137]\n",
      "train [Epoch 0/10] [Batch 278/1173] [Iter 278] [G_Loss 4.213767] [D_Loss 0.065247]\n",
      "train [Epoch 0/10] [Batch 279/1173] [Iter 279] [G_Loss 6.075961] [D_Loss 0.048199]\n",
      "train [Epoch 0/10] [Batch 280/1173] [Iter 280] [G_Loss 6.016119] [D_Loss 0.084173]\n",
      "train [Epoch 0/10] [Batch 281/1173] [Iter 281] [G_Loss 2.836951] [D_Loss 1.327034]\n",
      "train [Epoch 0/10] [Batch 282/1173] [Iter 282] [G_Loss 16.956360] [D_Loss 8.080203]\n",
      "train [Epoch 0/10] [Batch 283/1173] [Iter 283] [G_Loss 0.291584] [D_Loss 19.008310]\n",
      "train [Epoch 0/10] [Batch 284/1173] [Iter 284] [G_Loss 17.003914] [D_Loss 28.139765]\n",
      "train [Epoch 0/10] [Batch 285/1173] [Iter 285] [G_Loss 1.532961] [D_Loss 19.642565]\n",
      "train [Epoch 0/10] [Batch 286/1173] [Iter 286] [G_Loss 10.740038] [D_Loss 0.864224]\n",
      "train [Epoch 0/10] [Batch 287/1173] [Iter 287] [G_Loss 10.893683] [D_Loss 1.529550]\n",
      "train [Epoch 0/10] [Batch 288/1173] [Iter 288] [G_Loss 5.129844] [D_Loss 0.722379]\n",
      "train [Epoch 0/10] [Batch 289/1173] [Iter 289] [G_Loss 2.155705] [D_Loss 0.437158]\n",
      "train [Epoch 0/10] [Batch 290/1173] [Iter 290] [G_Loss 2.329245] [D_Loss 0.319606]\n",
      "train [Epoch 0/10] [Batch 291/1173] [Iter 291] [G_Loss 3.718125] [D_Loss 0.332244]\n",
      "train [Epoch 0/10] [Batch 292/1173] [Iter 292] [G_Loss 2.495378] [D_Loss 0.370782]\n",
      "train [Epoch 0/10] [Batch 293/1173] [Iter 293] [G_Loss 3.242620] [D_Loss 0.380429]\n",
      "train [Epoch 0/10] [Batch 294/1173] [Iter 294] [G_Loss 3.485649] [D_Loss 0.254400]\n",
      "train [Epoch 0/10] [Batch 295/1173] [Iter 295] [G_Loss 3.692875] [D_Loss 0.459704]\n",
      "train [Epoch 0/10] [Batch 296/1173] [Iter 296] [G_Loss 3.254429] [D_Loss 0.370586]\n",
      "train [Epoch 0/10] [Batch 297/1173] [Iter 297] [G_Loss 3.101141] [D_Loss 0.296615]\n",
      "train [Epoch 0/10] [Batch 298/1173] [Iter 298] [G_Loss 3.360204] [D_Loss 0.264131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 299/1173] [Iter 299] [G_Loss 3.745106] [D_Loss 0.352218]\n",
      "train [Epoch 0/10] [Batch 300/1173] [Iter 300] [G_Loss 3.680326] [D_Loss 0.220975]\n",
      "train [Epoch 0/10] [Batch 301/1173] [Iter 301] [G_Loss 3.767313] [D_Loss 0.225635]\n",
      "train [Epoch 0/10] [Batch 302/1173] [Iter 302] [G_Loss 3.969647] [D_Loss 0.321067]\n",
      "train [Epoch 0/10] [Batch 303/1173] [Iter 303] [G_Loss 3.954273] [D_Loss 0.198600]\n",
      "train [Epoch 0/10] [Batch 304/1173] [Iter 304] [G_Loss 4.247560] [D_Loss 0.171020]\n",
      "train [Epoch 0/10] [Batch 305/1173] [Iter 305] [G_Loss 3.963195] [D_Loss 0.250815]\n",
      "train [Epoch 0/10] [Batch 306/1173] [Iter 306] [G_Loss 3.561711] [D_Loss 0.281733]\n",
      "train [Epoch 0/10] [Batch 307/1173] [Iter 307] [G_Loss 2.430889] [D_Loss 0.240634]\n",
      "train [Epoch 0/10] [Batch 308/1173] [Iter 308] [G_Loss 4.597533] [D_Loss 0.313526]\n",
      "train [Epoch 0/10] [Batch 309/1173] [Iter 309] [G_Loss 3.928946] [D_Loss 0.112523]\n",
      "train [Epoch 0/10] [Batch 310/1173] [Iter 310] [G_Loss 3.814043] [D_Loss 0.158724]\n",
      "train [Epoch 0/10] [Batch 311/1173] [Iter 311] [G_Loss 3.833890] [D_Loss 0.119871]\n",
      "train [Epoch 0/10] [Batch 312/1173] [Iter 312] [G_Loss 4.323869] [D_Loss 0.124893]\n",
      "train [Epoch 0/10] [Batch 313/1173] [Iter 313] [G_Loss 4.102058] [D_Loss 0.146935]\n",
      "train [Epoch 0/10] [Batch 317/1173] [Iter 317] [G_Loss 4.057517] [D_Loss 0.089276]\n",
      "train [Epoch 0/10] [Batch 318/1173] [Iter 318] [G_Loss 5.820610] [D_Loss 0.041333]\n",
      "train [Epoch 0/10] [Batch 319/1173] [Iter 319] [G_Loss 5.995920] [D_Loss 0.111199]\n",
      "train [Epoch 0/10] [Batch 320/1173] [Iter 320] [G_Loss 5.792278] [D_Loss 0.076659]\n",
      "train [Epoch 0/10] [Batch 321/1173] [Iter 321] [G_Loss 4.796927] [D_Loss 0.054323]\n",
      "train [Epoch 0/10] [Batch 322/1173] [Iter 322] [G_Loss 5.408373] [D_Loss 0.064203]\n",
      "train [Epoch 0/10] [Batch 323/1173] [Iter 323] [G_Loss 4.627058] [D_Loss 0.087921]\n",
      "train [Epoch 0/10] [Batch 324/1173] [Iter 324] [G_Loss 4.376064] [D_Loss 0.129601]\n",
      "train [Epoch 0/10] [Batch 325/1173] [Iter 325] [G_Loss 4.812062] [D_Loss 0.079481]\n",
      "train [Epoch 0/10] [Batch 326/1173] [Iter 326] [G_Loss 5.183519] [D_Loss 0.041880]\n",
      "train [Epoch 0/10] [Batch 327/1173] [Iter 327] [G_Loss 5.247722] [D_Loss 0.111560]\n",
      "train [Epoch 0/10] [Batch 328/1173] [Iter 328] [G_Loss 2.858813] [D_Loss 0.141660]\n",
      "train [Epoch 0/10] [Batch 329/1173] [Iter 329] [G_Loss 10.412123] [D_Loss 0.174203]\n",
      "train [Epoch 0/10] [Batch 330/1173] [Iter 330] [G_Loss 8.964044] [D_Loss 0.155183]\n",
      "train [Epoch 0/10] [Batch 331/1173] [Iter 331] [G_Loss 5.633670] [D_Loss 0.094508]\n",
      "train [Epoch 0/10] [Batch 332/1173] [Iter 332] [G_Loss 4.647562] [D_Loss 0.059190]\n",
      "train [Epoch 0/10] [Batch 333/1173] [Iter 333] [G_Loss 6.387661] [D_Loss 0.050850]\n",
      "train [Epoch 0/10] [Batch 334/1173] [Iter 334] [G_Loss 7.327721] [D_Loss 0.027849]\n",
      "train [Epoch 0/10] [Batch 335/1173] [Iter 335] [G_Loss 6.906880] [D_Loss 0.071627]\n",
      "train [Epoch 0/10] [Batch 336/1173] [Iter 336] [G_Loss 5.900218] [D_Loss 0.019753]\n",
      "train [Epoch 0/10] [Batch 337/1173] [Iter 337] [G_Loss 5.310549] [D_Loss 0.068313]\n",
      "train [Epoch 0/10] [Batch 338/1173] [Iter 338] [G_Loss 4.539547] [D_Loss 0.048406]\n",
      "train [Epoch 0/10] [Batch 339/1173] [Iter 339] [G_Loss 7.555967] [D_Loss 0.074318]\n",
      "train [Epoch 0/10] [Batch 340/1173] [Iter 340] [G_Loss 5.779572] [D_Loss 0.023542]\n",
      "train [Epoch 0/10] [Batch 341/1173] [Iter 341] [G_Loss 6.044384] [D_Loss 0.048917]\n",
      "train [Epoch 0/10] [Batch 342/1173] [Iter 342] [G_Loss 6.205529] [D_Loss 0.041646]\n",
      "train [Epoch 0/10] [Batch 343/1173] [Iter 343] [G_Loss 7.971379] [D_Loss 0.123533]\n",
      "train [Epoch 0/10] [Batch 344/1173] [Iter 344] [G_Loss 4.522961] [D_Loss 0.091419]\n",
      "train [Epoch 0/10] [Batch 345/1173] [Iter 345] [G_Loss 8.171365] [D_Loss 0.106579]\n",
      "train [Epoch 0/10] [Batch 346/1173] [Iter 346] [G_Loss 7.768909] [D_Loss 0.008605]\n",
      "train [Epoch 0/10] [Batch 347/1173] [Iter 347] [G_Loss 7.236195] [D_Loss 0.038524]\n",
      "train [Epoch 0/10] [Batch 348/1173] [Iter 348] [G_Loss 5.344797] [D_Loss 0.077868]\n",
      "train [Epoch 0/10] [Batch 349/1173] [Iter 349] [G_Loss 4.077341] [D_Loss 0.106181]\n",
      "train [Epoch 0/10] [Batch 350/1173] [Iter 350] [G_Loss 9.215562] [D_Loss 0.033974]\n",
      "train [Epoch 0/10] [Batch 351/1173] [Iter 351] [G_Loss 9.408903] [D_Loss 0.162736]\n",
      "train [Epoch 0/10] [Batch 352/1173] [Iter 352] [G_Loss 3.772932] [D_Loss 0.189940]\n",
      "train [Epoch 0/10] [Batch 353/1173] [Iter 353] [G_Loss 11.319507] [D_Loss 0.321207]\n",
      "train [Epoch 0/10] [Batch 354/1173] [Iter 354] [G_Loss 10.038542] [D_Loss 0.022305]\n",
      "train [Epoch 0/10] [Batch 355/1173] [Iter 355] [G_Loss 9.216770] [D_Loss 0.097422]\n",
      "train [Epoch 0/10] [Batch 356/1173] [Iter 356] [G_Loss 7.575281] [D_Loss 0.020559]\n",
      "train [Epoch 0/10] [Batch 357/1173] [Iter 357] [G_Loss 6.973928] [D_Loss 0.017925]\n",
      "train [Epoch 0/10] [Batch 358/1173] [Iter 358] [G_Loss 6.300001] [D_Loss 0.040175]\n",
      "train [Epoch 0/10] [Batch 359/1173] [Iter 359] [G_Loss 6.491825] [D_Loss 0.055111]\n",
      "train [Epoch 0/10] [Batch 360/1173] [Iter 360] [G_Loss 6.478708] [D_Loss 0.017640]\n",
      "train [Epoch 0/10] [Batch 361/1173] [Iter 361] [G_Loss 7.669348] [D_Loss 0.013838]\n",
      "train [Epoch 0/10] [Batch 362/1173] [Iter 362] [G_Loss 8.568443] [D_Loss 0.052670]\n",
      "train [Epoch 0/10] [Batch 363/1173] [Iter 363] [G_Loss 6.979662] [D_Loss 0.019911]\n",
      "train [Epoch 0/10] [Batch 364/1173] [Iter 364] [G_Loss 5.885005] [D_Loss 0.070758]\n",
      "train [Epoch 0/10] [Batch 365/1173] [Iter 365] [G_Loss 5.113124] [D_Loss 0.057584]\n",
      "train [Epoch 0/10] [Batch 366/1173] [Iter 366] [G_Loss 10.365827] [D_Loss 0.081513]\n",
      "train [Epoch 0/10] [Batch 367/1173] [Iter 367] [G_Loss 9.742448] [D_Loss 0.106106]\n",
      "train [Epoch 0/10] [Batch 368/1173] [Iter 368] [G_Loss 6.686110] [D_Loss 0.043858]\n",
      "train [Epoch 0/10] [Batch 369/1173] [Iter 369] [G_Loss 5.819212] [D_Loss 0.047406]\n",
      "train [Epoch 0/10] [Batch 370/1173] [Iter 370] [G_Loss 6.388508] [D_Loss 0.032315]\n",
      "train [Epoch 0/10] [Batch 371/1173] [Iter 371] [G_Loss 8.360720] [D_Loss 0.075396]\n",
      "train [Epoch 0/10] [Batch 372/1173] [Iter 372] [G_Loss 8.234663] [D_Loss 0.024256]\n",
      "train [Epoch 0/10] [Batch 373/1173] [Iter 373] [G_Loss 7.046629] [D_Loss 0.023778]\n",
      "train [Epoch 0/10] [Batch 374/1173] [Iter 374] [G_Loss 7.001958] [D_Loss 0.042000]\n",
      "train [Epoch 0/10] [Batch 375/1173] [Iter 375] [G_Loss 7.748214] [D_Loss 0.044857]\n",
      "train [Epoch 0/10] [Batch 376/1173] [Iter 376] [G_Loss 6.596014] [D_Loss 0.037243]\n",
      "train [Epoch 0/10] [Batch 377/1173] [Iter 377] [G_Loss 5.141661] [D_Loss 0.074092]\n",
      "train [Epoch 0/10] [Batch 378/1173] [Iter 378] [G_Loss 11.291339] [D_Loss 0.125896]\n",
      "train [Epoch 0/10] [Batch 379/1173] [Iter 379] [G_Loss 9.269796] [D_Loss 0.035465]\n",
      "train [Epoch 0/10] [Batch 380/1173] [Iter 380] [G_Loss 6.679215] [D_Loss 0.050342]\n",
      "train [Epoch 0/10] [Batch 381/1173] [Iter 381] [G_Loss 7.151797] [D_Loss 0.087673]\n",
      "train [Epoch 0/10] [Batch 382/1173] [Iter 382] [G_Loss 11.339159] [D_Loss 0.189245]\n",
      "train [Epoch 0/10] [Batch 383/1173] [Iter 383] [G_Loss 2.254561] [D_Loss 1.330336]\n",
      "train [Epoch 0/10] [Batch 384/1173] [Iter 384] [G_Loss 22.145260] [D_Loss 7.152514]\n",
      "train [Epoch 0/10] [Batch 385/1173] [Iter 385] [G_Loss 9.417948] [D_Loss 0.310510]\n",
      "train [Epoch 0/10] [Batch 386/1173] [Iter 386] [G_Loss 5.192095] [D_Loss 1.066683]\n",
      "train [Epoch 0/10] [Batch 387/1173] [Iter 387] [G_Loss 5.409936] [D_Loss 0.918496]\n",
      "train [Epoch 0/10] [Batch 388/1173] [Iter 388] [G_Loss 6.511800] [D_Loss 0.337780]\n",
      "train [Epoch 0/10] [Batch 389/1173] [Iter 389] [G_Loss 7.069777] [D_Loss 0.358010]\n",
      "train [Epoch 0/10] [Batch 390/1173] [Iter 390] [G_Loss 6.801248] [D_Loss 0.341931]\n",
      "train [Epoch 0/10] [Batch 391/1173] [Iter 391] [G_Loss 6.166028] [D_Loss 0.198875]\n",
      "train [Epoch 0/10] [Batch 392/1173] [Iter 392] [G_Loss 5.374861] [D_Loss 0.206564]\n",
      "train [Epoch 0/10] [Batch 393/1173] [Iter 393] [G_Loss 5.337062] [D_Loss 0.339593]\n",
      "train [Epoch 0/10] [Batch 394/1173] [Iter 394] [G_Loss 4.310293] [D_Loss 0.216853]\n",
      "train [Epoch 0/10] [Batch 395/1173] [Iter 395] [G_Loss 4.572304] [D_Loss 0.154597]\n",
      "train [Epoch 0/10] [Batch 396/1173] [Iter 396] [G_Loss 4.485250] [D_Loss 0.214632]\n",
      "train [Epoch 0/10] [Batch 397/1173] [Iter 397] [G_Loss 3.724296] [D_Loss 0.350712]\n",
      "train [Epoch 0/10] [Batch 398/1173] [Iter 398] [G_Loss 4.785056] [D_Loss 0.366441]\n",
      "train [Epoch 0/10] [Batch 399/1173] [Iter 399] [G_Loss 4.068624] [D_Loss 0.288248]\n",
      "train [Epoch 0/10] [Batch 400/1173] [Iter 400] [G_Loss 5.010943] [D_Loss 0.283922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 401/1173] [Iter 401] [G_Loss 4.431911] [D_Loss 0.184866]\n",
      "train [Epoch 0/10] [Batch 402/1173] [Iter 402] [G_Loss 4.714238] [D_Loss 0.151776]\n",
      "train [Epoch 0/10] [Batch 403/1173] [Iter 403] [G_Loss 4.462899] [D_Loss 0.101024]\n",
      "train [Epoch 0/10] [Batch 404/1173] [Iter 404] [G_Loss 4.526664] [D_Loss 0.125413]\n",
      "train [Epoch 0/10] [Batch 405/1173] [Iter 405] [G_Loss 4.434633] [D_Loss 0.102941]\n",
      "train [Epoch 0/10] [Batch 406/1173] [Iter 406] [G_Loss 4.804996] [D_Loss 0.068827]\n",
      "train [Epoch 0/10] [Batch 407/1173] [Iter 407] [G_Loss 4.698168] [D_Loss 0.119272]\n",
      "train [Epoch 0/10] [Batch 408/1173] [Iter 408] [G_Loss 4.693053] [D_Loss 0.185635]\n",
      "train [Epoch 0/10] [Batch 409/1173] [Iter 409] [G_Loss 4.766107] [D_Loss 0.067783]\n",
      "train [Epoch 0/10] [Batch 410/1173] [Iter 410] [G_Loss 5.000703] [D_Loss 0.108786]\n",
      "train [Epoch 0/10] [Batch 411/1173] [Iter 411] [G_Loss 4.538275] [D_Loss 0.045660]\n",
      "train [Epoch 0/10] [Batch 412/1173] [Iter 412] [G_Loss 4.790104] [D_Loss 0.126760]\n",
      "train [Epoch 0/10] [Batch 413/1173] [Iter 413] [G_Loss 4.332966] [D_Loss 0.088697]\n",
      "train [Epoch 0/10] [Batch 414/1173] [Iter 414] [G_Loss 4.407137] [D_Loss 0.065870]\n",
      "train [Epoch 0/10] [Batch 415/1173] [Iter 415] [G_Loss 4.814325] [D_Loss 0.034429]\n",
      "train [Epoch 0/10] [Batch 416/1173] [Iter 416] [G_Loss 5.073462] [D_Loss 0.088242]\n",
      "train [Epoch 0/10] [Batch 417/1173] [Iter 417] [G_Loss 3.999001] [D_Loss 0.123153]\n",
      "train [Epoch 0/10] [Batch 418/1173] [Iter 418] [G_Loss 5.156525] [D_Loss 0.173712]\n",
      "train [Epoch 0/10] [Batch 419/1173] [Iter 419] [G_Loss 3.704276] [D_Loss 0.161329]\n",
      "train [Epoch 0/10] [Batch 420/1173] [Iter 420] [G_Loss 5.264580] [D_Loss 0.385979]\n",
      "train [Epoch 0/10] [Batch 421/1173] [Iter 421] [G_Loss 1.479522] [D_Loss 1.280775]\n",
      "train [Epoch 0/10] [Batch 422/1173] [Iter 422] [G_Loss 9.478685] [D_Loss 4.681139]\n",
      "train [Epoch 0/10] [Batch 423/1173] [Iter 423] [G_Loss 6.249105] [D_Loss 0.148658]\n",
      "train [Epoch 0/10] [Batch 424/1173] [Iter 424] [G_Loss 3.331283] [D_Loss 0.319787]\n",
      "train [Epoch 0/10] [Batch 425/1173] [Iter 425] [G_Loss 5.243836] [D_Loss 0.078746]\n",
      "train [Epoch 0/10] [Batch 426/1173] [Iter 426] [G_Loss 6.097316] [D_Loss 0.159770]\n",
      "train [Epoch 0/10] [Batch 427/1173] [Iter 427] [G_Loss 6.085451] [D_Loss 0.076957]\n",
      "train [Epoch 0/10] [Batch 428/1173] [Iter 428] [G_Loss 5.362007] [D_Loss 0.297676]\n",
      "train [Epoch 0/10] [Batch 429/1173] [Iter 429] [G_Loss 4.153491] [D_Loss 0.182143]\n",
      "train [Epoch 0/10] [Batch 430/1173] [Iter 430] [G_Loss 4.974999] [D_Loss 0.144043]\n",
      "train [Epoch 0/10] [Batch 431/1173] [Iter 431] [G_Loss 4.364342] [D_Loss 0.115458]\n",
      "train [Epoch 0/10] [Batch 432/1173] [Iter 432] [G_Loss 3.991685] [D_Loss 0.072632]\n",
      "train [Epoch 0/10] [Batch 433/1173] [Iter 433] [G_Loss 3.704458] [D_Loss 0.088281]\n",
      "train [Epoch 0/10] [Batch 434/1173] [Iter 434] [G_Loss 3.500210] [D_Loss 0.117103]\n",
      "train [Epoch 0/10] [Batch 435/1173] [Iter 435] [G_Loss 3.412207] [D_Loss 0.059834]\n",
      "train [Epoch 0/10] [Batch 436/1173] [Iter 436] [G_Loss 3.773484] [D_Loss 0.069180]\n",
      "train [Epoch 0/10] [Batch 437/1173] [Iter 437] [G_Loss 4.260672] [D_Loss 0.054407]\n",
      "train [Epoch 0/10] [Batch 438/1173] [Iter 438] [G_Loss 4.225552] [D_Loss 0.067405]\n",
      "train [Epoch 0/10] [Batch 439/1173] [Iter 439] [G_Loss 3.980860] [D_Loss 0.053319]\n",
      "train [Epoch 0/10] [Batch 440/1173] [Iter 440] [G_Loss 4.172999] [D_Loss 0.069733]\n",
      "train [Epoch 0/10] [Batch 441/1173] [Iter 441] [G_Loss 4.622528] [D_Loss 0.026542]\n",
      "train [Epoch 0/10] [Batch 442/1173] [Iter 442] [G_Loss 4.359950] [D_Loss 0.050998]\n",
      "train [Epoch 0/10] [Batch 443/1173] [Iter 443] [G_Loss 4.825619] [D_Loss 0.039346]\n",
      "train [Epoch 0/10] [Batch 444/1173] [Iter 444] [G_Loss 5.115527] [D_Loss 0.101176]\n",
      "train [Epoch 0/10] [Batch 445/1173] [Iter 445] [G_Loss 4.742634] [D_Loss 0.112016]\n",
      "train [Epoch 0/10] [Batch 446/1173] [Iter 446] [G_Loss 4.696273] [D_Loss 0.065860]\n",
      "train [Epoch 0/10] [Batch 447/1173] [Iter 447] [G_Loss 4.184486] [D_Loss 0.075710]\n",
      "train [Epoch 0/10] [Batch 448/1173] [Iter 448] [G_Loss 5.158720] [D_Loss 0.085515]\n",
      "train [Epoch 0/10] [Batch 449/1173] [Iter 449] [G_Loss 3.465942] [D_Loss 0.220681]\n",
      "train [Epoch 0/10] [Batch 450/1173] [Iter 450] [G_Loss 8.483070] [D_Loss 0.942007]\n",
      "train [Epoch 0/10] [Batch 451/1173] [Iter 451] [G_Loss 2.183971] [D_Loss 1.820021]\n",
      "train [Epoch 0/10] [Batch 452/1173] [Iter 452] [G_Loss 7.697154] [D_Loss 1.153379]\n",
      "train [Epoch 0/10] [Batch 453/1173] [Iter 453] [G_Loss 6.074123] [D_Loss 0.326703]\n",
      "train [Epoch 0/10] [Batch 454/1173] [Iter 454] [G_Loss 3.738908] [D_Loss 0.119313]\n",
      "train [Epoch 0/10] [Batch 455/1173] [Iter 455] [G_Loss 2.791002] [D_Loss 0.183555]\n",
      "train [Epoch 0/10] [Batch 456/1173] [Iter 456] [G_Loss 3.896162] [D_Loss 0.144622]\n",
      "train [Epoch 0/10] [Batch 457/1173] [Iter 457] [G_Loss 4.772864] [D_Loss 0.059546]\n",
      "train [Epoch 0/10] [Batch 458/1173] [Iter 458] [G_Loss 5.191182] [D_Loss 0.042215]\n",
      "train [Epoch 0/10] [Batch 459/1173] [Iter 459] [G_Loss 5.261355] [D_Loss 0.058743]\n",
      "train [Epoch 0/10] [Batch 460/1173] [Iter 460] [G_Loss 5.142383] [D_Loss 0.053073]\n",
      "train [Epoch 0/10] [Batch 461/1173] [Iter 461] [G_Loss 5.083322] [D_Loss 0.042496]\n",
      "train [Epoch 0/10] [Batch 462/1173] [Iter 462] [G_Loss 4.896710] [D_Loss 0.074483]\n",
      "train [Epoch 0/10] [Batch 463/1173] [Iter 463] [G_Loss 4.818425] [D_Loss 0.076407]\n",
      "train [Epoch 0/10] [Batch 464/1173] [Iter 464] [G_Loss 4.310556] [D_Loss 0.070322]\n",
      "train [Epoch 0/10] [Batch 465/1173] [Iter 465] [G_Loss 4.790581] [D_Loss 0.042263]\n",
      "train [Epoch 0/10] [Batch 466/1173] [Iter 466] [G_Loss 4.918614] [D_Loss 0.074427]\n",
      "train [Epoch 0/10] [Batch 467/1173] [Iter 467] [G_Loss 4.672972] [D_Loss 0.041830]\n",
      "train [Epoch 0/10] [Batch 468/1173] [Iter 468] [G_Loss 4.796256] [D_Loss 0.044987]\n",
      "train [Epoch 0/10] [Batch 469/1173] [Iter 469] [G_Loss 5.012573] [D_Loss 0.111211]\n",
      "train [Epoch 0/10] [Batch 470/1173] [Iter 470] [G_Loss 4.830403] [D_Loss 0.039998]\n",
      "train [Epoch 0/10] [Batch 471/1173] [Iter 471] [G_Loss 4.520292] [D_Loss 0.044820]\n",
      "train [Epoch 0/10] [Batch 472/1173] [Iter 472] [G_Loss 5.073722] [D_Loss 0.121690]\n",
      "train [Epoch 0/10] [Batch 473/1173] [Iter 473] [G_Loss 5.134444] [D_Loss 0.036868]\n",
      "train [Epoch 0/10] [Batch 474/1173] [Iter 474] [G_Loss 5.707844] [D_Loss 0.030659]\n",
      "train [Epoch 0/10] [Batch 475/1173] [Iter 475] [G_Loss 5.610367] [D_Loss 0.027668]\n",
      "train [Epoch 0/10] [Batch 476/1173] [Iter 476] [G_Loss 5.475780] [D_Loss 0.037626]\n",
      "train [Epoch 0/10] [Batch 477/1173] [Iter 477] [G_Loss 5.364496] [D_Loss 0.034636]\n",
      "train [Epoch 0/10] [Batch 478/1173] [Iter 478] [G_Loss 5.282878] [D_Loss 0.050011]\n",
      "train [Epoch 0/10] [Batch 479/1173] [Iter 479] [G_Loss 5.110241] [D_Loss 0.040458]\n",
      "train [Epoch 0/10] [Batch 480/1173] [Iter 480] [G_Loss 5.189803] [D_Loss 0.042565]\n",
      "train [Epoch 0/10] [Batch 481/1173] [Iter 481] [G_Loss 5.373956] [D_Loss 0.027203]\n",
      "train [Epoch 0/10] [Batch 482/1173] [Iter 482] [G_Loss 5.518171] [D_Loss 0.104401]\n",
      "train [Epoch 0/10] [Batch 483/1173] [Iter 483] [G_Loss 4.673907] [D_Loss 0.080438]\n",
      "train [Epoch 0/10] [Batch 484/1173] [Iter 484] [G_Loss 4.600941] [D_Loss 0.111078]\n",
      "train [Epoch 0/10] [Batch 485/1173] [Iter 485] [G_Loss 5.292586] [D_Loss 0.110142]\n",
      "train [Epoch 0/10] [Batch 486/1173] [Iter 486] [G_Loss 5.170856] [D_Loss 0.039067]\n",
      "train [Epoch 0/10] [Batch 487/1173] [Iter 487] [G_Loss 4.846807] [D_Loss 0.035467]\n",
      "train [Epoch 0/10] [Batch 488/1173] [Iter 488] [G_Loss 5.381212] [D_Loss 0.129359]\n",
      "train [Epoch 0/10] [Batch 489/1173] [Iter 489] [G_Loss 5.613173] [D_Loss 0.049109]\n",
      "train [Epoch 0/10] [Batch 490/1173] [Iter 490] [G_Loss 5.283789] [D_Loss 0.046781]\n",
      "train [Epoch 0/10] [Batch 491/1173] [Iter 491] [G_Loss 5.412199] [D_Loss 0.025787]\n",
      "train [Epoch 0/10] [Batch 492/1173] [Iter 492] [G_Loss 5.624826] [D_Loss 0.015496]\n",
      "train [Epoch 0/10] [Batch 493/1173] [Iter 493] [G_Loss 6.037304] [D_Loss 0.016782]\n",
      "train [Epoch 0/10] [Batch 494/1173] [Iter 494] [G_Loss 6.262583] [D_Loss 0.145402]\n",
      "train [Epoch 0/10] [Batch 495/1173] [Iter 495] [G_Loss 5.559501] [D_Loss 0.019811]\n",
      "train [Epoch 0/10] [Batch 496/1173] [Iter 496] [G_Loss 4.986208] [D_Loss 0.023943]\n",
      "train [Epoch 0/10] [Batch 497/1173] [Iter 497] [G_Loss 5.091240] [D_Loss 0.052427]\n",
      "train [Epoch 0/10] [Batch 498/1173] [Iter 498] [G_Loss 5.191199] [D_Loss 0.053428]\n",
      "train [Epoch 0/10] [Batch 499/1173] [Iter 499] [G_Loss 5.369989] [D_Loss 0.024054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 500/1173] [Iter 500] [G_Loss 5.627161] [D_Loss 0.028684]\n",
      "train [Epoch 0/10] [Batch 501/1173] [Iter 501] [G_Loss 5.561035] [D_Loss 0.057582]\n",
      "train [Epoch 0/10] [Batch 502/1173] [Iter 502] [G_Loss 5.652781] [D_Loss 0.062978]\n",
      "train [Epoch 0/10] [Batch 503/1173] [Iter 503] [G_Loss 5.335539] [D_Loss 0.050799]\n",
      "train [Epoch 0/10] [Batch 504/1173] [Iter 504] [G_Loss 4.254573] [D_Loss 0.098439]\n",
      "train [Epoch 0/10] [Batch 505/1173] [Iter 505] [G_Loss 4.020305] [D_Loss 0.241347]\n",
      "train [Epoch 0/10] [Batch 506/1173] [Iter 506] [G_Loss 4.289789] [D_Loss 0.286270]\n",
      "train [Epoch 0/10] [Batch 507/1173] [Iter 507] [G_Loss 8.119051] [D_Loss 0.359231]\n",
      "train [Epoch 0/10] [Batch 508/1173] [Iter 508] [G_Loss 6.055717] [D_Loss 0.032122]\n",
      "train [Epoch 0/10] [Batch 509/1173] [Iter 509] [G_Loss 3.131901] [D_Loss 0.109895]\n",
      "train [Epoch 0/10] [Batch 510/1173] [Iter 510] [G_Loss 6.086740] [D_Loss 0.062333]\n",
      "train [Epoch 0/10] [Batch 511/1173] [Iter 511] [G_Loss 6.810036] [D_Loss 0.072846]\n",
      "train [Epoch 0/10] [Batch 512/1173] [Iter 512] [G_Loss 6.312502] [D_Loss 0.062347]\n",
      "train [Epoch 0/10] [Batch 513/1173] [Iter 513] [G_Loss 4.817284] [D_Loss 0.070623]\n",
      "train [Epoch 0/10] [Batch 514/1173] [Iter 514] [G_Loss 4.892557] [D_Loss 0.056109]\n",
      "train [Epoch 0/10] [Batch 515/1173] [Iter 515] [G_Loss 5.362070] [D_Loss 0.021478]\n",
      "train [Epoch 0/10] [Batch 516/1173] [Iter 516] [G_Loss 6.136837] [D_Loss 0.026728]\n",
      "train [Epoch 0/10] [Batch 517/1173] [Iter 517] [G_Loss 6.257101] [D_Loss 0.048835]\n",
      "train [Epoch 0/10] [Batch 518/1173] [Iter 518] [G_Loss 6.446936] [D_Loss 0.022431]\n",
      "train [Epoch 0/10] [Batch 519/1173] [Iter 519] [G_Loss 6.331585] [D_Loss 0.035261]\n",
      "train [Epoch 0/10] [Batch 520/1173] [Iter 520] [G_Loss 4.879702] [D_Loss 0.080347]\n",
      "train [Epoch 0/10] [Batch 521/1173] [Iter 521] [G_Loss 7.061606] [D_Loss 0.036321]\n",
      "train [Epoch 0/10] [Batch 522/1173] [Iter 522] [G_Loss 7.171215] [D_Loss 0.135207]\n",
      "train [Epoch 0/10] [Batch 523/1173] [Iter 523] [G_Loss 4.353421] [D_Loss 0.085598]\n",
      "train [Epoch 0/10] [Batch 524/1173] [Iter 524] [G_Loss 8.651211] [D_Loss 0.041818]\n",
      "train [Epoch 0/10] [Batch 525/1173] [Iter 525] [G_Loss 8.769061] [D_Loss 0.144374]\n",
      "train [Epoch 0/10] [Batch 526/1173] [Iter 526] [G_Loss 4.378579] [D_Loss 0.086276]\n",
      "train [Epoch 0/10] [Batch 527/1173] [Iter 527] [G_Loss 6.886475] [D_Loss 0.023781]\n",
      "train [Epoch 0/10] [Batch 528/1173] [Iter 528] [G_Loss 7.239022] [D_Loss 0.039200]\n",
      "train [Epoch 0/10] [Batch 529/1173] [Iter 529] [G_Loss 7.281711] [D_Loss 0.034839]\n",
      "train [Epoch 0/10] [Batch 530/1173] [Iter 530] [G_Loss 5.348217] [D_Loss 0.031342]\n",
      "train [Epoch 0/10] [Batch 531/1173] [Iter 531] [G_Loss 6.397540] [D_Loss 0.061949]\n",
      "train [Epoch 0/10] [Batch 532/1173] [Iter 532] [G_Loss 6.823280] [D_Loss 0.067949]\n",
      "train [Epoch 0/10] [Batch 533/1173] [Iter 533] [G_Loss 6.063841] [D_Loss 0.032714]\n",
      "train [Epoch 0/10] [Batch 534/1173] [Iter 534] [G_Loss 5.065392] [D_Loss 0.038013]\n",
      "train [Epoch 0/10] [Batch 535/1173] [Iter 535] [G_Loss 6.994352] [D_Loss 0.016799]\n",
      "train [Epoch 0/10] [Batch 536/1173] [Iter 536] [G_Loss 7.015663] [D_Loss 0.104325]\n",
      "train [Epoch 0/10] [Batch 537/1173] [Iter 537] [G_Loss 6.088097] [D_Loss 0.029994]\n",
      "train [Epoch 0/10] [Batch 538/1173] [Iter 538] [G_Loss 4.015913] [D_Loss 0.092089]\n",
      "train [Epoch 0/10] [Batch 539/1173] [Iter 539] [G_Loss 8.484270] [D_Loss 0.086901]\n",
      "train [Epoch 0/10] [Batch 540/1173] [Iter 540] [G_Loss 8.345803] [D_Loss 0.213168]\n",
      "train [Epoch 0/10] [Batch 541/1173] [Iter 541] [G_Loss 6.377578] [D_Loss 0.125715]\n",
      "train [Epoch 0/10] [Batch 542/1173] [Iter 542] [G_Loss 3.877507] [D_Loss 0.080485]\n",
      "train [Epoch 0/10] [Batch 543/1173] [Iter 543] [G_Loss 6.265242] [D_Loss 0.017252]\n",
      "train [Epoch 0/10] [Batch 544/1173] [Iter 544] [G_Loss 6.566428] [D_Loss 0.033459]\n",
      "train [Epoch 0/10] [Batch 545/1173] [Iter 545] [G_Loss 6.439001] [D_Loss 0.027785]\n",
      "train [Epoch 0/10] [Batch 546/1173] [Iter 546] [G_Loss 5.601633] [D_Loss 0.012847]\n",
      "train [Epoch 0/10] [Batch 547/1173] [Iter 547] [G_Loss 4.980395] [D_Loss 0.077715]\n",
      "train [Epoch 0/10] [Batch 548/1173] [Iter 548] [G_Loss 4.901198] [D_Loss 0.044883]\n",
      "train [Epoch 0/10] [Batch 549/1173] [Iter 549] [G_Loss 5.385035] [D_Loss 0.106434]\n",
      "train [Epoch 0/10] [Batch 550/1173] [Iter 550] [G_Loss 4.383127] [D_Loss 0.035504]\n",
      "train [Epoch 0/10] [Batch 551/1173] [Iter 551] [G_Loss 5.067141] [D_Loss 0.034255]\n",
      "train [Epoch 0/10] [Batch 552/1173] [Iter 552] [G_Loss 6.048120] [D_Loss 0.022685]\n",
      "train [Epoch 0/10] [Batch 553/1173] [Iter 553] [G_Loss 6.366318] [D_Loss 0.014734]\n",
      "train [Epoch 0/10] [Batch 554/1173] [Iter 554] [G_Loss 6.021845] [D_Loss 0.014225]\n",
      "train [Epoch 0/10] [Batch 555/1173] [Iter 555] [G_Loss 5.990354] [D_Loss 0.010064]\n",
      "train [Epoch 0/10] [Batch 556/1173] [Iter 556] [G_Loss 5.864744] [D_Loss 0.013969]\n",
      "train [Epoch 0/10] [Batch 557/1173] [Iter 557] [G_Loss 6.004322] [D_Loss 0.012608]\n",
      "train [Epoch 0/10] [Batch 558/1173] [Iter 558] [G_Loss 5.898090] [D_Loss 0.048432]\n",
      "train [Epoch 0/10] [Batch 559/1173] [Iter 559] [G_Loss 4.822371] [D_Loss 0.031964]\n",
      "train [Epoch 0/10] [Batch 560/1173] [Iter 560] [G_Loss 5.598937] [D_Loss 0.010147]\n",
      "train [Epoch 0/10] [Batch 561/1173] [Iter 561] [G_Loss 6.397833] [D_Loss 0.011125]\n",
      "train [Epoch 0/10] [Batch 562/1173] [Iter 562] [G_Loss 6.532303] [D_Loss 0.063570]\n",
      "train [Epoch 0/10] [Batch 563/1173] [Iter 563] [G_Loss 5.469934] [D_Loss 0.012912]\n",
      "train [Epoch 0/10] [Batch 564/1173] [Iter 564] [G_Loss 5.199903] [D_Loss 0.040372]\n",
      "train [Epoch 0/10] [Batch 565/1173] [Iter 565] [G_Loss 5.513010] [D_Loss 0.034515]\n",
      "train [Epoch 0/10] [Batch 566/1173] [Iter 566] [G_Loss 5.466318] [D_Loss 0.048548]\n",
      "train [Epoch 0/10] [Batch 567/1173] [Iter 567] [G_Loss 4.873473] [D_Loss 0.034510]\n",
      "train [Epoch 0/10] [Batch 568/1173] [Iter 568] [G_Loss 5.220398] [D_Loss 0.029096]\n",
      "train [Epoch 0/10] [Batch 569/1173] [Iter 569] [G_Loss 6.158714] [D_Loss 0.008446]\n",
      "train [Epoch 0/10] [Batch 570/1173] [Iter 570] [G_Loss 6.624197] [D_Loss 0.015063]\n",
      "train [Epoch 0/10] [Batch 571/1173] [Iter 571] [G_Loss 6.473684] [D_Loss 0.026229]\n",
      "train [Epoch 0/10] [Batch 572/1173] [Iter 572] [G_Loss 6.373043] [D_Loss 0.066957]\n",
      "train [Epoch 0/10] [Batch 573/1173] [Iter 573] [G_Loss 6.002687] [D_Loss 0.010233]\n",
      "train [Epoch 0/10] [Batch 574/1173] [Iter 574] [G_Loss 5.016258] [D_Loss 0.023070]\n",
      "train [Epoch 0/10] [Batch 575/1173] [Iter 575] [G_Loss 5.233765] [D_Loss 0.014567]\n",
      "train [Epoch 0/10] [Batch 576/1173] [Iter 576] [G_Loss 6.494287] [D_Loss 0.021629]\n",
      "train [Epoch 0/10] [Batch 577/1173] [Iter 577] [G_Loss 6.709716] [D_Loss 0.037436]\n",
      "train [Epoch 0/10] [Batch 578/1173] [Iter 578] [G_Loss 6.334695] [D_Loss 0.009866]\n",
      "train [Epoch 0/10] [Batch 579/1173] [Iter 579] [G_Loss 6.421082] [D_Loss 0.009164]\n",
      "train [Epoch 0/10] [Batch 580/1173] [Iter 580] [G_Loss 6.208592] [D_Loss 0.005609]\n",
      "train [Epoch 0/10] [Batch 581/1173] [Iter 581] [G_Loss 6.151875] [D_Loss 0.014261]\n",
      "train [Epoch 0/10] [Batch 582/1173] [Iter 582] [G_Loss 5.682081] [D_Loss 0.011438]\n",
      "train [Epoch 0/10] [Batch 583/1173] [Iter 583] [G_Loss 6.319845] [D_Loss 0.012467]\n",
      "train [Epoch 0/10] [Batch 584/1173] [Iter 584] [G_Loss 6.589262] [D_Loss 0.010608]\n",
      "train [Epoch 0/10] [Batch 585/1173] [Iter 585] [G_Loss 5.963007] [D_Loss 0.004742]\n",
      "train [Epoch 0/10] [Batch 586/1173] [Iter 586] [G_Loss 6.443772] [D_Loss 0.014115]\n",
      "train [Epoch 0/10] [Batch 587/1173] [Iter 587] [G_Loss 6.480186] [D_Loss 0.017030]\n",
      "train [Epoch 0/10] [Batch 591/1173] [Iter 591] [G_Loss 5.662761] [D_Loss 0.014370]\n",
      "train [Epoch 0/10] [Batch 592/1173] [Iter 592] [G_Loss 6.045365] [D_Loss 0.007100]\n",
      "train [Epoch 0/10] [Batch 593/1173] [Iter 593] [G_Loss 6.216703] [D_Loss 0.115260]\n",
      "train [Epoch 0/10] [Batch 594/1173] [Iter 594] [G_Loss 4.207660] [D_Loss 0.047680]\n",
      "train [Epoch 0/10] [Batch 595/1173] [Iter 595] [G_Loss 6.246358] [D_Loss 0.007487]\n",
      "train [Epoch 0/10] [Batch 596/1173] [Iter 596] [G_Loss 7.710866] [D_Loss 0.091125]\n",
      "train [Epoch 0/10] [Batch 597/1173] [Iter 597] [G_Loss 6.639529] [D_Loss 0.118264]\n",
      "train [Epoch 0/10] [Batch 598/1173] [Iter 598] [G_Loss 3.570606] [D_Loss 0.118970]\n",
      "train [Epoch 0/10] [Batch 599/1173] [Iter 599] [G_Loss 11.277252] [D_Loss 0.050869]\n",
      "train [Epoch 0/10] [Batch 600/1173] [Iter 600] [G_Loss 12.478925] [D_Loss 0.181831]\n",
      "train [Epoch 0/10] [Batch 601/1173] [Iter 601] [G_Loss 9.295721] [D_Loss 0.054271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 602/1173] [Iter 602] [G_Loss 6.723105] [D_Loss 0.003369]\n",
      "train [Epoch 0/10] [Batch 603/1173] [Iter 603] [G_Loss 6.152917] [D_Loss 0.007940]\n",
      "train [Epoch 0/10] [Batch 604/1173] [Iter 604] [G_Loss 5.560339] [D_Loss 0.019515]\n",
      "train [Epoch 0/10] [Batch 605/1173] [Iter 605] [G_Loss 6.595509] [D_Loss 0.005873]\n",
      "train [Epoch 0/10] [Batch 606/1173] [Iter 606] [G_Loss 6.585356] [D_Loss 0.009348]\n",
      "train [Epoch 0/10] [Batch 607/1173] [Iter 607] [G_Loss 7.736100] [D_Loss 0.024549]\n",
      "train [Epoch 0/10] [Batch 608/1173] [Iter 608] [G_Loss 6.448489] [D_Loss 0.011093]\n",
      "train [Epoch 0/10] [Batch 609/1173] [Iter 609] [G_Loss 6.420691] [D_Loss 0.010438]\n",
      "train [Epoch 0/10] [Batch 610/1173] [Iter 610] [G_Loss 7.210567] [D_Loss 0.021283]\n",
      "train [Epoch 0/10] [Batch 611/1173] [Iter 611] [G_Loss 7.200096] [D_Loss 0.004602]\n",
      "train [Epoch 0/10] [Batch 612/1173] [Iter 612] [G_Loss 7.342530] [D_Loss 0.025826]\n",
      "train [Epoch 0/10] [Batch 613/1173] [Iter 613] [G_Loss 6.790118] [D_Loss 0.010499]\n",
      "train [Epoch 0/10] [Batch 614/1173] [Iter 614] [G_Loss 6.583258] [D_Loss 0.011926]\n",
      "train [Epoch 0/10] [Batch 615/1173] [Iter 615] [G_Loss 5.825882] [D_Loss 0.015479]\n",
      "train [Epoch 0/10] [Batch 616/1173] [Iter 616] [G_Loss 8.971534] [D_Loss 0.047629]\n",
      "train [Epoch 0/10] [Batch 617/1173] [Iter 617] [G_Loss 6.580445] [D_Loss 0.077671]\n",
      "train [Epoch 0/10] [Batch 618/1173] [Iter 618] [G_Loss 11.143734] [D_Loss 0.001522]\n",
      "train [Epoch 0/10] [Batch 619/1173] [Iter 619] [G_Loss 12.267022] [D_Loss 0.124097]\n",
      "train [Epoch 0/10] [Batch 620/1173] [Iter 620] [G_Loss 9.224005] [D_Loss 0.048142]\n",
      "train [Epoch 0/10] [Batch 621/1173] [Iter 621] [G_Loss 5.529297] [D_Loss 0.016512]\n",
      "train [Epoch 0/10] [Batch 622/1173] [Iter 622] [G_Loss 5.028962] [D_Loss 0.066432]\n",
      "train [Epoch 0/10] [Batch 623/1173] [Iter 623] [G_Loss 10.224832] [D_Loss 0.010195]\n",
      "train [Epoch 0/10] [Batch 624/1173] [Iter 624] [G_Loss 11.863361] [D_Loss 0.000808]\n",
      "train [Epoch 0/10] [Batch 625/1173] [Iter 625] [G_Loss 11.774693] [D_Loss 0.004428]\n",
      "train [Epoch 0/10] [Batch 626/1173] [Iter 626] [G_Loss 11.291373] [D_Loss 0.022912]\n",
      "train [Epoch 0/10] [Batch 627/1173] [Iter 627] [G_Loss 10.934055] [D_Loss 0.002456]\n",
      "train [Epoch 0/10] [Batch 628/1173] [Iter 628] [G_Loss 11.454714] [D_Loss 0.010706]\n",
      "train [Epoch 0/10] [Batch 629/1173] [Iter 629] [G_Loss 10.545306] [D_Loss 0.024565]\n",
      "train [Epoch 0/10] [Batch 630/1173] [Iter 630] [G_Loss 6.397577] [D_Loss 0.019281]\n",
      "train [Epoch 0/10] [Batch 631/1173] [Iter 631] [G_Loss 4.392896] [D_Loss 0.063165]\n",
      "train [Epoch 0/10] [Batch 632/1173] [Iter 632] [G_Loss 13.763321] [D_Loss 0.135007]\n",
      "train [Epoch 0/10] [Batch 633/1173] [Iter 633] [G_Loss 13.278984] [D_Loss 0.118884]\n",
      "train [Epoch 0/10] [Batch 634/1173] [Iter 634] [G_Loss 9.759994] [D_Loss 0.005642]\n",
      "train [Epoch 0/10] [Batch 635/1173] [Iter 635] [G_Loss 7.243546] [D_Loss 0.015447]\n",
      "train [Epoch 0/10] [Batch 636/1173] [Iter 636] [G_Loss 5.991502] [D_Loss 0.025562]\n",
      "train [Epoch 0/10] [Batch 637/1173] [Iter 637] [G_Loss 7.545091] [D_Loss 0.007051]\n",
      "train [Epoch 0/10] [Batch 638/1173] [Iter 638] [G_Loss 6.503456] [D_Loss 0.005396]\n",
      "train [Epoch 0/10] [Batch 639/1173] [Iter 639] [G_Loss 5.425934] [D_Loss 0.019055]\n",
      "train [Epoch 0/10] [Batch 640/1173] [Iter 640] [G_Loss 9.462745] [D_Loss 0.000603]\n",
      "train [Epoch 0/10] [Batch 641/1173] [Iter 641] [G_Loss 9.881295] [D_Loss 0.011128]\n",
      "train [Epoch 0/10] [Batch 642/1173] [Iter 642] [G_Loss 10.792216] [D_Loss 0.000876]\n",
      "train [Epoch 0/10] [Batch 643/1173] [Iter 643] [G_Loss 10.829635] [D_Loss 0.001637]\n",
      "train [Epoch 0/10] [Batch 644/1173] [Iter 644] [G_Loss 10.419617] [D_Loss 0.000412]\n",
      "train [Epoch 0/10] [Batch 645/1173] [Iter 645] [G_Loss 9.793847] [D_Loss 0.046699]\n",
      "train [Epoch 0/10] [Batch 646/1173] [Iter 646] [G_Loss 8.611902] [D_Loss 0.000890]\n",
      "train [Epoch 0/10] [Batch 647/1173] [Iter 647] [G_Loss 7.325404] [D_Loss 0.004108]\n",
      "train [Epoch 0/10] [Batch 648/1173] [Iter 648] [G_Loss 7.253625] [D_Loss 0.003449]\n",
      "train [Epoch 0/10] [Batch 649/1173] [Iter 649] [G_Loss 9.830339] [D_Loss 0.011526]\n",
      "train [Epoch 0/10] [Batch 650/1173] [Iter 650] [G_Loss 6.287912] [D_Loss 0.017106]\n",
      "train [Epoch 0/10] [Batch 651/1173] [Iter 651] [G_Loss 7.555943] [D_Loss 0.005559]\n",
      "train [Epoch 0/10] [Batch 652/1173] [Iter 652] [G_Loss 7.165847] [D_Loss 0.004935]\n",
      "train [Epoch 0/10] [Batch 653/1173] [Iter 653] [G_Loss 7.574601] [D_Loss 0.055402]\n",
      "train [Epoch 0/10] [Batch 654/1173] [Iter 654] [G_Loss 4.796290] [D_Loss 0.103223]\n",
      "train [Epoch 0/10] [Batch 655/1173] [Iter 655] [G_Loss 18.270498] [D_Loss 0.434548]\n",
      "train [Epoch 0/10] [Batch 656/1173] [Iter 656] [G_Loss 15.181562] [D_Loss 0.280327]\n",
      "train [Epoch 0/10] [Batch 657/1173] [Iter 657] [G_Loss 9.605698] [D_Loss 0.077419]\n",
      "train [Epoch 0/10] [Batch 658/1173] [Iter 658] [G_Loss 6.658019] [D_Loss 0.025911]\n",
      "train [Epoch 0/10] [Batch 659/1173] [Iter 659] [G_Loss 4.445661] [D_Loss 0.027011]\n",
      "train [Epoch 0/10] [Batch 660/1173] [Iter 660] [G_Loss 5.311755] [D_Loss 0.012134]\n",
      "train [Epoch 0/10] [Batch 661/1173] [Iter 661] [G_Loss 6.137887] [D_Loss 0.004849]\n",
      "train [Epoch 0/10] [Batch 662/1173] [Iter 662] [G_Loss 7.181037] [D_Loss 0.008391]\n",
      "train [Epoch 0/10] [Batch 663/1173] [Iter 663] [G_Loss 7.391731] [D_Loss 0.001780]\n",
      "train [Epoch 0/10] [Batch 664/1173] [Iter 664] [G_Loss 7.920122] [D_Loss 0.003038]\n",
      "train [Epoch 0/10] [Batch 665/1173] [Iter 665] [G_Loss 7.470789] [D_Loss 0.005468]\n",
      "train [Epoch 0/10] [Batch 666/1173] [Iter 666] [G_Loss 7.267076] [D_Loss 0.168889]\n",
      "train [Epoch 0/10] [Batch 667/1173] [Iter 667] [G_Loss 4.159819] [D_Loss 0.040994]\n",
      "train [Epoch 0/10] [Batch 668/1173] [Iter 668] [G_Loss 6.572489] [D_Loss 0.015875]\n",
      "train [Epoch 0/10] [Batch 669/1173] [Iter 669] [G_Loss 7.585090] [D_Loss 0.003565]\n",
      "train [Epoch 0/10] [Batch 670/1173] [Iter 670] [G_Loss 6.399024] [D_Loss 0.003512]\n",
      "train [Epoch 0/10] [Batch 671/1173] [Iter 671] [G_Loss 6.934864] [D_Loss 0.003779]\n",
      "train [Epoch 0/10] [Batch 672/1173] [Iter 672] [G_Loss 6.870616] [D_Loss 0.045462]\n",
      "train [Epoch 0/10] [Batch 673/1173] [Iter 673] [G_Loss 6.194282] [D_Loss 0.005847]\n",
      "train [Epoch 0/10] [Batch 674/1173] [Iter 674] [G_Loss 6.755943] [D_Loss 0.067842]\n",
      "train [Epoch 0/10] [Batch 675/1173] [Iter 675] [G_Loss 4.075414] [D_Loss 0.041286]\n",
      "train [Epoch 0/10] [Batch 676/1173] [Iter 676] [G_Loss 7.504794] [D_Loss 0.078441]\n",
      "train [Epoch 0/10] [Batch 677/1173] [Iter 677] [G_Loss 7.875986] [D_Loss 0.041508]\n",
      "train [Epoch 0/10] [Batch 678/1173] [Iter 678] [G_Loss 6.263638] [D_Loss 0.004776]\n",
      "train [Epoch 0/10] [Batch 679/1173] [Iter 679] [G_Loss 5.748793] [D_Loss 0.010894]\n",
      "train [Epoch 0/10] [Batch 680/1173] [Iter 680] [G_Loss 6.686441] [D_Loss 0.004620]\n",
      "train [Epoch 0/10] [Batch 681/1173] [Iter 681] [G_Loss 8.013762] [D_Loss 0.001418]\n",
      "train [Epoch 0/10] [Batch 682/1173] [Iter 682] [G_Loss 8.110904] [D_Loss 0.001664]\n",
      "train [Epoch 0/10] [Batch 683/1173] [Iter 683] [G_Loss 8.622484] [D_Loss 0.029589]\n",
      "train [Epoch 0/10] [Batch 684/1173] [Iter 684] [G_Loss 6.618734] [D_Loss 0.020933]\n",
      "train [Epoch 0/10] [Batch 685/1173] [Iter 685] [G_Loss 7.954324] [D_Loss 0.017036]\n",
      "train [Epoch 0/10] [Batch 686/1173] [Iter 686] [G_Loss 6.853783] [D_Loss 0.004159]\n",
      "train [Epoch 0/10] [Batch 687/1173] [Iter 687] [G_Loss 7.181062] [D_Loss 0.006869]\n",
      "train [Epoch 0/10] [Batch 688/1173] [Iter 688] [G_Loss 7.149271] [D_Loss 0.004811]\n",
      "train [Epoch 0/10] [Batch 689/1173] [Iter 689] [G_Loss 7.787025] [D_Loss 0.002916]\n",
      "train [Epoch 0/10] [Batch 690/1173] [Iter 690] [G_Loss 7.960167] [D_Loss 0.052406]\n",
      "train [Epoch 0/10] [Batch 691/1173] [Iter 691] [G_Loss 6.180161] [D_Loss 0.028364]\n",
      "train [Epoch 0/10] [Batch 692/1173] [Iter 692] [G_Loss 6.957699] [D_Loss 0.015813]\n",
      "train [Epoch 0/10] [Batch 693/1173] [Iter 693] [G_Loss 6.755096] [D_Loss 0.017790]\n",
      "train [Epoch 0/10] [Batch 694/1173] [Iter 694] [G_Loss 8.075586] [D_Loss 0.006417]\n",
      "train [Epoch 0/10] [Batch 695/1173] [Iter 695] [G_Loss 8.994300] [D_Loss 0.004798]\n",
      "train [Epoch 0/10] [Batch 696/1173] [Iter 696] [G_Loss 9.911924] [D_Loss 0.001834]\n",
      "train [Epoch 0/10] [Batch 697/1173] [Iter 697] [G_Loss 10.073022] [D_Loss 0.047444]\n",
      "train [Epoch 0/10] [Batch 698/1173] [Iter 698] [G_Loss 7.966167] [D_Loss 0.075393]\n",
      "train [Epoch 0/10] [Batch 699/1173] [Iter 699] [G_Loss 5.832085] [D_Loss 0.050284]\n",
      "train [Epoch 0/10] [Batch 700/1173] [Iter 700] [G_Loss 9.661720] [D_Loss 0.010552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 701/1173] [Iter 701] [G_Loss 10.404201] [D_Loss 0.002342]\n",
      "train [Epoch 0/10] [Batch 702/1173] [Iter 702] [G_Loss 10.977517] [D_Loss 0.000852]\n",
      "train [Epoch 0/10] [Batch 703/1173] [Iter 703] [G_Loss 10.586782] [D_Loss 0.017514]\n",
      "train [Epoch 0/10] [Batch 704/1173] [Iter 704] [G_Loss 9.473842] [D_Loss 0.022955]\n",
      "train [Epoch 0/10] [Batch 705/1173] [Iter 705] [G_Loss 7.459170] [D_Loss 0.019317]\n",
      "train [Epoch 0/10] [Batch 706/1173] [Iter 706] [G_Loss 8.168605] [D_Loss 0.029464]\n",
      "train [Epoch 0/10] [Batch 707/1173] [Iter 707] [G_Loss 8.290555] [D_Loss 0.006897]\n",
      "train [Epoch 0/10] [Batch 708/1173] [Iter 708] [G_Loss 7.173402] [D_Loss 0.030355]\n",
      "train [Epoch 0/10] [Batch 709/1173] [Iter 709] [G_Loss 11.552355] [D_Loss 0.010059]\n",
      "train [Epoch 0/10] [Batch 710/1173] [Iter 710] [G_Loss 12.137691] [D_Loss 0.039948]\n",
      "train [Epoch 0/10] [Batch 711/1173] [Iter 711] [G_Loss 11.023128] [D_Loss 0.008389]\n",
      "train [Epoch 0/10] [Batch 712/1173] [Iter 712] [G_Loss 9.765117] [D_Loss 0.003173]\n",
      "train [Epoch 0/10] [Batch 713/1173] [Iter 713] [G_Loss 9.401491] [D_Loss 0.017363]\n",
      "train [Epoch 0/10] [Batch 714/1173] [Iter 714] [G_Loss 6.122370] [D_Loss 0.061920]\n",
      "train [Epoch 0/10] [Batch 715/1173] [Iter 715] [G_Loss 15.648603] [D_Loss 0.084332]\n",
      "train [Epoch 0/10] [Batch 716/1173] [Iter 716] [G_Loss 15.493426] [D_Loss 0.056679]\n",
      "train [Epoch 0/10] [Batch 717/1173] [Iter 717] [G_Loss 13.633491] [D_Loss 0.044437]\n",
      "train [Epoch 0/10] [Batch 718/1173] [Iter 718] [G_Loss 11.903486] [D_Loss 0.001733]\n",
      "train [Epoch 0/10] [Batch 719/1173] [Iter 719] [G_Loss 10.696742] [D_Loss 0.002738]\n",
      "train [Epoch 0/10] [Batch 720/1173] [Iter 720] [G_Loss 8.133884] [D_Loss 0.007007]\n",
      "train [Epoch 0/10] [Batch 721/1173] [Iter 721] [G_Loss 6.805416] [D_Loss 0.032009]\n",
      "train [Epoch 0/10] [Batch 722/1173] [Iter 722] [G_Loss 13.630371] [D_Loss 0.165367]\n",
      "train [Epoch 0/10] [Batch 723/1173] [Iter 723] [G_Loss 13.676595] [D_Loss 0.092159]\n",
      "train [Epoch 0/10] [Batch 724/1173] [Iter 724] [G_Loss 8.749289] [D_Loss 0.141116]\n",
      "train [Epoch 0/10] [Batch 725/1173] [Iter 725] [G_Loss 4.754681] [D_Loss 0.133647]\n",
      "train [Epoch 0/10] [Batch 726/1173] [Iter 726] [G_Loss 12.460245] [D_Loss 0.022036]\n",
      "train [Epoch 0/10] [Batch 727/1173] [Iter 727] [G_Loss 12.873703] [D_Loss 0.000222]\n",
      "train [Epoch 0/10] [Batch 728/1173] [Iter 728] [G_Loss 11.142144] [D_Loss 0.000815]\n",
      "train [Epoch 0/10] [Batch 729/1173] [Iter 729] [G_Loss 10.562709] [D_Loss 0.000159]\n",
      "train [Epoch 0/10] [Batch 730/1173] [Iter 730] [G_Loss 8.151819] [D_Loss 0.010069]\n",
      "train [Epoch 0/10] [Batch 731/1173] [Iter 731] [G_Loss 8.089199] [D_Loss 0.063733]\n",
      "train [Epoch 0/10] [Batch 732/1173] [Iter 732] [G_Loss 8.803389] [D_Loss 0.071425]\n",
      "train [Epoch 0/10] [Batch 733/1173] [Iter 733] [G_Loss 8.452659] [D_Loss 0.044381]\n",
      "train [Epoch 0/10] [Batch 734/1173] [Iter 734] [G_Loss 7.247767] [D_Loss 0.014584]\n",
      "train [Epoch 0/10] [Batch 735/1173] [Iter 735] [G_Loss 8.699389] [D_Loss 0.001883]\n",
      "train [Epoch 0/10] [Batch 736/1173] [Iter 736] [G_Loss 9.121411] [D_Loss 0.001440]\n",
      "train [Epoch 0/10] [Batch 737/1173] [Iter 737] [G_Loss 9.704757] [D_Loss 0.004613]\n",
      "train [Epoch 0/10] [Batch 738/1173] [Iter 738] [G_Loss 8.958810] [D_Loss 0.020181]\n",
      "train [Epoch 0/10] [Batch 739/1173] [Iter 739] [G_Loss 8.648255] [D_Loss 0.016964]\n",
      "train [Epoch 0/10] [Batch 740/1173] [Iter 740] [G_Loss 7.660429] [D_Loss 0.053083]\n",
      "train [Epoch 0/10] [Batch 741/1173] [Iter 741] [G_Loss 6.568905] [D_Loss 0.019665]\n",
      "train [Epoch 0/10] [Batch 742/1173] [Iter 742] [G_Loss 8.402991] [D_Loss 0.001554]\n",
      "train [Epoch 0/10] [Batch 743/1173] [Iter 743] [G_Loss 9.175238] [D_Loss 0.000909]\n",
      "train [Epoch 0/10] [Batch 744/1173] [Iter 744] [G_Loss 9.000722] [D_Loss 0.001217]\n",
      "train [Epoch 0/10] [Batch 745/1173] [Iter 745] [G_Loss 9.908518] [D_Loss 0.053927]\n",
      "train [Epoch 0/10] [Batch 746/1173] [Iter 746] [G_Loss 7.725759] [D_Loss 0.002852]\n",
      "train [Epoch 0/10] [Batch 747/1173] [Iter 747] [G_Loss 7.966799] [D_Loss 0.001868]\n",
      "train [Epoch 0/10] [Batch 748/1173] [Iter 748] [G_Loss 7.594377] [D_Loss 0.003437]\n",
      "train [Epoch 0/10] [Batch 749/1173] [Iter 749] [G_Loss 8.242542] [D_Loss 0.020415]\n",
      "train [Epoch 0/10] [Batch 750/1173] [Iter 750] [G_Loss 7.144124] [D_Loss 0.003608]\n",
      "train [Epoch 0/10] [Batch 751/1173] [Iter 751] [G_Loss 7.430737] [D_Loss 0.011743]\n",
      "train [Epoch 0/10] [Batch 752/1173] [Iter 752] [G_Loss 7.351327] [D_Loss 0.002467]\n",
      "train [Epoch 0/10] [Batch 753/1173] [Iter 753] [G_Loss 7.292800] [D_Loss 0.005171]\n",
      "train [Epoch 0/10] [Batch 754/1173] [Iter 754] [G_Loss 8.574681] [D_Loss 0.002810]\n",
      "train [Epoch 0/10] [Batch 755/1173] [Iter 755] [G_Loss 8.751526] [D_Loss 0.015191]\n",
      "train [Epoch 0/10] [Batch 756/1173] [Iter 756] [G_Loss 9.559854] [D_Loss 0.003577]\n",
      "train [Epoch 0/10] [Batch 757/1173] [Iter 757] [G_Loss 8.511991] [D_Loss 0.002700]\n",
      "train [Epoch 0/10] [Batch 758/1173] [Iter 758] [G_Loss 8.584073] [D_Loss 0.063447]\n",
      "train [Epoch 0/10] [Batch 759/1173] [Iter 759] [G_Loss 7.051559] [D_Loss 0.131258]\n",
      "train [Epoch 0/10] [Batch 760/1173] [Iter 760] [G_Loss 5.090859] [D_Loss 0.043192]\n",
      "train [Epoch 0/10] [Batch 761/1173] [Iter 761] [G_Loss 9.146336] [D_Loss 0.001151]\n",
      "train [Epoch 0/10] [Batch 762/1173] [Iter 762] [G_Loss 10.461511] [D_Loss 0.010739]\n",
      "train [Epoch 0/10] [Batch 763/1173] [Iter 763] [G_Loss 10.622683] [D_Loss 0.002774]\n",
      "train [Epoch 0/10] [Batch 764/1173] [Iter 764] [G_Loss 9.523701] [D_Loss 0.005513]\n",
      "train [Epoch 0/10] [Batch 765/1173] [Iter 765] [G_Loss 9.269409] [D_Loss 0.048435]\n",
      "train [Epoch 0/10] [Batch 766/1173] [Iter 766] [G_Loss 7.579215] [D_Loss 0.006640]\n",
      "train [Epoch 0/10] [Batch 767/1173] [Iter 767] [G_Loss 6.636077] [D_Loss 0.006321]\n",
      "train [Epoch 0/10] [Batch 768/1173] [Iter 768] [G_Loss 6.609331] [D_Loss 0.011185]\n",
      "train [Epoch 0/10] [Batch 769/1173] [Iter 769] [G_Loss 7.313048] [D_Loss 0.003723]\n",
      "train [Epoch 0/10] [Batch 770/1173] [Iter 770] [G_Loss 7.482625] [D_Loss 0.014059]\n",
      "train [Epoch 0/10] [Batch 771/1173] [Iter 771] [G_Loss 6.675654] [D_Loss 0.004459]\n",
      "train [Epoch 0/10] [Batch 772/1173] [Iter 772] [G_Loss 7.342107] [D_Loss 0.003851]\n",
      "train [Epoch 0/10] [Batch 773/1173] [Iter 773] [G_Loss 7.725748] [D_Loss 0.008117]\n",
      "train [Epoch 0/10] [Batch 774/1173] [Iter 774] [G_Loss 7.458088] [D_Loss 0.040007]\n",
      "train [Epoch 0/10] [Batch 775/1173] [Iter 775] [G_Loss 6.081510] [D_Loss 0.008531]\n",
      "train [Epoch 0/10] [Batch 776/1173] [Iter 776] [G_Loss 6.590459] [D_Loss 0.065189]\n",
      "train [Epoch 0/10] [Batch 777/1173] [Iter 777] [G_Loss 8.533450] [D_Loss 0.002407]\n",
      "train [Epoch 0/10] [Batch 778/1173] [Iter 778] [G_Loss 10.102810] [D_Loss 0.000362]\n",
      "train [Epoch 0/10] [Batch 779/1173] [Iter 779] [G_Loss 10.171610] [D_Loss 0.101581]\n",
      "train [Epoch 0/10] [Batch 780/1173] [Iter 780] [G_Loss 7.775960] [D_Loss 0.015029]\n",
      "train [Epoch 0/10] [Batch 781/1173] [Iter 781] [G_Loss 6.411978] [D_Loss 0.011541]\n",
      "train [Epoch 0/10] [Batch 782/1173] [Iter 782] [G_Loss 6.196536] [D_Loss 0.011043]\n",
      "train [Epoch 0/10] [Batch 783/1173] [Iter 783] [G_Loss 7.637987] [D_Loss 0.003819]\n",
      "train [Epoch 0/10] [Batch 784/1173] [Iter 784] [G_Loss 8.573192] [D_Loss 0.012101]\n",
      "train [Epoch 0/10] [Batch 785/1173] [Iter 785] [G_Loss 8.836633] [D_Loss 0.002285]\n",
      "train [Epoch 0/10] [Batch 786/1173] [Iter 786] [G_Loss 8.932556] [D_Loss 0.000689]\n",
      "train [Epoch 0/10] [Batch 787/1173] [Iter 787] [G_Loss 8.826045] [D_Loss 0.000762]\n",
      "train [Epoch 0/10] [Batch 788/1173] [Iter 788] [G_Loss 8.972253] [D_Loss 0.008315]\n",
      "train [Epoch 0/10] [Batch 789/1173] [Iter 789] [G_Loss 8.534209] [D_Loss 0.000846]\n",
      "train [Epoch 0/10] [Batch 790/1173] [Iter 790] [G_Loss 8.427838] [D_Loss 0.000659]\n",
      "train [Epoch 0/10] [Batch 791/1173] [Iter 791] [G_Loss 7.996679] [D_Loss 0.002853]\n",
      "train [Epoch 0/10] [Batch 792/1173] [Iter 792] [G_Loss 8.312749] [D_Loss 0.001695]\n",
      "train [Epoch 0/10] [Batch 793/1173] [Iter 793] [G_Loss 8.660844] [D_Loss 0.003997]\n",
      "train [Epoch 0/10] [Batch 794/1173] [Iter 794] [G_Loss 8.692751] [D_Loss 0.071874]\n",
      "train [Epoch 0/10] [Batch 795/1173] [Iter 795] [G_Loss 6.843882] [D_Loss 0.005898]\n",
      "train [Epoch 0/10] [Batch 796/1173] [Iter 796] [G_Loss 6.195149] [D_Loss 0.007715]\n",
      "train [Epoch 0/10] [Batch 797/1173] [Iter 797] [G_Loss 7.304980] [D_Loss 0.006320]\n",
      "train [Epoch 0/10] [Batch 798/1173] [Iter 798] [G_Loss 7.025069] [D_Loss 0.012874]\n",
      "train [Epoch 0/10] [Batch 799/1173] [Iter 799] [G_Loss 7.656514] [D_Loss 0.018288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 800/1173] [Iter 800] [G_Loss 10.149006] [D_Loss 0.000928]\n",
      "train [Epoch 0/10] [Batch 801/1173] [Iter 801] [G_Loss 11.632154] [D_Loss 0.079637]\n",
      "train [Epoch 0/10] [Batch 802/1173] [Iter 802] [G_Loss 10.886811] [D_Loss 0.000275]\n",
      "train [Epoch 0/10] [Batch 803/1173] [Iter 803] [G_Loss 9.822719] [D_Loss 0.002087]\n",
      "train [Epoch 0/10] [Batch 804/1173] [Iter 804] [G_Loss 8.876369] [D_Loss 0.003053]\n",
      "train [Epoch 0/10] [Batch 805/1173] [Iter 805] [G_Loss 10.216339] [D_Loss 0.003661]\n",
      "train [Epoch 0/10] [Batch 806/1173] [Iter 806] [G_Loss 8.535340] [D_Loss 0.002091]\n",
      "train [Epoch 0/10] [Batch 807/1173] [Iter 807] [G_Loss 8.296196] [D_Loss 0.006000]\n",
      "train [Epoch 0/10] [Batch 808/1173] [Iter 808] [G_Loss 9.278138] [D_Loss 0.003064]\n",
      "train [Epoch 0/10] [Batch 809/1173] [Iter 809] [G_Loss 9.630798] [D_Loss 0.061664]\n",
      "train [Epoch 0/10] [Batch 810/1173] [Iter 810] [G_Loss 7.620356] [D_Loss 0.006051]\n",
      "train [Epoch 0/10] [Batch 811/1173] [Iter 811] [G_Loss 8.279477] [D_Loss 0.001650]\n",
      "train [Epoch 0/10] [Batch 812/1173] [Iter 812] [G_Loss 8.019169] [D_Loss 0.003620]\n",
      "train [Epoch 0/10] [Batch 813/1173] [Iter 813] [G_Loss 8.020020] [D_Loss 0.003704]\n",
      "train [Epoch 0/10] [Batch 814/1173] [Iter 814] [G_Loss 8.159608] [D_Loss 0.038188]\n",
      "train [Epoch 0/10] [Batch 815/1173] [Iter 815] [G_Loss 7.735179] [D_Loss 0.005887]\n",
      "train [Epoch 0/10] [Batch 816/1173] [Iter 816] [G_Loss 8.210601] [D_Loss 0.047001]\n",
      "train [Epoch 0/10] [Batch 817/1173] [Iter 817] [G_Loss 7.247197] [D_Loss 0.005390]\n",
      "train [Epoch 0/10] [Batch 818/1173] [Iter 818] [G_Loss 7.287090] [D_Loss 0.011991]\n",
      "train [Epoch 0/10] [Batch 819/1173] [Iter 819] [G_Loss 8.551712] [D_Loss 0.010174]\n",
      "train [Epoch 0/10] [Batch 820/1173] [Iter 820] [G_Loss 8.329367] [D_Loss 0.002413]\n",
      "train [Epoch 0/10] [Batch 821/1173] [Iter 821] [G_Loss 7.898989] [D_Loss 0.010098]\n",
      "train [Epoch 0/10] [Batch 822/1173] [Iter 822] [G_Loss 8.643183] [D_Loss 0.008517]\n",
      "train [Epoch 0/10] [Batch 823/1173] [Iter 823] [G_Loss 7.437948] [D_Loss 0.007636]\n",
      "train [Epoch 0/10] [Batch 824/1173] [Iter 824] [G_Loss 8.297736] [D_Loss 0.004361]\n",
      "train [Epoch 0/10] [Batch 825/1173] [Iter 825] [G_Loss 8.772199] [D_Loss 0.003626]\n",
      "train [Epoch 0/10] [Batch 826/1173] [Iter 826] [G_Loss 9.020040] [D_Loss 0.002235]\n",
      "train [Epoch 0/10] [Batch 827/1173] [Iter 827] [G_Loss 9.660316] [D_Loss 0.001109]\n",
      "train [Epoch 0/10] [Batch 828/1173] [Iter 828] [G_Loss 9.549024] [D_Loss 0.005226]\n",
      "train [Epoch 0/10] [Batch 829/1173] [Iter 829] [G_Loss 9.156674] [D_Loss 0.004654]\n",
      "train [Epoch 0/10] [Batch 830/1173] [Iter 830] [G_Loss 8.410849] [D_Loss 0.014072]\n",
      "train [Epoch 0/10] [Batch 831/1173] [Iter 831] [G_Loss 6.971238] [D_Loss 0.033895]\n",
      "train [Epoch 0/10] [Batch 832/1173] [Iter 832] [G_Loss 8.588752] [D_Loss 0.001352]\n",
      "train [Epoch 0/10] [Batch 833/1173] [Iter 833] [G_Loss 9.737043] [D_Loss 0.000418]\n",
      "train [Epoch 0/10] [Batch 834/1173] [Iter 834] [G_Loss 10.246500] [D_Loss 0.000490]\n",
      "train [Epoch 0/10] [Batch 835/1173] [Iter 835] [G_Loss 10.312348] [D_Loss 0.000264]\n",
      "train [Epoch 0/10] [Batch 836/1173] [Iter 836] [G_Loss 10.352612] [D_Loss 0.000375]\n",
      "train [Epoch 0/10] [Batch 837/1173] [Iter 837] [G_Loss 9.860435] [D_Loss 0.000659]\n",
      "train [Epoch 0/10] [Batch 838/1173] [Iter 838] [G_Loss 9.855697] [D_Loss 0.010081]\n",
      "train [Epoch 0/10] [Batch 839/1173] [Iter 839] [G_Loss 8.894206] [D_Loss 0.039686]\n",
      "train [Epoch 0/10] [Batch 840/1173] [Iter 840] [G_Loss 8.521546] [D_Loss 0.002300]\n",
      "train [Epoch 0/10] [Batch 841/1173] [Iter 841] [G_Loss 7.762372] [D_Loss 0.002682]\n",
      "train [Epoch 0/10] [Batch 842/1173] [Iter 842] [G_Loss 8.009221] [D_Loss 0.001673]\n",
      "train [Epoch 0/10] [Batch 843/1173] [Iter 843] [G_Loss 7.965526] [D_Loss 0.011919]\n",
      "train [Epoch 0/10] [Batch 844/1173] [Iter 844] [G_Loss 7.646221] [D_Loss 0.005906]\n",
      "train [Epoch 0/10] [Batch 845/1173] [Iter 845] [G_Loss 8.223460] [D_Loss 0.003505]\n",
      "train [Epoch 0/10] [Batch 846/1173] [Iter 846] [G_Loss 8.810043] [D_Loss 0.000881]\n",
      "train [Epoch 0/10] [Batch 847/1173] [Iter 847] [G_Loss 9.117581] [D_Loss 0.001266]\n",
      "train [Epoch 0/10] [Batch 848/1173] [Iter 848] [G_Loss 9.915133] [D_Loss 0.001182]\n",
      "train [Epoch 0/10] [Batch 849/1173] [Iter 849] [G_Loss 9.613615] [D_Loss 0.000763]\n",
      "train [Epoch 0/10] [Batch 850/1173] [Iter 850] [G_Loss 10.517090] [D_Loss 0.000886]\n",
      "train [Epoch 0/10] [Batch 851/1173] [Iter 851] [G_Loss 10.052466] [D_Loss 0.001069]\n",
      "train [Epoch 0/10] [Batch 852/1173] [Iter 852] [G_Loss 10.162480] [D_Loss 0.000247]\n",
      "train [Epoch 0/10] [Batch 853/1173] [Iter 853] [G_Loss 9.950175] [D_Loss 0.000301]\n",
      "train [Epoch 0/10] [Batch 854/1173] [Iter 854] [G_Loss 10.654446] [D_Loss 0.000232]\n",
      "train [Epoch 0/10] [Batch 855/1173] [Iter 855] [G_Loss 10.866489] [D_Loss 0.023518]\n",
      "train [Epoch 0/10] [Batch 856/1173] [Iter 856] [G_Loss 9.769862] [D_Loss 0.002632]\n",
      "train [Epoch 0/10] [Batch 857/1173] [Iter 857] [G_Loss 8.964376] [D_Loss 0.011925]\n",
      "train [Epoch 0/10] [Batch 858/1173] [Iter 858] [G_Loss 10.940670] [D_Loss 0.000785]\n",
      "train [Epoch 0/10] [Batch 859/1173] [Iter 859] [G_Loss 11.771931] [D_Loss 0.011835]\n",
      "train [Epoch 0/10] [Batch 860/1173] [Iter 860] [G_Loss 9.965711] [D_Loss 0.000498]\n",
      "train [Epoch 0/10] [Batch 861/1173] [Iter 861] [G_Loss 9.945336] [D_Loss 0.000611]\n",
      "train [Epoch 0/10] [Batch 862/1173] [Iter 862] [G_Loss 10.126493] [D_Loss 0.000261]\n",
      "train [Epoch 0/10] [Batch 863/1173] [Iter 863] [G_Loss 10.686437] [D_Loss 0.016203]\n",
      "train [Epoch 0/10] [Batch 864/1173] [Iter 864] [G_Loss 9.082399] [D_Loss 0.001535]\n",
      "train [Epoch 0/10] [Batch 865/1173] [Iter 865] [G_Loss 7.543553] [D_Loss 0.004357]\n",
      "train [Epoch 0/10] [Batch 866/1173] [Iter 866] [G_Loss 9.039532] [D_Loss 0.001420]\n",
      "train [Epoch 0/10] [Batch 867/1173] [Iter 867] [G_Loss 8.889716] [D_Loss 0.000783]\n",
      "train [Epoch 0/10] [Batch 868/1173] [Iter 868] [G_Loss 9.066962] [D_Loss 0.001210]\n",
      "train [Epoch 0/10] [Batch 869/1173] [Iter 869] [G_Loss 9.395209] [D_Loss 0.013482]\n",
      "train [Epoch 0/10] [Batch 870/1173] [Iter 870] [G_Loss 7.371604] [D_Loss 0.013248]\n",
      "train [Epoch 0/10] [Batch 871/1173] [Iter 871] [G_Loss 10.003856] [D_Loss 0.021480]\n",
      "train [Epoch 0/10] [Batch 872/1173] [Iter 872] [G_Loss 8.428865] [D_Loss 0.005709]\n",
      "train [Epoch 0/10] [Batch 873/1173] [Iter 873] [G_Loss 8.561670] [D_Loss 0.001134]\n",
      "train [Epoch 0/10] [Batch 874/1173] [Iter 874] [G_Loss 8.545002] [D_Loss 0.002775]\n",
      "train [Epoch 0/10] [Batch 875/1173] [Iter 875] [G_Loss 8.283495] [D_Loss 0.003064]\n",
      "train [Epoch 0/10] [Batch 876/1173] [Iter 876] [G_Loss 8.187613] [D_Loss 0.003875]\n",
      "train [Epoch 0/10] [Batch 877/1173] [Iter 877] [G_Loss 10.019587] [D_Loss 0.000260]\n",
      "train [Epoch 0/10] [Batch 878/1173] [Iter 878] [G_Loss 9.890783] [D_Loss 0.000204]\n",
      "train [Epoch 0/10] [Batch 879/1173] [Iter 879] [G_Loss 10.044519] [D_Loss 0.000335]\n",
      "train [Epoch 0/10] [Batch 880/1173] [Iter 880] [G_Loss 9.540573] [D_Loss 0.014516]\n",
      "train [Epoch 0/10] [Batch 881/1173] [Iter 881] [G_Loss 6.496417] [D_Loss 0.029519]\n",
      "train [Epoch 0/10] [Batch 882/1173] [Iter 882] [G_Loss 11.039978] [D_Loss 0.001175]\n",
      "train [Epoch 0/10] [Batch 883/1173] [Iter 883] [G_Loss 11.989289] [D_Loss 0.040355]\n",
      "train [Epoch 0/10] [Batch 884/1173] [Iter 884] [G_Loss 11.104604] [D_Loss 0.002889]\n",
      "train [Epoch 0/10] [Batch 885/1173] [Iter 885] [G_Loss 8.822353] [D_Loss 0.014068]\n",
      "train [Epoch 0/10] [Batch 886/1173] [Iter 886] [G_Loss 10.470830] [D_Loss 0.036190]\n",
      "train [Epoch 0/10] [Batch 887/1173] [Iter 887] [G_Loss 9.846968] [D_Loss 0.004793]\n",
      "train [Epoch 0/10] [Batch 888/1173] [Iter 888] [G_Loss 8.720338] [D_Loss 0.002255]\n",
      "train [Epoch 0/10] [Batch 889/1173] [Iter 889] [G_Loss 8.220256] [D_Loss 0.001371]\n",
      "train [Epoch 0/10] [Batch 890/1173] [Iter 890] [G_Loss 8.724242] [D_Loss 0.002163]\n",
      "train [Epoch 0/10] [Batch 891/1173] [Iter 891] [G_Loss 8.474983] [D_Loss 0.030385]\n",
      "train [Epoch 0/10] [Batch 892/1173] [Iter 892] [G_Loss 7.412282] [D_Loss 0.002306]\n",
      "train [Epoch 0/10] [Batch 893/1173] [Iter 893] [G_Loss 5.962523] [D_Loss 0.011444]\n",
      "train [Epoch 0/10] [Batch 894/1173] [Iter 894] [G_Loss 8.003500] [D_Loss 0.002211]\n",
      "train [Epoch 0/10] [Batch 895/1173] [Iter 895] [G_Loss 9.327819] [D_Loss 0.000466]\n",
      "train [Epoch 0/10] [Batch 896/1173] [Iter 896] [G_Loss 9.846034] [D_Loss 0.006887]\n",
      "train [Epoch 0/10] [Batch 897/1173] [Iter 897] [G_Loss 9.567217] [D_Loss 0.024467]\n",
      "train [Epoch 0/10] [Batch 898/1173] [Iter 898] [G_Loss 7.758968] [D_Loss 0.005183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 899/1173] [Iter 899] [G_Loss 8.257024] [D_Loss 0.001495]\n",
      "train [Epoch 0/10] [Batch 900/1173] [Iter 900] [G_Loss 7.673631] [D_Loss 0.002192]\n",
      "train [Epoch 0/10] [Batch 901/1173] [Iter 901] [G_Loss 8.162182] [D_Loss 0.001060]\n",
      "train [Epoch 0/10] [Batch 902/1173] [Iter 902] [G_Loss 8.833540] [D_Loss 0.019674]\n",
      "train [Epoch 0/10] [Batch 903/1173] [Iter 903] [G_Loss 7.710322] [D_Loss 0.002497]\n",
      "train [Epoch 0/10] [Batch 904/1173] [Iter 904] [G_Loss 7.620625] [D_Loss 0.004890]\n",
      "train [Epoch 0/10] [Batch 905/1173] [Iter 905] [G_Loss 7.244477] [D_Loss 0.001962]\n",
      "train [Epoch 0/10] [Batch 906/1173] [Iter 906] [G_Loss 7.255416] [D_Loss 0.006805]\n",
      "train [Epoch 0/10] [Batch 907/1173] [Iter 907] [G_Loss 8.597007] [D_Loss 0.004149]\n",
      "train [Epoch 0/10] [Batch 908/1173] [Iter 908] [G_Loss 9.206196] [D_Loss 0.000377]\n",
      "train [Epoch 0/10] [Batch 909/1173] [Iter 909] [G_Loss 9.507601] [D_Loss 0.005939]\n",
      "train [Epoch 0/10] [Batch 910/1173] [Iter 910] [G_Loss 8.178848] [D_Loss 0.001225]\n",
      "train [Epoch 0/10] [Batch 911/1173] [Iter 911] [G_Loss 8.501546] [D_Loss 0.005768]\n",
      "train [Epoch 0/10] [Batch 912/1173] [Iter 912] [G_Loss 8.150565] [D_Loss 0.001127]\n",
      "train [Epoch 0/10] [Batch 913/1173] [Iter 913] [G_Loss 8.156853] [D_Loss 0.001948]\n",
      "train [Epoch 0/10] [Batch 914/1173] [Iter 914] [G_Loss 8.387880] [D_Loss 0.010588]\n",
      "train [Epoch 0/10] [Batch 915/1173] [Iter 915] [G_Loss 7.982600] [D_Loss 0.002283]\n",
      "train [Epoch 0/10] [Batch 916/1173] [Iter 916] [G_Loss 6.970825] [D_Loss 0.004780]\n",
      "train [Epoch 0/10] [Batch 917/1173] [Iter 917] [G_Loss 8.539463] [D_Loss 0.000615]\n",
      "train [Epoch 0/10] [Batch 918/1173] [Iter 918] [G_Loss 9.413693] [D_Loss 0.000556]\n",
      "train [Epoch 0/10] [Batch 919/1173] [Iter 919] [G_Loss 9.511086] [D_Loss 0.001165]\n",
      "train [Epoch 0/10] [Batch 920/1173] [Iter 920] [G_Loss 9.722678] [D_Loss 0.000250]\n",
      "train [Epoch 0/10] [Batch 921/1173] [Iter 921] [G_Loss 11.051328] [D_Loss 0.000066]\n",
      "train [Epoch 0/10] [Batch 922/1173] [Iter 922] [G_Loss 10.770491] [D_Loss 0.018343]\n",
      "train [Epoch 0/10] [Batch 923/1173] [Iter 923] [G_Loss 9.060281] [D_Loss 0.000612]\n",
      "train [Epoch 0/10] [Batch 924/1173] [Iter 924] [G_Loss 8.766102] [D_Loss 0.015647]\n",
      "train [Epoch 0/10] [Batch 925/1173] [Iter 925] [G_Loss 7.271436] [D_Loss 0.001599]\n",
      "train [Epoch 0/10] [Batch 926/1173] [Iter 926] [G_Loss 7.510835] [D_Loss 0.003145]\n",
      "train [Epoch 0/10] [Batch 927/1173] [Iter 927] [G_Loss 8.129455] [D_Loss 0.001058]\n",
      "train [Epoch 0/10] [Batch 928/1173] [Iter 928] [G_Loss 7.826865] [D_Loss 0.029999]\n",
      "train [Epoch 0/10] [Batch 929/1173] [Iter 929] [G_Loss 7.295793] [D_Loss 0.003787]\n",
      "train [Epoch 0/10] [Batch 930/1173] [Iter 930] [G_Loss 7.705682] [D_Loss 0.001440]\n",
      "train [Epoch 0/10] [Batch 931/1173] [Iter 931] [G_Loss 7.935992] [D_Loss 0.002464]\n",
      "train [Epoch 0/10] [Batch 932/1173] [Iter 932] [G_Loss 8.362719] [D_Loss 0.001880]\n",
      "train [Epoch 0/10] [Batch 933/1173] [Iter 933] [G_Loss 10.173153] [D_Loss 0.009974]\n",
      "train [Epoch 0/10] [Batch 934/1173] [Iter 934] [G_Loss 9.523085] [D_Loss 0.010645]\n",
      "train [Epoch 0/10] [Batch 935/1173] [Iter 935] [G_Loss 7.751339] [D_Loss 0.001504]\n",
      "train [Epoch 0/10] [Batch 936/1173] [Iter 936] [G_Loss 7.282040] [D_Loss 0.002274]\n",
      "train [Epoch 0/10] [Batch 937/1173] [Iter 937] [G_Loss 8.121438] [D_Loss 0.001521]\n",
      "train [Epoch 0/10] [Batch 938/1173] [Iter 938] [G_Loss 8.359624] [D_Loss 0.001097]\n",
      "train [Epoch 0/10] [Batch 939/1173] [Iter 939] [G_Loss 8.817219] [D_Loss 0.001625]\n",
      "train [Epoch 0/10] [Batch 940/1173] [Iter 940] [G_Loss 9.622631] [D_Loss 0.002262]\n",
      "train [Epoch 0/10] [Batch 941/1173] [Iter 941] [G_Loss 9.978352] [D_Loss 0.000311]\n",
      "train [Epoch 0/10] [Batch 942/1173] [Iter 942] [G_Loss 10.505908] [D_Loss 0.002417]\n",
      "train [Epoch 0/10] [Batch 943/1173] [Iter 943] [G_Loss 9.967235] [D_Loss 0.000177]\n",
      "train [Epoch 0/10] [Batch 944/1173] [Iter 944] [G_Loss 9.918509] [D_Loss 0.000509]\n",
      "train [Epoch 0/10] [Batch 945/1173] [Iter 945] [G_Loss 10.480998] [D_Loss 0.000260]\n",
      "train [Epoch 0/10] [Batch 946/1173] [Iter 946] [G_Loss 11.054705] [D_Loss 0.000097]\n",
      "train [Epoch 0/10] [Batch 947/1173] [Iter 947] [G_Loss 9.588881] [D_Loss 0.000458]\n",
      "train [Epoch 0/10] [Batch 948/1173] [Iter 948] [G_Loss 10.188103] [D_Loss 0.001416]\n",
      "train [Epoch 0/10] [Batch 949/1173] [Iter 949] [G_Loss 9.703196] [D_Loss 0.000326]\n",
      "train [Epoch 0/10] [Batch 950/1173] [Iter 950] [G_Loss 9.825670] [D_Loss 0.000274]\n",
      "train [Epoch 0/10] [Batch 951/1173] [Iter 951] [G_Loss 9.702805] [D_Loss 0.000282]\n",
      "train [Epoch 0/10] [Batch 952/1173] [Iter 952] [G_Loss 10.132651] [D_Loss 0.002182]\n",
      "train [Epoch 0/10] [Batch 953/1173] [Iter 953] [G_Loss 9.151299] [D_Loss 0.000343]\n",
      "train [Epoch 0/10] [Batch 954/1173] [Iter 954] [G_Loss 9.536283] [D_Loss 0.041020]\n",
      "train [Epoch 0/10] [Batch 955/1173] [Iter 955] [G_Loss 3.212436] [D_Loss 0.218641]\n",
      "train [Epoch 0/10] [Batch 956/1173] [Iter 956] [G_Loss 41.928020] [D_Loss 0.059988]\n",
      "train [Epoch 0/10] [Batch 957/1173] [Iter 957] [G_Loss 49.731640] [D_Loss 0.882240]\n",
      "train [Epoch 0/10] [Batch 958/1173] [Iter 958] [G_Loss 30.226999] [D_Loss 0.173360]\n",
      "train [Epoch 0/10] [Batch 959/1173] [Iter 959] [G_Loss 23.094975] [D_Loss 0.124529]\n",
      "train [Epoch 0/10] [Batch 960/1173] [Iter 960] [G_Loss 13.835772] [D_Loss 0.068126]\n",
      "train [Epoch 0/10] [Batch 961/1173] [Iter 961] [G_Loss 1.872218] [D_Loss 0.867801]\n",
      "train [Epoch 0/10] [Batch 962/1173] [Iter 962] [G_Loss 22.482313] [D_Loss 0.104900]\n",
      "train [Epoch 0/10] [Batch 963/1173] [Iter 963] [G_Loss 25.512856] [D_Loss 0.112589]\n",
      "train [Epoch 0/10] [Batch 964/1173] [Iter 964] [G_Loss 24.338764] [D_Loss 0.096431]\n",
      "train [Epoch 0/10] [Batch 965/1173] [Iter 965] [G_Loss 22.050774] [D_Loss 0.276577]\n",
      "train [Epoch 0/10] [Batch 966/1173] [Iter 966] [G_Loss 18.888935] [D_Loss 0.287392]\n",
      "train [Epoch 0/10] [Batch 967/1173] [Iter 967] [G_Loss 16.344646] [D_Loss 0.000548]\n",
      "train [Epoch 0/10] [Batch 968/1173] [Iter 968] [G_Loss 15.010432] [D_Loss 0.017285]\n",
      "train [Epoch 0/10] [Batch 969/1173] [Iter 969] [G_Loss 14.851171] [D_Loss 0.024997]\n",
      "train [Epoch 0/10] [Batch 970/1173] [Iter 970] [G_Loss 13.381531] [D_Loss 0.000190]\n",
      "train [Epoch 0/10] [Batch 971/1173] [Iter 971] [G_Loss 12.859480] [D_Loss 0.042109]\n",
      "train [Epoch 0/10] [Batch 972/1173] [Iter 972] [G_Loss 10.844511] [D_Loss 0.000049]\n",
      "train [Epoch 0/10] [Batch 973/1173] [Iter 973] [G_Loss 10.669927] [D_Loss 0.095123]\n",
      "train [Epoch 0/10] [Batch 974/1173] [Iter 974] [G_Loss 9.175759] [D_Loss 0.163859]\n",
      "train [Epoch 0/10] [Batch 975/1173] [Iter 975] [G_Loss 4.612172] [D_Loss 0.048091]\n",
      "train [Epoch 0/10] [Batch 976/1173] [Iter 976] [G_Loss 4.571207] [D_Loss 0.016921]\n",
      "train [Epoch 0/10] [Batch 977/1173] [Iter 977] [G_Loss 8.267291] [D_Loss 0.000380]\n",
      "train [Epoch 0/10] [Batch 978/1173] [Iter 978] [G_Loss 9.517702] [D_Loss 0.001372]\n",
      "train [Epoch 0/10] [Batch 979/1173] [Iter 979] [G_Loss 9.953070] [D_Loss 0.010437]\n",
      "train [Epoch 0/10] [Batch 980/1173] [Iter 980] [G_Loss 9.837360] [D_Loss 0.001935]\n",
      "train [Epoch 0/10] [Batch 981/1173] [Iter 981] [G_Loss 9.625538] [D_Loss 0.000102]\n",
      "train [Epoch 0/10] [Batch 982/1173] [Iter 982] [G_Loss 9.141698] [D_Loss 0.000415]\n",
      "train [Epoch 0/10] [Batch 983/1173] [Iter 983] [G_Loss 8.930775] [D_Loss 0.000236]\n",
      "train [Epoch 0/10] [Batch 984/1173] [Iter 984] [G_Loss 9.096293] [D_Loss 0.001288]\n",
      "train [Epoch 0/10] [Batch 985/1173] [Iter 985] [G_Loss 8.328718] [D_Loss 0.000385]\n",
      "train [Epoch 0/10] [Batch 986/1173] [Iter 986] [G_Loss 8.244014] [D_Loss 0.000428]\n",
      "train [Epoch 0/10] [Batch 987/1173] [Iter 987] [G_Loss 8.095112] [D_Loss 0.000711]\n",
      "train [Epoch 0/10] [Batch 988/1173] [Iter 988] [G_Loss 8.389014] [D_Loss 0.000337]\n",
      "train [Epoch 0/10] [Batch 989/1173] [Iter 989] [G_Loss 7.922424] [D_Loss 0.001157]\n",
      "train [Epoch 0/10] [Batch 990/1173] [Iter 990] [G_Loss 8.125472] [D_Loss 0.002627]\n",
      "train [Epoch 0/10] [Batch 991/1173] [Iter 991] [G_Loss 7.746098] [D_Loss 0.001438]\n",
      "train [Epoch 0/10] [Batch 992/1173] [Iter 992] [G_Loss 7.608645] [D_Loss 0.001036]\n",
      "train [Epoch 0/10] [Batch 993/1173] [Iter 993] [G_Loss 7.620110] [D_Loss 0.001038]\n",
      "train [Epoch 0/10] [Batch 994/1173] [Iter 994] [G_Loss 7.649762] [D_Loss 0.000759]\n",
      "train [Epoch 0/10] [Batch 995/1173] [Iter 995] [G_Loss 7.553663] [D_Loss 0.000963]\n",
      "train [Epoch 0/10] [Batch 996/1173] [Iter 996] [G_Loss 7.786399] [D_Loss 0.042978]\n",
      "train [Epoch 0/10] [Batch 997/1173] [Iter 997] [G_Loss 7.134502] [D_Loss 0.021369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 998/1173] [Iter 998] [G_Loss 6.391749] [D_Loss 0.002607]\n",
      "train [Epoch 0/10] [Batch 999/1173] [Iter 999] [G_Loss 6.540923] [D_Loss 0.001799]\n",
      "train [Epoch 0/10] [Batch 1000/1173] [Iter 1000] [G_Loss 7.033642] [D_Loss 0.001097]\n",
      "train [Epoch 0/10] [Batch 1001/1173] [Iter 1001] [G_Loss 6.432572] [D_Loss 0.002268]\n",
      "train [Epoch 0/10] [Batch 1002/1173] [Iter 1002] [G_Loss 6.543198] [D_Loss 0.003486]\n",
      "train [Epoch 0/10] [Batch 1003/1173] [Iter 1003] [G_Loss 6.432829] [D_Loss 0.002576]\n",
      "train [Epoch 0/10] [Batch 1004/1173] [Iter 1004] [G_Loss 6.357534] [D_Loss 0.006029]\n",
      "train [Epoch 0/10] [Batch 1005/1173] [Iter 1005] [G_Loss 6.458215] [D_Loss 0.002830]\n",
      "train [Epoch 0/10] [Batch 1006/1173] [Iter 1006] [G_Loss 6.790162] [D_Loss 0.001817]\n",
      "train [Epoch 0/10] [Batch 1007/1173] [Iter 1007] [G_Loss 6.283223] [D_Loss 0.002807]\n",
      "train [Epoch 0/10] [Batch 1008/1173] [Iter 1008] [G_Loss 6.950981] [D_Loss 0.001999]\n",
      "train [Epoch 0/10] [Batch 1009/1173] [Iter 1009] [G_Loss 7.687881] [D_Loss 0.000982]\n",
      "train [Epoch 0/10] [Batch 1010/1173] [Iter 1010] [G_Loss 6.784961] [D_Loss 0.001484]\n",
      "train [Epoch 0/10] [Batch 1011/1173] [Iter 1011] [G_Loss 7.130394] [D_Loss 0.058053]\n",
      "train [Epoch 0/10] [Batch 1012/1173] [Iter 1012] [G_Loss 8.079775] [D_Loss 0.000424]\n",
      "train [Epoch 0/10] [Batch 1013/1173] [Iter 1013] [G_Loss 8.188634] [D_Loss 0.011323]\n",
      "train [Epoch 0/10] [Batch 1014/1173] [Iter 1014] [G_Loss 8.295249] [D_Loss 0.000379]\n",
      "train [Epoch 0/10] [Batch 1015/1173] [Iter 1015] [G_Loss 7.959992] [D_Loss 0.000776]\n",
      "train [Epoch 0/10] [Batch 1016/1173] [Iter 1016] [G_Loss 8.257826] [D_Loss 0.000357]\n",
      "train [Epoch 0/10] [Batch 1017/1173] [Iter 1017] [G_Loss 8.061454] [D_Loss 0.000479]\n",
      "train [Epoch 0/10] [Batch 1018/1173] [Iter 1018] [G_Loss 8.230506] [D_Loss 0.000464]\n",
      "train [Epoch 0/10] [Batch 1019/1173] [Iter 1019] [G_Loss 8.104471] [D_Loss 0.001703]\n",
      "train [Epoch 0/10] [Batch 1020/1173] [Iter 1020] [G_Loss 7.852897] [D_Loss 0.000615]\n",
      "train [Epoch 0/10] [Batch 1021/1173] [Iter 1021] [G_Loss 7.786973] [D_Loss 0.000589]\n",
      "train [Epoch 0/10] [Batch 1022/1173] [Iter 1022] [G_Loss 7.660285] [D_Loss 0.000838]\n",
      "train [Epoch 0/10] [Batch 1023/1173] [Iter 1023] [G_Loss 7.883232] [D_Loss 0.000555]\n",
      "train [Epoch 0/10] [Batch 1024/1173] [Iter 1024] [G_Loss 8.375089] [D_Loss 0.013257]\n",
      "train [Epoch 0/10] [Batch 1025/1173] [Iter 1025] [G_Loss 8.249263] [D_Loss 0.000567]\n",
      "train [Epoch 0/10] [Batch 1026/1173] [Iter 1026] [G_Loss 8.074910] [D_Loss 0.000515]\n",
      "train [Epoch 0/10] [Batch 1027/1173] [Iter 1027] [G_Loss 8.057968] [D_Loss 0.001522]\n",
      "train [Epoch 0/10] [Batch 1028/1173] [Iter 1028] [G_Loss 8.261880] [D_Loss 0.000425]\n",
      "train [Epoch 0/10] [Batch 1029/1173] [Iter 1029] [G_Loss 8.129566] [D_Loss 0.000667]\n",
      "train [Epoch 0/10] [Batch 1030/1173] [Iter 1030] [G_Loss 8.216342] [D_Loss 0.001123]\n",
      "train [Epoch 0/10] [Batch 1031/1173] [Iter 1031] [G_Loss 8.756135] [D_Loss 0.000276]\n",
      "train [Epoch 0/10] [Batch 1032/1173] [Iter 1032] [G_Loss 8.630373] [D_Loss 0.000240]\n",
      "train [Epoch 0/10] [Batch 1033/1173] [Iter 1033] [G_Loss 8.779669] [D_Loss 0.000335]\n",
      "train [Epoch 0/10] [Batch 1034/1173] [Iter 1034] [G_Loss 8.359262] [D_Loss 0.000501]\n",
      "train [Epoch 0/10] [Batch 1035/1173] [Iter 1035] [G_Loss 7.959538] [D_Loss 0.013622]\n",
      "train [Epoch 0/10] [Batch 1036/1173] [Iter 1036] [G_Loss 8.344239] [D_Loss 0.002852]\n",
      "train [Epoch 0/10] [Batch 1037/1173] [Iter 1037] [G_Loss 8.842081] [D_Loss 0.000158]\n",
      "train [Epoch 0/10] [Batch 1038/1173] [Iter 1038] [G_Loss 8.428555] [D_Loss 0.000303]\n",
      "train [Epoch 0/10] [Batch 1039/1173] [Iter 1039] [G_Loss 8.113241] [D_Loss 0.000528]\n",
      "train [Epoch 0/10] [Batch 1040/1173] [Iter 1040] [G_Loss 8.195118] [D_Loss 0.000438]\n",
      "train [Epoch 0/10] [Batch 1041/1173] [Iter 1041] [G_Loss 8.318699] [D_Loss 0.000278]\n",
      "train [Epoch 0/10] [Batch 1042/1173] [Iter 1042] [G_Loss 8.564277] [D_Loss 0.002060]\n",
      "train [Epoch 0/10] [Batch 1043/1173] [Iter 1043] [G_Loss 8.231499] [D_Loss 0.000353]\n",
      "train [Epoch 0/10] [Batch 1044/1173] [Iter 1044] [G_Loss 8.442795] [D_Loss 0.000320]\n",
      "train [Epoch 0/10] [Batch 1045/1173] [Iter 1045] [G_Loss 8.849805] [D_Loss 0.000221]\n",
      "train [Epoch 0/10] [Batch 1046/1173] [Iter 1046] [G_Loss 8.421336] [D_Loss 0.000260]\n",
      "train [Epoch 0/10] [Batch 1047/1173] [Iter 1047] [G_Loss 8.605161] [D_Loss 0.000205]\n",
      "train [Epoch 0/10] [Batch 1048/1173] [Iter 1048] [G_Loss 8.450862] [D_Loss 0.000330]\n",
      "train [Epoch 0/10] [Batch 1049/1173] [Iter 1049] [G_Loss 8.603175] [D_Loss 0.000252]\n",
      "train [Epoch 0/10] [Batch 1050/1173] [Iter 1050] [G_Loss 8.565665] [D_Loss 0.001459]\n",
      "train [Epoch 0/10] [Batch 1051/1173] [Iter 1051] [G_Loss 8.411252] [D_Loss 0.000306]\n",
      "train [Epoch 0/10] [Batch 1052/1173] [Iter 1052] [G_Loss 8.740178] [D_Loss 0.000305]\n",
      "train [Epoch 0/10] [Batch 1053/1173] [Iter 1053] [G_Loss 8.381743] [D_Loss 0.000369]\n",
      "train [Epoch 0/10] [Batch 1054/1173] [Iter 1054] [G_Loss 8.225734] [D_Loss 0.000627]\n",
      "train [Epoch 0/10] [Batch 1055/1173] [Iter 1055] [G_Loss 8.255650] [D_Loss 0.000361]\n",
      "train [Epoch 0/10] [Batch 1056/1173] [Iter 1056] [G_Loss 8.108477] [D_Loss 0.035770]\n",
      "train [Epoch 0/10] [Batch 1057/1173] [Iter 1057] [G_Loss 8.839593] [D_Loss 0.000380]\n",
      "train [Epoch 0/10] [Batch 1058/1173] [Iter 1058] [G_Loss 7.667134] [D_Loss 0.000529]\n",
      "train [Epoch 0/10] [Batch 1059/1173] [Iter 1059] [G_Loss 11.415227] [D_Loss 0.004972]\n",
      "train [Epoch 0/10] [Batch 1060/1173] [Iter 1060] [G_Loss 8.483212] [D_Loss 0.000351]\n",
      "train [Epoch 0/10] [Batch 1061/1173] [Iter 1061] [G_Loss 7.755158] [D_Loss 0.000647]\n",
      "train [Epoch 0/10] [Batch 1062/1173] [Iter 1062] [G_Loss 7.499078] [D_Loss 0.000731]\n",
      "train [Epoch 0/10] [Batch 1063/1173] [Iter 1063] [G_Loss 7.662137] [D_Loss 0.000611]\n",
      "train [Epoch 0/10] [Batch 1064/1173] [Iter 1064] [G_Loss 7.872721] [D_Loss 0.000515]\n",
      "train [Epoch 0/10] [Batch 1065/1173] [Iter 1065] [G_Loss 8.130506] [D_Loss 0.000524]\n",
      "train [Epoch 0/10] [Batch 1066/1173] [Iter 1066] [G_Loss 7.715864] [D_Loss 0.001336]\n",
      "train [Epoch 0/10] [Batch 1067/1173] [Iter 1067] [G_Loss 7.582420] [D_Loss 0.000619]\n",
      "train [Epoch 0/10] [Batch 1068/1173] [Iter 1068] [G_Loss 7.954870] [D_Loss 0.000533]\n",
      "train [Epoch 0/10] [Batch 1069/1173] [Iter 1069] [G_Loss 7.597987] [D_Loss 0.000676]\n",
      "train [Epoch 0/10] [Batch 1070/1173] [Iter 1070] [G_Loss 8.261389] [D_Loss 0.000422]\n",
      "train [Epoch 0/10] [Batch 1071/1173] [Iter 1071] [G_Loss 7.695601] [D_Loss 0.000719]\n",
      "train [Epoch 0/10] [Batch 1072/1173] [Iter 1072] [G_Loss 8.030249] [D_Loss 0.000476]\n",
      "train [Epoch 0/10] [Batch 1073/1173] [Iter 1073] [G_Loss 7.942594] [D_Loss 0.000468]\n",
      "train [Epoch 0/10] [Batch 1074/1173] [Iter 1074] [G_Loss 7.867870] [D_Loss 0.001166]\n",
      "train [Epoch 0/10] [Batch 1075/1173] [Iter 1075] [G_Loss 7.861739] [D_Loss 0.000455]\n",
      "train [Epoch 0/10] [Batch 1076/1173] [Iter 1076] [G_Loss 8.091049] [D_Loss 0.000444]\n",
      "train [Epoch 0/10] [Batch 1077/1173] [Iter 1077] [G_Loss 7.855820] [D_Loss 0.000628]\n",
      "train [Epoch 0/10] [Batch 1078/1173] [Iter 1078] [G_Loss 8.258933] [D_Loss 0.000504]\n",
      "train [Epoch 0/10] [Batch 1079/1173] [Iter 1079] [G_Loss 8.083639] [D_Loss 0.000481]\n",
      "train [Epoch 0/10] [Batch 1080/1173] [Iter 1080] [G_Loss 7.971806] [D_Loss 0.001246]\n",
      "train [Epoch 0/10] [Batch 1081/1173] [Iter 1081] [G_Loss 8.218391] [D_Loss 0.000496]\n",
      "train [Epoch 0/10] [Batch 1082/1173] [Iter 1082] [G_Loss 8.492677] [D_Loss 0.000304]\n",
      "train [Epoch 0/10] [Batch 1083/1173] [Iter 1083] [G_Loss 7.868087] [D_Loss 0.001985]\n",
      "train [Epoch 0/10] [Batch 1084/1173] [Iter 1084] [G_Loss 7.971289] [D_Loss 0.000489]\n",
      "train [Epoch 0/10] [Batch 1085/1173] [Iter 1085] [G_Loss 8.061052] [D_Loss 0.000610]\n",
      "train [Epoch 0/10] [Batch 1086/1173] [Iter 1086] [G_Loss 7.968860] [D_Loss 0.000427]\n",
      "train [Epoch 0/10] [Batch 1087/1173] [Iter 1087] [G_Loss 7.938269] [D_Loss 0.001199]\n",
      "train [Epoch 0/10] [Batch 1088/1173] [Iter 1088] [G_Loss 8.233068] [D_Loss 0.000381]\n",
      "train [Epoch 0/10] [Batch 1089/1173] [Iter 1089] [G_Loss 8.571446] [D_Loss 0.000241]\n",
      "train [Epoch 0/10] [Batch 1090/1173] [Iter 1090] [G_Loss 8.377792] [D_Loss 0.000300]\n",
      "train [Epoch 0/10] [Batch 1091/1173] [Iter 1091] [G_Loss 8.286414] [D_Loss 0.001056]\n",
      "train [Epoch 0/10] [Batch 1092/1173] [Iter 1092] [G_Loss 8.290221] [D_Loss 0.000961]\n",
      "train [Epoch 0/10] [Batch 1093/1173] [Iter 1093] [G_Loss 8.268753] [D_Loss 0.000339]\n",
      "train [Epoch 0/10] [Batch 1094/1173] [Iter 1094] [G_Loss 8.321158] [D_Loss 0.000467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 1095/1173] [Iter 1095] [G_Loss 8.421984] [D_Loss 0.000300]\n",
      "train [Epoch 0/10] [Batch 1096/1173] [Iter 1096] [G_Loss 8.430933] [D_Loss 0.002832]\n",
      "train [Epoch 0/10] [Batch 1097/1173] [Iter 1097] [G_Loss 8.247802] [D_Loss 0.001410]\n",
      "train [Epoch 0/10] [Batch 1098/1173] [Iter 1098] [G_Loss 8.104677] [D_Loss 0.000587]\n",
      "train [Epoch 0/10] [Batch 1099/1173] [Iter 1099] [G_Loss 8.371483] [D_Loss 0.000666]\n",
      "train [Epoch 0/10] [Batch 1100/1173] [Iter 1100] [G_Loss 8.310215] [D_Loss 0.000573]\n",
      "train [Epoch 0/10] [Batch 1101/1173] [Iter 1101] [G_Loss 8.115624] [D_Loss 0.000560]\n",
      "train [Epoch 0/10] [Batch 1102/1173] [Iter 1102] [G_Loss 9.286182] [D_Loss 0.000112]\n",
      "train [Epoch 0/10] [Batch 1103/1173] [Iter 1103] [G_Loss 8.260077] [D_Loss 0.000330]\n",
      "train [Epoch 0/10] [Batch 1104/1173] [Iter 1104] [G_Loss 8.564466] [D_Loss 0.000318]\n",
      "train [Epoch 0/10] [Batch 1105/1173] [Iter 1105] [G_Loss 9.527561] [D_Loss 0.000221]\n",
      "train [Epoch 0/10] [Batch 1106/1173] [Iter 1106] [G_Loss 8.701002] [D_Loss 0.000223]\n",
      "train [Epoch 0/10] [Batch 1107/1173] [Iter 1107] [G_Loss 8.598139] [D_Loss 0.000285]\n",
      "train [Epoch 0/10] [Batch 1108/1173] [Iter 1108] [G_Loss 8.881608] [D_Loss 0.001826]\n",
      "train [Epoch 0/10] [Batch 1109/1173] [Iter 1109] [G_Loss 8.691768] [D_Loss 0.000590]\n",
      "train [Epoch 0/10] [Batch 1110/1173] [Iter 1110] [G_Loss 8.555116] [D_Loss 0.004893]\n",
      "train [Epoch 0/10] [Batch 1111/1173] [Iter 1111] [G_Loss 9.167007] [D_Loss 0.000282]\n",
      "train [Epoch 0/10] [Batch 1112/1173] [Iter 1112] [G_Loss 8.350010] [D_Loss 0.000327]\n",
      "train [Epoch 0/10] [Batch 1113/1173] [Iter 1113] [G_Loss 8.279150] [D_Loss 0.000355]\n",
      "train [Epoch 0/10] [Batch 1114/1173] [Iter 1114] [G_Loss 8.260507] [D_Loss 0.001139]\n",
      "train [Epoch 0/10] [Batch 1115/1173] [Iter 1115] [G_Loss 7.982100] [D_Loss 0.001227]\n",
      "train [Epoch 0/10] [Batch 1116/1173] [Iter 1116] [G_Loss 8.525272] [D_Loss 0.000308]\n",
      "train [Epoch 0/10] [Batch 1117/1173] [Iter 1117] [G_Loss 8.134647] [D_Loss 0.000388]\n",
      "train [Epoch 0/10] [Batch 1118/1173] [Iter 1118] [G_Loss 8.523831] [D_Loss 0.000267]\n",
      "train [Epoch 0/10] [Batch 1119/1173] [Iter 1119] [G_Loss 8.630769] [D_Loss 0.000194]\n",
      "train [Epoch 0/10] [Batch 1120/1173] [Iter 1120] [G_Loss 8.096688] [D_Loss 0.000600]\n",
      "train [Epoch 0/10] [Batch 1121/1173] [Iter 1121] [G_Loss 8.307844] [D_Loss 0.000424]\n",
      "train [Epoch 0/10] [Batch 1122/1173] [Iter 1122] [G_Loss 8.274940] [D_Loss 0.000350]\n",
      "train [Epoch 0/10] [Batch 1123/1173] [Iter 1123] [G_Loss 8.763281] [D_Loss 0.000406]\n",
      "train [Epoch 0/10] [Batch 1124/1173] [Iter 1124] [G_Loss 8.413143] [D_Loss 0.001131]\n",
      "train [Epoch 0/10] [Batch 1125/1173] [Iter 1125] [G_Loss 8.068827] [D_Loss 0.000515]\n",
      "train [Epoch 0/10] [Batch 1126/1173] [Iter 1126] [G_Loss 8.249207] [D_Loss 0.000342]\n",
      "train [Epoch 0/10] [Batch 1127/1173] [Iter 1127] [G_Loss 8.452030] [D_Loss 0.000261]\n",
      "train [Epoch 0/10] [Batch 1128/1173] [Iter 1128] [G_Loss 8.224533] [D_Loss 0.000356]\n",
      "train [Epoch 0/10] [Batch 1129/1173] [Iter 1129] [G_Loss 9.763881] [D_Loss 0.000720]\n",
      "train [Epoch 0/10] [Batch 1130/1173] [Iter 1130] [G_Loss 8.547482] [D_Loss 0.000308]\n",
      "train [Epoch 0/10] [Batch 1131/1173] [Iter 1131] [G_Loss 8.358254] [D_Loss 0.000300]\n",
      "train [Epoch 0/10] [Batch 1132/1173] [Iter 1132] [G_Loss 8.515635] [D_Loss 0.000363]\n",
      "train [Epoch 0/10] [Batch 1133/1173] [Iter 1133] [G_Loss 8.931005] [D_Loss 0.022924]\n",
      "train [Epoch 0/10] [Batch 1134/1173] [Iter 1134] [G_Loss 8.737445] [D_Loss 0.000233]\n",
      "train [Epoch 0/10] [Batch 1135/1173] [Iter 1135] [G_Loss 9.396845] [D_Loss 0.000107]\n",
      "train [Epoch 0/10] [Batch 1136/1173] [Iter 1136] [G_Loss 9.284248] [D_Loss 0.000124]\n",
      "train [Epoch 0/10] [Batch 1137/1173] [Iter 1137] [G_Loss 9.456160] [D_Loss 0.000116]\n",
      "train [Epoch 0/10] [Batch 1138/1173] [Iter 1138] [G_Loss 9.376281] [D_Loss 0.000117]\n",
      "train [Epoch 0/10] [Batch 1139/1173] [Iter 1139] [G_Loss 9.227790] [D_Loss 0.000163]\n",
      "train [Epoch 0/10] [Batch 1140/1173] [Iter 1140] [G_Loss 10.222731] [D_Loss 0.000068]\n",
      "train [Epoch 0/10] [Batch 1141/1173] [Iter 1141] [G_Loss 8.896365] [D_Loss 0.000317]\n",
      "train [Epoch 0/10] [Batch 1142/1173] [Iter 1142] [G_Loss 8.827791] [D_Loss 0.000170]\n",
      "train [Epoch 0/10] [Batch 1143/1173] [Iter 1143] [G_Loss 8.769415] [D_Loss 0.000221]\n",
      "train [Epoch 0/10] [Batch 1144/1173] [Iter 1144] [G_Loss 9.138590] [D_Loss 0.000820]\n",
      "train [Epoch 0/10] [Batch 1145/1173] [Iter 1145] [G_Loss 9.603031] [D_Loss 0.000130]\n",
      "train [Epoch 0/10] [Batch 1146/1173] [Iter 1146] [G_Loss 9.632281] [D_Loss 0.000084]\n",
      "train [Epoch 0/10] [Batch 1147/1173] [Iter 1147] [G_Loss 9.435476] [D_Loss 0.000107]\n",
      "train [Epoch 0/10] [Batch 1148/1173] [Iter 1148] [G_Loss 9.009573] [D_Loss 0.018892]\n",
      "train [Epoch 0/10] [Batch 1149/1173] [Iter 1149] [G_Loss 8.429134] [D_Loss 0.000266]\n",
      "train [Epoch 0/10] [Batch 1150/1173] [Iter 1150] [G_Loss 8.256208] [D_Loss 0.000395]\n",
      "train [Epoch 0/10] [Batch 1151/1173] [Iter 1151] [G_Loss 8.309353] [D_Loss 0.000306]\n",
      "train [Epoch 0/10] [Batch 1152/1173] [Iter 1152] [G_Loss 8.306880] [D_Loss 0.000374]\n",
      "train [Epoch 0/10] [Batch 1153/1173] [Iter 1153] [G_Loss 8.495209] [D_Loss 0.000468]\n",
      "train [Epoch 0/10] [Batch 1154/1173] [Iter 1154] [G_Loss 8.760300] [D_Loss 0.000200]\n",
      "train [Epoch 0/10] [Batch 1155/1173] [Iter 1155] [G_Loss 8.632080] [D_Loss 0.000249]\n",
      "train [Epoch 0/10] [Batch 1156/1173] [Iter 1156] [G_Loss 8.289900] [D_Loss 0.000291]\n",
      "train [Epoch 0/10] [Batch 1157/1173] [Iter 1157] [G_Loss 8.444064] [D_Loss 0.000236]\n",
      "train [Epoch 0/10] [Batch 1158/1173] [Iter 1158] [G_Loss 8.374169] [D_Loss 0.000516]\n",
      "train [Epoch 0/10] [Batch 1159/1173] [Iter 1159] [G_Loss 9.730825] [D_Loss 0.000095]\n",
      "train [Epoch 0/10] [Batch 1160/1173] [Iter 1160] [G_Loss 8.793211] [D_Loss 0.003157]\n",
      "train [Epoch 0/10] [Batch 1161/1173] [Iter 1161] [G_Loss 9.164660] [D_Loss 0.000151]\n",
      "train [Epoch 0/10] [Batch 1162/1173] [Iter 1162] [G_Loss 8.901550] [D_Loss 0.032432]\n",
      "train [Epoch 0/10] [Batch 1163/1173] [Iter 1163] [G_Loss 8.389331] [D_Loss 0.000405]\n",
      "train [Epoch 0/10] [Batch 1164/1173] [Iter 1164] [G_Loss 8.344934] [D_Loss 0.000317]\n",
      "train [Epoch 0/10] [Batch 1165/1173] [Iter 1165] [G_Loss 8.120896] [D_Loss 0.000701]\n",
      "train [Epoch 0/10] [Batch 1166/1173] [Iter 1166] [G_Loss 8.081496] [D_Loss 0.000406]\n",
      "train [Epoch 0/10] [Batch 1167/1173] [Iter 1167] [G_Loss 8.142255] [D_Loss 0.000533]\n",
      "train [Epoch 0/10] [Batch 1168/1173] [Iter 1168] [G_Loss 8.601495] [D_Loss 0.000247]\n",
      "train [Epoch 0/10] [Batch 1169/1173] [Iter 1169] [G_Loss 8.416198] [D_Loss 0.000284]\n",
      "train [Epoch 0/10] [Batch 1170/1173] [Iter 1170] [G_Loss 8.481613] [D_Loss 0.000306]\n",
      "train [Epoch 0/10] [Batch 1171/1173] [Iter 1171] [G_Loss 8.358215] [D_Loss 0.000355]\n",
      "train [Epoch 0/10] [Batch 1172/1173] [Iter 1172] [G_Loss 8.841711] [D_Loss 0.000288]\n",
      "val [Epoch 0/10] [Batch 0/31] [Iter 0] [G_Loss 8.739025] [D_Loss 0.000207]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 15.90 GiB total capacity; 15.09 GiB already allocated; 11.88 MiB free; 148.91 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a152329c3349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_gan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/cs231n-proj/src/scripts/train_gan.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(generator, discriminator, save)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalidation_rate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mval_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0msave_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../../outputs/gan/val_losses.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs231n-proj/src/scripts/train_gan.py\u001b[0m in \u001b[0;36mtrain_batches\u001b[0;34m(epoch, generator, discriminator, dataloader, train, save)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Generate images using the generator, find loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs231n-proj/src/scripts/train_gan.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleakyRelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleakyRelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleakyRelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    144\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    145\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     return torch.max_pool2d(\n\u001b[0;32m--> 494\u001b[0;31m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m max_pool2d = torch._jit_internal.boolean_dispatch(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 15.90 GiB total capacity; 15.09 GiB already allocated; 11.88 MiB free; 148.91 MiB cached)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_gan import *\n",
    "\n",
    "train_model(generator, discriminator, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.87s)\n",
      "creating index...\n",
      "index created!\n",
      "train [Epoch 0/10] [Batch 0/2345] [Iter 0] [G_Loss 0.987184] [D_Loss 1.406835]\n",
      "train [Epoch 0/10] [Batch 1/2345] [Iter 1] [G_Loss 51.995876] [D_Loss 0.095660]\n",
      "train [Epoch 0/10] [Batch 2/2345] [Iter 2] [G_Loss 4.838243] [D_Loss 11.466956]\n",
      "train [Epoch 0/10] [Batch 3/2345] [Iter 3] [G_Loss 94.425568] [D_Loss 35.702202]\n",
      "train [Epoch 0/10] [Batch 4/2345] [Iter 4] [G_Loss 40.008671] [D_Loss 6.659234]\n",
      "train [Epoch 0/10] [Batch 5/2345] [Iter 5] [G_Loss 4.465932] [D_Loss 1.476046]\n",
      "train [Epoch 0/10] [Batch 6/2345] [Iter 6] [G_Loss 1.738161] [D_Loss 2.731522]\n",
      "train [Epoch 0/10] [Batch 7/2345] [Iter 7] [G_Loss 15.739103] [D_Loss 0.527812]\n",
      "train [Epoch 0/10] [Batch 8/2345] [Iter 8] [G_Loss 17.393213] [D_Loss 1.801924]\n",
      "train [Epoch 0/10] [Batch 9/2345] [Iter 9] [G_Loss 8.765978] [D_Loss 0.346226]\n",
      "train [Epoch 0/10] [Batch 10/2345] [Iter 10] [G_Loss 5.073224] [D_Loss 0.568537]\n",
      "train [Epoch 0/10] [Batch 11/2345] [Iter 11] [G_Loss 4.906363] [D_Loss 0.345075]\n",
      "train [Epoch 0/10] [Batch 12/2345] [Iter 12] [G_Loss 5.693705] [D_Loss 0.232083]\n",
      "train [Epoch 0/10] [Batch 13/2345] [Iter 13] [G_Loss 5.716680] [D_Loss 0.672821]\n",
      "train [Epoch 0/10] [Batch 14/2345] [Iter 14] [G_Loss 5.486662] [D_Loss 0.482325]\n",
      "train [Epoch 0/10] [Batch 15/2345] [Iter 15] [G_Loss 4.683285] [D_Loss 0.417847]\n",
      "train [Epoch 0/10] [Batch 16/2345] [Iter 16] [G_Loss 4.972945] [D_Loss 0.835158]\n",
      "train [Epoch 0/10] [Batch 17/2345] [Iter 17] [G_Loss 5.261238] [D_Loss 0.597865]\n",
      "train [Epoch 0/10] [Batch 18/2345] [Iter 18] [G_Loss 2.935289] [D_Loss 0.577656]\n",
      "train [Epoch 0/10] [Batch 19/2345] [Iter 19] [G_Loss 3.372373] [D_Loss 0.896508]\n",
      "train [Epoch 0/10] [Batch 20/2345] [Iter 20] [G_Loss 2.551097] [D_Loss 0.793033]\n",
      "train [Epoch 0/10] [Batch 21/2345] [Iter 21] [G_Loss 3.526149] [D_Loss 0.691335]\n",
      "train [Epoch 0/10] [Batch 22/2345] [Iter 22] [G_Loss 1.182305] [D_Loss 0.892308]\n",
      "train [Epoch 0/10] [Batch 23/2345] [Iter 23] [G_Loss 3.452971] [D_Loss 0.874142]\n",
      "train [Epoch 0/10] [Batch 24/2345] [Iter 24] [G_Loss 3.080774] [D_Loss 0.674671]\n",
      "train [Epoch 0/10] [Batch 25/2345] [Iter 25] [G_Loss 1.901156] [D_Loss 0.502427]\n",
      "train [Epoch 0/10] [Batch 26/2345] [Iter 26] [G_Loss 1.797920] [D_Loss 0.936053]\n",
      "train [Epoch 0/10] [Batch 27/2345] [Iter 27] [G_Loss 1.850723] [D_Loss 0.498313]\n",
      "train [Epoch 0/10] [Batch 28/2345] [Iter 28] [G_Loss 2.609921] [D_Loss 0.638792]\n",
      "train [Epoch 0/10] [Batch 29/2345] [Iter 29] [G_Loss 2.213735] [D_Loss 0.556907]\n",
      "train [Epoch 0/10] [Batch 30/2345] [Iter 30] [G_Loss 2.233943] [D_Loss 0.622694]\n",
      "train [Epoch 0/10] [Batch 31/2345] [Iter 31] [G_Loss 2.130943] [D_Loss 0.450353]\n",
      "train [Epoch 0/10] [Batch 32/2345] [Iter 32] [G_Loss 2.597273] [D_Loss 0.489441]\n",
      "train [Epoch 0/10] [Batch 33/2345] [Iter 33] [G_Loss 1.889793] [D_Loss 0.541685]\n",
      "train [Epoch 0/10] [Batch 34/2345] [Iter 34] [G_Loss 5.043309] [D_Loss 0.618077]\n",
      "train [Epoch 0/10] [Batch 35/2345] [Iter 35] [G_Loss 2.113059] [D_Loss 0.591803]\n",
      "train [Epoch 0/10] [Batch 36/2345] [Iter 36] [G_Loss 3.813595] [D_Loss 0.541357]\n",
      "train [Epoch 0/10] [Batch 37/2345] [Iter 37] [G_Loss 2.707597] [D_Loss 0.454042]\n",
      "train [Epoch 0/10] [Batch 38/2345] [Iter 38] [G_Loss 2.737788] [D_Loss 0.464258]\n",
      "train [Epoch 0/10] [Batch 39/2345] [Iter 39] [G_Loss 2.501041] [D_Loss 0.440273]\n",
      "train [Epoch 0/10] [Batch 40/2345] [Iter 40] [G_Loss 3.317083] [D_Loss 0.558665]\n",
      "train [Epoch 0/10] [Batch 41/2345] [Iter 41] [G_Loss 2.065743] [D_Loss 0.825592]\n",
      "train [Epoch 0/10] [Batch 42/2345] [Iter 42] [G_Loss 5.622511] [D_Loss 0.934115]\n",
      "train [Epoch 0/10] [Batch 43/2345] [Iter 43] [G_Loss 1.087542] [D_Loss 1.425926]\n",
      "train [Epoch 0/10] [Batch 44/2345] [Iter 44] [G_Loss 4.243993] [D_Loss 0.866193]\n",
      "train [Epoch 0/10] [Batch 45/2345] [Iter 45] [G_Loss 3.534935] [D_Loss 0.463438]\n",
      "train [Epoch 0/10] [Batch 46/2345] [Iter 46] [G_Loss 2.422054] [D_Loss 0.519082]\n",
      "train [Epoch 0/10] [Batch 47/2345] [Iter 47] [G_Loss 2.345169] [D_Loss 0.574148]\n",
      "train [Epoch 0/10] [Batch 48/2345] [Iter 48] [G_Loss 2.202339] [D_Loss 0.576162]\n",
      "train [Epoch 0/10] [Batch 49/2345] [Iter 49] [G_Loss 2.377196] [D_Loss 0.492217]\n",
      "train [Epoch 0/10] [Batch 50/2345] [Iter 50] [G_Loss 2.439236] [D_Loss 0.498123]\n",
      "train [Epoch 0/10] [Batch 51/2345] [Iter 51] [G_Loss 1.854260] [D_Loss 0.558199]\n",
      "train [Epoch 0/10] [Batch 52/2345] [Iter 52] [G_Loss 2.745821] [D_Loss 0.650232]\n",
      "train [Epoch 0/10] [Batch 53/2345] [Iter 53] [G_Loss 2.042698] [D_Loss 0.493379]\n",
      "train [Epoch 0/10] [Batch 54/2345] [Iter 54] [G_Loss 3.085011] [D_Loss 0.568630]\n",
      "train [Epoch 0/10] [Batch 55/2345] [Iter 55] [G_Loss 1.691916] [D_Loss 0.713941]\n",
      "train [Epoch 0/10] [Batch 56/2345] [Iter 56] [G_Loss 5.097907] [D_Loss 0.604905]\n",
      "train [Epoch 0/10] [Batch 57/2345] [Iter 57] [G_Loss 3.228074] [D_Loss 0.652940]\n",
      "train [Epoch 0/10] [Batch 58/2345] [Iter 58] [G_Loss 2.140631] [D_Loss 0.910674]\n",
      "train [Epoch 0/10] [Batch 59/2345] [Iter 59] [G_Loss 4.476811] [D_Loss 0.669371]\n",
      "train [Epoch 0/10] [Batch 60/2345] [Iter 60] [G_Loss 2.182896] [D_Loss 0.796596]\n",
      "train [Epoch 0/10] [Batch 61/2345] [Iter 61] [G_Loss 1.134285] [D_Loss 0.972028]\n",
      "train [Epoch 0/10] [Batch 62/2345] [Iter 62] [G_Loss 3.606194] [D_Loss 0.785496]\n",
      "train [Epoch 0/10] [Batch 63/2345] [Iter 63] [G_Loss 2.498786] [D_Loss 0.524642]\n",
      "train [Epoch 0/10] [Batch 64/2345] [Iter 64] [G_Loss 1.989403] [D_Loss 0.606981]\n",
      "train [Epoch 0/10] [Batch 65/2345] [Iter 65] [G_Loss 1.802288] [D_Loss 0.683964]\n",
      "train [Epoch 0/10] [Batch 66/2345] [Iter 66] [G_Loss 2.487312] [D_Loss 0.583634]\n",
      "train [Epoch 0/10] [Batch 67/2345] [Iter 67] [G_Loss 2.056360] [D_Loss 0.616164]\n",
      "train [Epoch 0/10] [Batch 68/2345] [Iter 68] [G_Loss 1.612853] [D_Loss 0.656469]\n",
      "train [Epoch 0/10] [Batch 69/2345] [Iter 69] [G_Loss 2.800396] [D_Loss 0.546345]\n",
      "train [Epoch 0/10] [Batch 70/2345] [Iter 70] [G_Loss 2.637573] [D_Loss 0.469750]\n",
      "train [Epoch 0/10] [Batch 71/2345] [Iter 71] [G_Loss 2.147703] [D_Loss 0.624322]\n",
      "train [Epoch 0/10] [Batch 72/2345] [Iter 72] [G_Loss 3.515370] [D_Loss 0.679588]\n",
      "train [Epoch 0/10] [Batch 73/2345] [Iter 73] [G_Loss 1.467956] [D_Loss 0.699022]\n",
      "train [Epoch 0/10] [Batch 74/2345] [Iter 74] [G_Loss 3.848436] [D_Loss 1.049337]\n",
      "train [Epoch 0/10] [Batch 75/2345] [Iter 75] [G_Loss 1.140463] [D_Loss 0.843240]\n",
      "train [Epoch 0/10] [Batch 76/2345] [Iter 76] [G_Loss 3.169274] [D_Loss 0.687468]\n",
      "train [Epoch 0/10] [Batch 77/2345] [Iter 77] [G_Loss 2.986452] [D_Loss 0.545941]\n",
      "train [Epoch 0/10] [Batch 78/2345] [Iter 78] [G_Loss 2.149983] [D_Loss 0.864112]\n",
      "train [Epoch 0/10] [Batch 79/2345] [Iter 79] [G_Loss 1.833740] [D_Loss 0.566902]\n",
      "train [Epoch 0/10] [Batch 80/2345] [Iter 80] [G_Loss 2.375446] [D_Loss 0.459857]\n",
      "train [Epoch 0/10] [Batch 81/2345] [Iter 81] [G_Loss 2.332821] [D_Loss 0.653681]\n",
      "train [Epoch 0/10] [Batch 82/2345] [Iter 82] [G_Loss 1.763703] [D_Loss 0.470323]\n",
      "train [Epoch 0/10] [Batch 83/2345] [Iter 83] [G_Loss 2.441684] [D_Loss 0.578872]\n",
      "train [Epoch 0/10] [Batch 84/2345] [Iter 84] [G_Loss 3.169505] [D_Loss 0.666283]\n",
      "train [Epoch 0/10] [Batch 85/2345] [Iter 85] [G_Loss 2.070823] [D_Loss 0.802934]\n",
      "train [Epoch 0/10] [Batch 86/2345] [Iter 86] [G_Loss 2.248907] [D_Loss 0.517138]\n",
      "train [Epoch 0/10] [Batch 87/2345] [Iter 87] [G_Loss 2.048362] [D_Loss 0.641801]\n",
      "train [Epoch 0/10] [Batch 88/2345] [Iter 88] [G_Loss 2.607187] [D_Loss 0.642771]\n",
      "train [Epoch 0/10] [Batch 89/2345] [Iter 89] [G_Loss 2.556970] [D_Loss 0.482903]\n",
      "train [Epoch 0/10] [Batch 90/2345] [Iter 90] [G_Loss 2.576738] [D_Loss 0.631720]\n",
      "train [Epoch 0/10] [Batch 91/2345] [Iter 91] [G_Loss 2.806764] [D_Loss 0.714092]\n",
      "train [Epoch 0/10] [Batch 92/2345] [Iter 92] [G_Loss 3.147630] [D_Loss 0.633668]\n",
      "train [Epoch 0/10] [Batch 93/2345] [Iter 93] [G_Loss 2.136791] [D_Loss 0.958353]\n",
      "train [Epoch 0/10] [Batch 94/2345] [Iter 94] [G_Loss 3.193679] [D_Loss 0.645481]\n",
      "train [Epoch 0/10] [Batch 95/2345] [Iter 95] [G_Loss 1.927089] [D_Loss 0.679974]\n",
      "train [Epoch 0/10] [Batch 96/2345] [Iter 96] [G_Loss 1.929779] [D_Loss 0.656738]\n",
      "train [Epoch 0/10] [Batch 97/2345] [Iter 97] [G_Loss 1.580783] [D_Loss 0.884436]\n",
      "train [Epoch 0/10] [Batch 98/2345] [Iter 98] [G_Loss 1.508271] [D_Loss 1.159232]\n",
      "train [Epoch 0/10] [Batch 99/2345] [Iter 99] [G_Loss 6.587679] [D_Loss 1.433269]\n",
      "train [Epoch 0/10] [Batch 100/2345] [Iter 100] [G_Loss 1.704192] [D_Loss 0.968593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 101/2345] [Iter 101] [G_Loss 1.085752] [D_Loss 0.844079]\n",
      "train [Epoch 0/10] [Batch 102/2345] [Iter 102] [G_Loss 1.307188] [D_Loss 1.164947]\n",
      "train [Epoch 0/10] [Batch 103/2345] [Iter 103] [G_Loss 0.636037] [D_Loss 1.652486]\n",
      "train [Epoch 0/10] [Batch 104/2345] [Iter 104] [G_Loss 2.849054] [D_Loss 0.761956]\n",
      "train [Epoch 0/10] [Batch 105/2345] [Iter 105] [G_Loss 2.611170] [D_Loss 0.766232]\n",
      "train [Epoch 0/10] [Batch 106/2345] [Iter 106] [G_Loss 1.534588] [D_Loss 0.949279]\n",
      "train [Epoch 0/10] [Batch 107/2345] [Iter 107] [G_Loss 2.070915] [D_Loss 0.679532]\n",
      "train [Epoch 0/10] [Batch 108/2345] [Iter 108] [G_Loss 2.255796] [D_Loss 0.768639]\n",
      "train [Epoch 0/10] [Batch 109/2345] [Iter 109] [G_Loss 1.656849] [D_Loss 0.679134]\n",
      "train [Epoch 0/10] [Batch 110/2345] [Iter 110] [G_Loss 1.659516] [D_Loss 0.669007]\n",
      "train [Epoch 0/10] [Batch 111/2345] [Iter 111] [G_Loss 1.560882] [D_Loss 0.743038]\n",
      "train [Epoch 0/10] [Batch 112/2345] [Iter 112] [G_Loss 1.563963] [D_Loss 0.647347]\n",
      "train [Epoch 0/10] [Batch 113/2345] [Iter 113] [G_Loss 1.273588] [D_Loss 0.797556]\n",
      "train [Epoch 0/10] [Batch 114/2345] [Iter 114] [G_Loss 2.823316] [D_Loss 0.760053]\n",
      "train [Epoch 0/10] [Batch 115/2345] [Iter 115] [G_Loss 1.896545] [D_Loss 0.808166]\n",
      "train [Epoch 0/10] [Batch 116/2345] [Iter 116] [G_Loss 2.464791] [D_Loss 0.561243]\n",
      "train [Epoch 0/10] [Batch 117/2345] [Iter 117] [G_Loss 3.032380] [D_Loss 0.576176]\n",
      "train [Epoch 0/10] [Batch 118/2345] [Iter 118] [G_Loss 2.425960] [D_Loss 0.778255]\n",
      "train [Epoch 0/10] [Batch 119/2345] [Iter 119] [G_Loss 2.970179] [D_Loss 0.633914]\n",
      "train [Epoch 0/10] [Batch 120/2345] [Iter 120] [G_Loss 1.535788] [D_Loss 1.063524]\n",
      "train [Epoch 0/10] [Batch 121/2345] [Iter 121] [G_Loss 7.122699] [D_Loss 1.227064]\n",
      "train [Epoch 0/10] [Batch 122/2345] [Iter 122] [G_Loss 2.701071] [D_Loss 0.600825]\n",
      "train [Epoch 0/10] [Batch 123/2345] [Iter 123] [G_Loss 1.878181] [D_Loss 0.660761]\n",
      "train [Epoch 0/10] [Batch 124/2345] [Iter 124] [G_Loss 2.059456] [D_Loss 0.510598]\n",
      "train [Epoch 0/10] [Batch 125/2345] [Iter 125] [G_Loss 2.102433] [D_Loss 0.665779]\n",
      "train [Epoch 0/10] [Batch 126/2345] [Iter 126] [G_Loss 2.156286] [D_Loss 0.663747]\n",
      "train [Epoch 0/10] [Batch 127/2345] [Iter 127] [G_Loss 2.275440] [D_Loss 0.647004]\n",
      "train [Epoch 0/10] [Batch 128/2345] [Iter 128] [G_Loss 1.737592] [D_Loss 0.567265]\n",
      "train [Epoch 0/10] [Batch 129/2345] [Iter 129] [G_Loss 1.565364] [D_Loss 0.593806]\n",
      "train [Epoch 0/10] [Batch 130/2345] [Iter 130] [G_Loss 1.990948] [D_Loss 0.473944]\n",
      "train [Epoch 0/10] [Batch 131/2345] [Iter 131] [G_Loss 1.757720] [D_Loss 0.613014]\n",
      "train [Epoch 0/10] [Batch 132/2345] [Iter 132] [G_Loss 1.889439] [D_Loss 0.506210]\n",
      "train [Epoch 0/10] [Batch 133/2345] [Iter 133] [G_Loss 3.553859] [D_Loss 0.639631]\n",
      "train [Epoch 0/10] [Batch 134/2345] [Iter 134] [G_Loss 2.559664] [D_Loss 0.624150]\n",
      "train [Epoch 0/10] [Batch 135/2345] [Iter 135] [G_Loss 3.421448] [D_Loss 0.909798]\n",
      "train [Epoch 0/10] [Batch 136/2345] [Iter 136] [G_Loss 2.209249] [D_Loss 0.985983]\n",
      "train [Epoch 0/10] [Batch 137/2345] [Iter 137] [G_Loss 2.410108] [D_Loss 0.645727]\n",
      "train [Epoch 0/10] [Batch 138/2345] [Iter 138] [G_Loss 2.307066] [D_Loss 0.691784]\n",
      "train [Epoch 0/10] [Batch 139/2345] [Iter 139] [G_Loss 1.699835] [D_Loss 0.516960]\n",
      "train [Epoch 0/10] [Batch 140/2345] [Iter 140] [G_Loss 2.444669] [D_Loss 0.788572]\n",
      "train [Epoch 0/10] [Batch 141/2345] [Iter 141] [G_Loss 2.617069] [D_Loss 0.632051]\n",
      "train [Epoch 0/10] [Batch 142/2345] [Iter 142] [G_Loss 2.233972] [D_Loss 1.003550]\n",
      "train [Epoch 0/10] [Batch 143/2345] [Iter 143] [G_Loss 2.364070] [D_Loss 0.846324]\n",
      "train [Epoch 0/10] [Batch 144/2345] [Iter 144] [G_Loss 1.203174] [D_Loss 0.955132]\n",
      "train [Epoch 0/10] [Batch 145/2345] [Iter 145] [G_Loss 1.875437] [D_Loss 0.869134]\n",
      "train [Epoch 0/10] [Batch 146/2345] [Iter 146] [G_Loss 1.204003] [D_Loss 0.827465]\n",
      "train [Epoch 0/10] [Batch 147/2345] [Iter 147] [G_Loss 1.658459] [D_Loss 0.826377]\n",
      "train [Epoch 0/10] [Batch 148/2345] [Iter 148] [G_Loss 1.690584] [D_Loss 0.744017]\n",
      "train [Epoch 0/10] [Batch 149/2345] [Iter 149] [G_Loss 1.512250] [D_Loss 0.700400]\n",
      "train [Epoch 0/10] [Batch 150/2345] [Iter 150] [G_Loss 2.447492] [D_Loss 0.810104]\n",
      "train [Epoch 0/10] [Batch 151/2345] [Iter 151] [G_Loss 1.257944] [D_Loss 0.900997]\n",
      "train [Epoch 0/10] [Batch 152/2345] [Iter 152] [G_Loss 1.731853] [D_Loss 0.608351]\n",
      "train [Epoch 0/10] [Batch 153/2345] [Iter 153] [G_Loss 1.917740] [D_Loss 0.600584]\n",
      "train [Epoch 0/10] [Batch 154/2345] [Iter 154] [G_Loss 1.628592] [D_Loss 0.658932]\n",
      "train [Epoch 0/10] [Batch 155/2345] [Iter 155] [G_Loss 2.706036] [D_Loss 0.808407]\n",
      "train [Epoch 0/10] [Batch 156/2345] [Iter 156] [G_Loss 2.245545] [D_Loss 0.552873]\n",
      "train [Epoch 0/10] [Batch 157/2345] [Iter 157] [G_Loss 2.598806] [D_Loss 0.609666]\n",
      "train [Epoch 0/10] [Batch 158/2345] [Iter 158] [G_Loss 2.069127] [D_Loss 0.610584]\n",
      "train [Epoch 0/10] [Batch 159/2345] [Iter 159] [G_Loss 3.770587] [D_Loss 0.659785]\n",
      "train [Epoch 0/10] [Batch 160/2345] [Iter 160] [G_Loss 2.152580] [D_Loss 0.450719]\n",
      "train [Epoch 0/10] [Batch 161/2345] [Iter 161] [G_Loss 2.236203] [D_Loss 0.540823]\n",
      "train [Epoch 0/10] [Batch 162/2345] [Iter 162] [G_Loss 2.657971] [D_Loss 0.493948]\n",
      "train [Epoch 0/10] [Batch 163/2345] [Iter 163] [G_Loss 2.959877] [D_Loss 0.457126]\n",
      "train [Epoch 0/10] [Batch 164/2345] [Iter 164] [G_Loss 2.913591] [D_Loss 0.601204]\n",
      "train [Epoch 0/10] [Batch 166/2345] [Iter 166] [G_Loss 1.695883] [D_Loss 1.281701]\n",
      "train [Epoch 0/10] [Batch 167/2345] [Iter 167] [G_Loss 1.371941] [D_Loss 0.770693]\n",
      "train [Epoch 0/10] [Batch 168/2345] [Iter 168] [G_Loss 2.363436] [D_Loss 0.834454]\n",
      "train [Epoch 0/10] [Batch 169/2345] [Iter 169] [G_Loss 2.395161] [D_Loss 0.774960]\n",
      "train [Epoch 0/10] [Batch 170/2345] [Iter 170] [G_Loss 1.847029] [D_Loss 0.735437]\n",
      "train [Epoch 0/10] [Batch 171/2345] [Iter 171] [G_Loss 1.780461] [D_Loss 0.665279]\n",
      "train [Epoch 0/10] [Batch 172/2345] [Iter 172] [G_Loss 1.521037] [D_Loss 0.964627]\n",
      "train [Epoch 0/10] [Batch 173/2345] [Iter 173] [G_Loss 1.537826] [D_Loss 0.802894]\n",
      "train [Epoch 0/10] [Batch 174/2345] [Iter 174] [G_Loss 1.506348] [D_Loss 0.721032]\n",
      "train [Epoch 0/10] [Batch 175/2345] [Iter 175] [G_Loss 1.597777] [D_Loss 0.653601]\n",
      "train [Epoch 0/10] [Batch 176/2345] [Iter 176] [G_Loss 1.345202] [D_Loss 0.893704]\n",
      "train [Epoch 0/10] [Batch 177/2345] [Iter 177] [G_Loss 1.936923] [D_Loss 0.758321]\n",
      "train [Epoch 0/10] [Batch 178/2345] [Iter 178] [G_Loss 0.914706] [D_Loss 0.941160]\n",
      "train [Epoch 0/10] [Batch 179/2345] [Iter 179] [G_Loss 2.621919] [D_Loss 0.715793]\n",
      "train [Epoch 0/10] [Batch 180/2345] [Iter 180] [G_Loss 2.150381] [D_Loss 0.792955]\n",
      "train [Epoch 0/10] [Batch 181/2345] [Iter 181] [G_Loss 2.768841] [D_Loss 0.781747]\n",
      "train [Epoch 0/10] [Batch 182/2345] [Iter 182] [G_Loss 1.636154] [D_Loss 0.905120]\n",
      "train [Epoch 0/10] [Batch 183/2345] [Iter 183] [G_Loss 1.968100] [D_Loss 0.943241]\n",
      "train [Epoch 0/10] [Batch 184/2345] [Iter 184] [G_Loss 0.934749] [D_Loss 0.955933]\n",
      "train [Epoch 0/10] [Batch 185/2345] [Iter 185] [G_Loss 2.522652] [D_Loss 1.247755]\n",
      "train [Epoch 0/10] [Batch 186/2345] [Iter 186] [G_Loss 1.215426] [D_Loss 0.903524]\n",
      "train [Epoch 0/10] [Batch 187/2345] [Iter 187] [G_Loss 1.171132] [D_Loss 0.857845]\n",
      "train [Epoch 0/10] [Batch 188/2345] [Iter 188] [G_Loss 1.646012] [D_Loss 0.693351]\n",
      "train [Epoch 0/10] [Batch 189/2345] [Iter 189] [G_Loss 1.687255] [D_Loss 0.798733]\n",
      "train [Epoch 0/10] [Batch 190/2345] [Iter 190] [G_Loss 1.622331] [D_Loss 0.557090]\n",
      "train [Epoch 0/10] [Batch 191/2345] [Iter 191] [G_Loss 2.538739] [D_Loss 0.473958]\n",
      "train [Epoch 0/10] [Batch 192/2345] [Iter 192] [G_Loss 1.947882] [D_Loss 0.642831]\n",
      "train [Epoch 0/10] [Batch 193/2345] [Iter 193] [G_Loss 1.843472] [D_Loss 0.702015]\n",
      "train [Epoch 0/10] [Batch 194/2345] [Iter 194] [G_Loss 3.372481] [D_Loss 0.805723]\n",
      "train [Epoch 0/10] [Batch 195/2345] [Iter 195] [G_Loss 1.276217] [D_Loss 1.581724]\n",
      "train [Epoch 0/10] [Batch 196/2345] [Iter 196] [G_Loss 4.445293] [D_Loss 0.550985]\n",
      "train [Epoch 0/10] [Batch 197/2345] [Iter 197] [G_Loss 3.138423] [D_Loss 0.901342]\n",
      "train [Epoch 0/10] [Batch 198/2345] [Iter 198] [G_Loss 1.562253] [D_Loss 0.848036]\n",
      "train [Epoch 0/10] [Batch 199/2345] [Iter 199] [G_Loss 1.597110] [D_Loss 1.074443]\n",
      "train [Epoch 0/10] [Batch 200/2345] [Iter 200] [G_Loss 1.402521] [D_Loss 0.830477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 201/2345] [Iter 201] [G_Loss 1.832077] [D_Loss 0.775806]\n",
      "train [Epoch 0/10] [Batch 202/2345] [Iter 202] [G_Loss 1.515338] [D_Loss 0.609049]\n",
      "train [Epoch 0/10] [Batch 203/2345] [Iter 203] [G_Loss 1.308689] [D_Loss 0.636974]\n",
      "train [Epoch 0/10] [Batch 204/2345] [Iter 204] [G_Loss 1.595215] [D_Loss 0.611809]\n",
      "train [Epoch 0/10] [Batch 205/2345] [Iter 205] [G_Loss 1.436140] [D_Loss 0.734127]\n",
      "train [Epoch 0/10] [Batch 206/2345] [Iter 206] [G_Loss 2.330035] [D_Loss 0.568254]\n",
      "train [Epoch 0/10] [Batch 207/2345] [Iter 207] [G_Loss 2.135788] [D_Loss 0.486647]\n",
      "train [Epoch 0/10] [Batch 208/2345] [Iter 208] [G_Loss 3.574006] [D_Loss 0.549781]\n",
      "train [Epoch 0/10] [Batch 209/2345] [Iter 209] [G_Loss 2.412510] [D_Loss 0.451221]\n",
      "train [Epoch 0/10] [Batch 210/2345] [Iter 210] [G_Loss 2.377070] [D_Loss 0.809464]\n",
      "train [Epoch 0/10] [Batch 211/2345] [Iter 211] [G_Loss 3.303960] [D_Loss 0.622221]\n",
      "train [Epoch 0/10] [Batch 212/2345] [Iter 212] [G_Loss 1.250336] [D_Loss 0.743313]\n",
      "train [Epoch 0/10] [Batch 213/2345] [Iter 213] [G_Loss 3.812486] [D_Loss 0.683608]\n",
      "train [Epoch 0/10] [Batch 214/2345] [Iter 214] [G_Loss 1.373163] [D_Loss 0.801828]\n",
      "train [Epoch 0/10] [Batch 215/2345] [Iter 215] [G_Loss 1.315560] [D_Loss 0.752724]\n",
      "train [Epoch 0/10] [Batch 216/2345] [Iter 216] [G_Loss 1.574929] [D_Loss 0.675704]\n",
      "train [Epoch 0/10] [Batch 217/2345] [Iter 217] [G_Loss 2.492531] [D_Loss 1.022558]\n",
      "train [Epoch 0/10] [Batch 218/2345] [Iter 218] [G_Loss 2.488683] [D_Loss 1.014963]\n",
      "train [Epoch 0/10] [Batch 219/2345] [Iter 219] [G_Loss 1.649625] [D_Loss 0.872239]\n",
      "train [Epoch 0/10] [Batch 220/2345] [Iter 220] [G_Loss 1.478092] [D_Loss 0.842073]\n",
      "train [Epoch 0/10] [Batch 221/2345] [Iter 221] [G_Loss 1.370414] [D_Loss 0.684873]\n",
      "train [Epoch 0/10] [Batch 222/2345] [Iter 222] [G_Loss 1.643306] [D_Loss 0.634246]\n",
      "train [Epoch 0/10] [Batch 223/2345] [Iter 223] [G_Loss 2.437113] [D_Loss 0.671988]\n",
      "train [Epoch 0/10] [Batch 224/2345] [Iter 224] [G_Loss 2.489337] [D_Loss 0.695348]\n",
      "train [Epoch 0/10] [Batch 225/2345] [Iter 225] [G_Loss 1.967813] [D_Loss 0.664807]\n",
      "train [Epoch 0/10] [Batch 226/2345] [Iter 226] [G_Loss 2.446139] [D_Loss 0.548347]\n",
      "train [Epoch 0/10] [Batch 227/2345] [Iter 227] [G_Loss 1.492437] [D_Loss 0.704930]\n",
      "train [Epoch 0/10] [Batch 228/2345] [Iter 228] [G_Loss 2.015138] [D_Loss 0.676189]\n",
      "train [Epoch 0/10] [Batch 229/2345] [Iter 229] [G_Loss 1.655938] [D_Loss 0.482010]\n",
      "train [Epoch 0/10] [Batch 230/2345] [Iter 230] [G_Loss 1.377312] [D_Loss 0.575129]\n",
      "train [Epoch 0/10] [Batch 231/2345] [Iter 231] [G_Loss 1.713536] [D_Loss 0.607645]\n",
      "train [Epoch 0/10] [Batch 232/2345] [Iter 232] [G_Loss 1.935130] [D_Loss 0.564168]\n",
      "train [Epoch 0/10] [Batch 233/2345] [Iter 233] [G_Loss 1.803655] [D_Loss 1.222155]\n",
      "train [Epoch 0/10] [Batch 234/2345] [Iter 234] [G_Loss 3.530670] [D_Loss 0.943553]\n",
      "train [Epoch 0/10] [Batch 235/2345] [Iter 235] [G_Loss 0.892580] [D_Loss 0.912526]\n",
      "train [Epoch 0/10] [Batch 236/2345] [Iter 236] [G_Loss 3.208797] [D_Loss 1.459996]\n",
      "train [Epoch 0/10] [Batch 237/2345] [Iter 237] [G_Loss 1.051967] [D_Loss 1.099982]\n",
      "train [Epoch 0/10] [Batch 238/2345] [Iter 238] [G_Loss 0.888178] [D_Loss 1.004719]\n",
      "train [Epoch 0/10] [Batch 239/2345] [Iter 239] [G_Loss 2.046325] [D_Loss 1.168730]\n",
      "train [Epoch 0/10] [Batch 240/2345] [Iter 240] [G_Loss 0.916218] [D_Loss 0.892427]\n",
      "train [Epoch 0/10] [Batch 241/2345] [Iter 241] [G_Loss 1.059620] [D_Loss 1.419974]\n",
      "train [Epoch 0/10] [Batch 242/2345] [Iter 242] [G_Loss 2.097242] [D_Loss 0.914346]\n",
      "train [Epoch 0/10] [Batch 243/2345] [Iter 243] [G_Loss 1.791301] [D_Loss 0.599649]\n",
      "train [Epoch 0/10] [Batch 244/2345] [Iter 244] [G_Loss 2.220646] [D_Loss 0.429959]\n",
      "train [Epoch 0/10] [Batch 245/2345] [Iter 245] [G_Loss 2.554283] [D_Loss 0.491091]\n",
      "train [Epoch 0/10] [Batch 246/2345] [Iter 246] [G_Loss 1.612908] [D_Loss 0.995827]\n",
      "train [Epoch 0/10] [Batch 247/2345] [Iter 247] [G_Loss 2.541911] [D_Loss 0.752809]\n",
      "train [Epoch 0/10] [Batch 248/2345] [Iter 248] [G_Loss 1.967966] [D_Loss 0.720654]\n",
      "train [Epoch 0/10] [Batch 249/2345] [Iter 249] [G_Loss 0.743182] [D_Loss 1.001577]\n",
      "train [Epoch 0/10] [Batch 250/2345] [Iter 250] [G_Loss 1.744954] [D_Loss 0.783986]\n",
      "train [Epoch 0/10] [Batch 251/2345] [Iter 251] [G_Loss 1.281708] [D_Loss 0.870507]\n",
      "train [Epoch 0/10] [Batch 252/2345] [Iter 252] [G_Loss 1.215206] [D_Loss 0.953014]\n",
      "train [Epoch 0/10] [Batch 253/2345] [Iter 253] [G_Loss 1.537964] [D_Loss 0.990132]\n",
      "train [Epoch 0/10] [Batch 254/2345] [Iter 254] [G_Loss 1.332525] [D_Loss 0.822878]\n",
      "train [Epoch 0/10] [Batch 255/2345] [Iter 255] [G_Loss 1.543962] [D_Loss 0.780928]\n",
      "train [Epoch 0/10] [Batch 256/2345] [Iter 256] [G_Loss 1.782634] [D_Loss 0.763402]\n",
      "train [Epoch 0/10] [Batch 257/2345] [Iter 257] [G_Loss 1.908043] [D_Loss 0.565171]\n",
      "train [Epoch 0/10] [Batch 258/2345] [Iter 258] [G_Loss 1.789881] [D_Loss 0.946962]\n",
      "train [Epoch 0/10] [Batch 259/2345] [Iter 259] [G_Loss 2.762130] [D_Loss 1.550906]\n",
      "train [Epoch 0/10] [Batch 260/2345] [Iter 260] [G_Loss 0.272890] [D_Loss 2.045277]\n",
      "train [Epoch 0/10] [Batch 261/2345] [Iter 261] [G_Loss 1.635063] [D_Loss 0.725793]\n",
      "train [Epoch 0/10] [Batch 262/2345] [Iter 262] [G_Loss 0.871614] [D_Loss 0.958666]\n",
      "train [Epoch 0/10] [Batch 263/2345] [Iter 263] [G_Loss 1.814228] [D_Loss 1.034986]\n",
      "train [Epoch 0/10] [Batch 264/2345] [Iter 264] [G_Loss 1.168424] [D_Loss 0.749964]\n",
      "train [Epoch 0/10] [Batch 265/2345] [Iter 265] [G_Loss 1.164029] [D_Loss 0.762447]\n",
      "train [Epoch 0/10] [Batch 266/2345] [Iter 266] [G_Loss 1.478861] [D_Loss 0.634262]\n",
      "train [Epoch 0/10] [Batch 267/2345] [Iter 267] [G_Loss 1.769819] [D_Loss 0.679840]\n",
      "train [Epoch 0/10] [Batch 268/2345] [Iter 268] [G_Loss 1.760012] [D_Loss 0.712595]\n",
      "train [Epoch 0/10] [Batch 269/2345] [Iter 269] [G_Loss 1.681704] [D_Loss 0.691745]\n",
      "train [Epoch 0/10] [Batch 270/2345] [Iter 270] [G_Loss 1.651674] [D_Loss 0.690285]\n",
      "train [Epoch 0/10] [Batch 271/2345] [Iter 271] [G_Loss 1.190895] [D_Loss 0.739416]\n",
      "train [Epoch 0/10] [Batch 272/2345] [Iter 272] [G_Loss 2.886365] [D_Loss 1.049472]\n",
      "train [Epoch 0/10] [Batch 273/2345] [Iter 273] [G_Loss 5.247231] [D_Loss 1.081395]\n",
      "train [Epoch 0/10] [Batch 274/2345] [Iter 274] [G_Loss 3.215505] [D_Loss 0.561287]\n",
      "train [Epoch 0/10] [Batch 275/2345] [Iter 275] [G_Loss 24.548595] [D_Loss 3.656608]\n",
      "train [Epoch 0/10] [Batch 276/2345] [Iter 276] [G_Loss 0.489703] [D_Loss 2.370447]\n",
      "train [Epoch 0/10] [Batch 277/2345] [Iter 277] [G_Loss 4.651306] [D_Loss 0.761864]\n",
      "train [Epoch 0/10] [Batch 278/2345] [Iter 278] [G_Loss 3.691336] [D_Loss 0.462526]\n",
      "train [Epoch 0/10] [Batch 279/2345] [Iter 279] [G_Loss 1.775521] [D_Loss 0.576067]\n",
      "train [Epoch 0/10] [Batch 280/2345] [Iter 280] [G_Loss 2.594200] [D_Loss 0.564689]\n",
      "train [Epoch 0/10] [Batch 281/2345] [Iter 281] [G_Loss 3.642196] [D_Loss 0.541092]\n",
      "train [Epoch 0/10] [Batch 282/2345] [Iter 282] [G_Loss 3.237149] [D_Loss 0.334444]\n",
      "train [Epoch 0/10] [Batch 283/2345] [Iter 283] [G_Loss 2.629919] [D_Loss 0.370175]\n",
      "train [Epoch 0/10] [Batch 284/2345] [Iter 284] [G_Loss 3.250173] [D_Loss 0.482962]\n",
      "train [Epoch 0/10] [Batch 285/2345] [Iter 285] [G_Loss 1.313670] [D_Loss 0.791494]\n",
      "train [Epoch 0/10] [Batch 286/2345] [Iter 286] [G_Loss 5.405901] [D_Loss 2.736325]\n",
      "train [Epoch 0/10] [Batch 287/2345] [Iter 287] [G_Loss 0.099690] [D_Loss 3.039041]\n",
      "train [Epoch 0/10] [Batch 288/2345] [Iter 288] [G_Loss 2.524122] [D_Loss 0.859690]\n",
      "train [Epoch 0/10] [Batch 289/2345] [Iter 289] [G_Loss 2.480544] [D_Loss 0.766666]\n",
      "train [Epoch 0/10] [Batch 290/2345] [Iter 290] [G_Loss 1.755378] [D_Loss 0.603512]\n",
      "train [Epoch 0/10] [Batch 291/2345] [Iter 291] [G_Loss 1.950904] [D_Loss 0.547177]\n",
      "train [Epoch 0/10] [Batch 292/2345] [Iter 292] [G_Loss 4.282646] [D_Loss 0.958552]\n",
      "train [Epoch 0/10] [Batch 293/2345] [Iter 293] [G_Loss 0.319931] [D_Loss 2.538620]\n",
      "train [Epoch 0/10] [Batch 294/2345] [Iter 294] [G_Loss 1.683988] [D_Loss 0.513704]\n",
      "train [Epoch 0/10] [Batch 295/2345] [Iter 295] [G_Loss 2.799392] [D_Loss 0.465056]\n",
      "train [Epoch 0/10] [Batch 296/2345] [Iter 296] [G_Loss 2.386303] [D_Loss 0.449873]\n",
      "train [Epoch 0/10] [Batch 297/2345] [Iter 297] [G_Loss 0.779719] [D_Loss 0.965755]\n",
      "train [Epoch 0/10] [Batch 298/2345] [Iter 298] [G_Loss 7.450002] [D_Loss 5.145976]\n",
      "train [Epoch 0/10] [Batch 299/2345] [Iter 299] [G_Loss 1.287130] [D_Loss 1.187548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 300/2345] [Iter 300] [G_Loss 0.874904] [D_Loss 1.025306]\n",
      "train [Epoch 0/10] [Batch 301/2345] [Iter 301] [G_Loss 0.791601] [D_Loss 1.050784]\n",
      "train [Epoch 0/10] [Batch 302/2345] [Iter 302] [G_Loss 0.913387] [D_Loss 1.007380]\n",
      "train [Epoch 0/10] [Batch 303/2345] [Iter 303] [G_Loss 1.136974] [D_Loss 0.887584]\n",
      "train [Epoch 0/10] [Batch 304/2345] [Iter 304] [G_Loss 1.366886] [D_Loss 0.830682]\n",
      "train [Epoch 0/10] [Batch 305/2345] [Iter 305] [G_Loss 1.174173] [D_Loss 0.785846]\n",
      "train [Epoch 0/10] [Batch 306/2345] [Iter 306] [G_Loss 4.978845] [D_Loss 2.066573]\n",
      "train [Epoch 0/10] [Batch 307/2345] [Iter 307] [G_Loss 0.056075] [D_Loss 4.346786]\n",
      "train [Epoch 0/10] [Batch 308/2345] [Iter 308] [G_Loss 1.220800] [D_Loss 0.856248]\n",
      "train [Epoch 0/10] [Batch 309/2345] [Iter 309] [G_Loss 7.666074] [D_Loss 1.217994]\n",
      "train [Epoch 0/10] [Batch 310/2345] [Iter 310] [G_Loss 1.437885] [D_Loss 1.336709]\n",
      "train [Epoch 0/10] [Batch 311/2345] [Iter 311] [G_Loss 0.932631] [D_Loss 0.924045]\n",
      "train [Epoch 0/10] [Batch 312/2345] [Iter 312] [G_Loss 1.844826] [D_Loss 0.800109]\n",
      "train [Epoch 0/10] [Batch 313/2345] [Iter 313] [G_Loss 2.586605] [D_Loss 0.606445]\n",
      "train [Epoch 0/10] [Batch 314/2345] [Iter 314] [G_Loss 3.112116] [D_Loss 0.666257]\n",
      "train [Epoch 0/10] [Batch 315/2345] [Iter 315] [G_Loss 2.318418] [D_Loss 0.546688]\n",
      "train [Epoch 0/10] [Batch 316/2345] [Iter 316] [G_Loss 2.485403] [D_Loss 0.473849]\n",
      "train [Epoch 0/10] [Batch 317/2345] [Iter 317] [G_Loss 2.658813] [D_Loss 0.647633]\n",
      "train [Epoch 0/10] [Batch 318/2345] [Iter 318] [G_Loss 2.263792] [D_Loss 0.834844]\n",
      "train [Epoch 0/10] [Batch 319/2345] [Iter 319] [G_Loss 1.172568] [D_Loss 0.669138]\n",
      "train [Epoch 0/10] [Batch 320/2345] [Iter 320] [G_Loss 1.030811] [D_Loss 0.852718]\n",
      "train [Epoch 0/10] [Batch 321/2345] [Iter 321] [G_Loss 1.807496] [D_Loss 0.820004]\n",
      "train [Epoch 0/10] [Batch 322/2345] [Iter 322] [G_Loss 1.544052] [D_Loss 0.767045]\n",
      "train [Epoch 0/10] [Batch 323/2345] [Iter 323] [G_Loss 1.763172] [D_Loss 0.726313]\n",
      "train [Epoch 0/10] [Batch 324/2345] [Iter 324] [G_Loss 2.536195] [D_Loss 0.607005]\n",
      "train [Epoch 0/10] [Batch 325/2345] [Iter 325] [G_Loss 2.524660] [D_Loss 0.772626]\n",
      "train [Epoch 0/10] [Batch 326/2345] [Iter 326] [G_Loss 3.146937] [D_Loss 0.532896]\n",
      "train [Epoch 0/10] [Batch 327/2345] [Iter 327] [G_Loss 1.441110] [D_Loss 0.896015]\n",
      "train [Epoch 0/10] [Batch 334/2345] [Iter 334] [G_Loss 1.780254] [D_Loss 0.764422]\n",
      "train [Epoch 0/10] [Batch 335/2345] [Iter 335] [G_Loss 2.126687] [D_Loss 0.625208]\n",
      "train [Epoch 0/10] [Batch 336/2345] [Iter 336] [G_Loss 1.290596] [D_Loss 0.833459]\n",
      "train [Epoch 0/10] [Batch 337/2345] [Iter 337] [G_Loss 1.856951] [D_Loss 0.887040]\n",
      "train [Epoch 0/10] [Batch 338/2345] [Iter 338] [G_Loss 0.743344] [D_Loss 1.269170]\n",
      "train [Epoch 0/10] [Batch 339/2345] [Iter 339] [G_Loss 1.558662] [D_Loss 0.890511]\n",
      "train [Epoch 0/10] [Batch 340/2345] [Iter 340] [G_Loss 1.486118] [D_Loss 1.085266]\n",
      "train [Epoch 0/10] [Batch 341/2345] [Iter 341] [G_Loss 0.930981] [D_Loss 0.898519]\n",
      "train [Epoch 0/10] [Batch 342/2345] [Iter 342] [G_Loss 1.692239] [D_Loss 0.798004]\n",
      "train [Epoch 0/10] [Batch 343/2345] [Iter 343] [G_Loss 0.927244] [D_Loss 0.906792]\n",
      "train [Epoch 0/10] [Batch 344/2345] [Iter 344] [G_Loss 1.400845] [D_Loss 0.717028]\n",
      "train [Epoch 0/10] [Batch 345/2345] [Iter 345] [G_Loss 1.241145] [D_Loss 0.779921]\n",
      "train [Epoch 0/10] [Batch 346/2345] [Iter 346] [G_Loss 1.161549] [D_Loss 0.973172]\n",
      "train [Epoch 0/10] [Batch 347/2345] [Iter 347] [G_Loss 1.179121] [D_Loss 0.867815]\n",
      "train [Epoch 0/10] [Batch 348/2345] [Iter 348] [G_Loss 1.748584] [D_Loss 0.900470]\n",
      "train [Epoch 0/10] [Batch 349/2345] [Iter 349] [G_Loss 1.183018] [D_Loss 0.811648]\n",
      "train [Epoch 0/10] [Batch 350/2345] [Iter 350] [G_Loss 1.069196] [D_Loss 1.042390]\n",
      "train [Epoch 0/10] [Batch 351/2345] [Iter 351] [G_Loss 1.447289] [D_Loss 1.256446]\n",
      "train [Epoch 0/10] [Batch 352/2345] [Iter 352] [G_Loss 1.348367] [D_Loss 0.902499]\n",
      "train [Epoch 0/10] [Batch 353/2345] [Iter 353] [G_Loss 0.979141] [D_Loss 0.951547]\n",
      "train [Epoch 0/10] [Batch 354/2345] [Iter 354] [G_Loss 1.186624] [D_Loss 0.987127]\n",
      "train [Epoch 0/10] [Batch 355/2345] [Iter 355] [G_Loss 1.187482] [D_Loss 0.919001]\n",
      "train [Epoch 0/10] [Batch 356/2345] [Iter 356] [G_Loss 1.116014] [D_Loss 0.844255]\n",
      "train [Epoch 0/10] [Batch 357/2345] [Iter 357] [G_Loss 1.661668] [D_Loss 0.793680]\n",
      "train [Epoch 0/10] [Batch 358/2345] [Iter 358] [G_Loss 1.081479] [D_Loss 1.030947]\n",
      "train [Epoch 0/10] [Batch 359/2345] [Iter 359] [G_Loss 1.176401] [D_Loss 0.881433]\n",
      "train [Epoch 0/10] [Batch 360/2345] [Iter 360] [G_Loss 1.864744] [D_Loss 1.030170]\n",
      "train [Epoch 0/10] [Batch 361/2345] [Iter 361] [G_Loss 0.574514] [D_Loss 1.415129]\n",
      "train [Epoch 0/10] [Batch 362/2345] [Iter 362] [G_Loss 1.994204] [D_Loss 1.046329]\n",
      "train [Epoch 0/10] [Batch 363/2345] [Iter 363] [G_Loss 1.132195] [D_Loss 0.805886]\n",
      "train [Epoch 0/10] [Batch 364/2345] [Iter 364] [G_Loss 0.822255] [D_Loss 1.035587]\n",
      "train [Epoch 0/10] [Batch 365/2345] [Iter 365] [G_Loss 1.444813] [D_Loss 0.995066]\n",
      "train [Epoch 0/10] [Batch 366/2345] [Iter 366] [G_Loss 1.736347] [D_Loss 0.925051]\n",
      "train [Epoch 0/10] [Batch 367/2345] [Iter 367] [G_Loss 1.443404] [D_Loss 0.757894]\n",
      "train [Epoch 0/10] [Batch 368/2345] [Iter 368] [G_Loss 1.793659] [D_Loss 1.009717]\n",
      "train [Epoch 0/10] [Batch 369/2345] [Iter 369] [G_Loss 1.418585] [D_Loss 0.880086]\n",
      "train [Epoch 0/10] [Batch 370/2345] [Iter 370] [G_Loss 1.077316] [D_Loss 1.234686]\n",
      "train [Epoch 0/10] [Batch 371/2345] [Iter 371] [G_Loss 1.054710] [D_Loss 1.032487]\n",
      "train [Epoch 0/10] [Batch 372/2345] [Iter 372] [G_Loss 1.212597] [D_Loss 0.980976]\n",
      "train [Epoch 0/10] [Batch 373/2345] [Iter 373] [G_Loss 0.959168] [D_Loss 0.926542]\n",
      "train [Epoch 0/10] [Batch 374/2345] [Iter 374] [G_Loss 1.210691] [D_Loss 0.631251]\n",
      "train [Epoch 0/10] [Batch 375/2345] [Iter 375] [G_Loss 1.718534] [D_Loss 0.502679]\n",
      "train [Epoch 0/10] [Batch 376/2345] [Iter 376] [G_Loss 1.219311] [D_Loss 0.815205]\n",
      "train [Epoch 0/10] [Batch 377/2345] [Iter 377] [G_Loss 1.345477] [D_Loss 0.720571]\n",
      "train [Epoch 0/10] [Batch 378/2345] [Iter 378] [G_Loss 1.154657] [D_Loss 1.080780]\n",
      "train [Epoch 0/10] [Batch 379/2345] [Iter 379] [G_Loss 1.445591] [D_Loss 1.242322]\n",
      "train [Epoch 0/10] [Batch 380/2345] [Iter 380] [G_Loss 1.521371] [D_Loss 0.728466]\n",
      "train [Epoch 0/10] [Batch 381/2345] [Iter 381] [G_Loss 1.474465] [D_Loss 0.826405]\n",
      "train [Epoch 0/10] [Batch 382/2345] [Iter 382] [G_Loss 1.566265] [D_Loss 0.759785]\n",
      "train [Epoch 0/10] [Batch 383/2345] [Iter 383] [G_Loss 1.710674] [D_Loss 1.190198]\n",
      "train [Epoch 0/10] [Batch 384/2345] [Iter 384] [G_Loss 1.717524] [D_Loss 1.031780]\n",
      "train [Epoch 0/10] [Batch 385/2345] [Iter 385] [G_Loss 1.224135] [D_Loss 0.797468]\n",
      "train [Epoch 0/10] [Batch 386/2345] [Iter 386] [G_Loss 0.998831] [D_Loss 0.986218]\n",
      "train [Epoch 0/10] [Batch 387/2345] [Iter 387] [G_Loss 1.433134] [D_Loss 0.749506]\n",
      "train [Epoch 0/10] [Batch 388/2345] [Iter 388] [G_Loss 1.313233] [D_Loss 0.592938]\n",
      "train [Epoch 0/10] [Batch 389/2345] [Iter 389] [G_Loss 1.770249] [D_Loss 0.728373]\n",
      "train [Epoch 0/10] [Batch 390/2345] [Iter 390] [G_Loss 1.346837] [D_Loss 0.751708]\n",
      "train [Epoch 0/10] [Batch 391/2345] [Iter 391] [G_Loss 1.405894] [D_Loss 0.774774]\n",
      "train [Epoch 0/10] [Batch 392/2345] [Iter 392] [G_Loss 1.697839] [D_Loss 1.108829]\n",
      "train [Epoch 0/10] [Batch 393/2345] [Iter 393] [G_Loss 1.261487] [D_Loss 1.095601]\n",
      "train [Epoch 0/10] [Batch 394/2345] [Iter 394] [G_Loss 1.109231] [D_Loss 1.166996]\n",
      "train [Epoch 0/10] [Batch 395/2345] [Iter 395] [G_Loss 0.854130] [D_Loss 1.584220]\n",
      "train [Epoch 0/10] [Batch 396/2345] [Iter 396] [G_Loss 1.700697] [D_Loss 1.347403]\n",
      "train [Epoch 0/10] [Batch 397/2345] [Iter 397] [G_Loss 2.390512] [D_Loss 0.804920]\n",
      "train [Epoch 0/10] [Batch 398/2345] [Iter 398] [G_Loss 1.562582] [D_Loss 0.942699]\n",
      "train [Epoch 0/10] [Batch 399/2345] [Iter 399] [G_Loss 1.147895] [D_Loss 1.126963]\n",
      "train [Epoch 0/10] [Batch 400/2345] [Iter 400] [G_Loss 1.452350] [D_Loss 1.222908]\n",
      "train [Epoch 0/10] [Batch 401/2345] [Iter 401] [G_Loss 1.444009] [D_Loss 1.370629]\n",
      "train [Epoch 0/10] [Batch 406/2345] [Iter 406] [G_Loss 0.773040] [D_Loss 1.183448]\n",
      "train [Epoch 0/10] [Batch 407/2345] [Iter 407] [G_Loss 0.932556] [D_Loss 1.031188]\n",
      "train [Epoch 0/10] [Batch 408/2345] [Iter 408] [G_Loss 1.086646] [D_Loss 1.115217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 409/2345] [Iter 409] [G_Loss 0.715435] [D_Loss 1.096937]\n",
      "train [Epoch 0/10] [Batch 410/2345] [Iter 410] [G_Loss 1.278187] [D_Loss 0.878397]\n",
      "train [Epoch 0/10] [Batch 411/2345] [Iter 411] [G_Loss 1.017664] [D_Loss 0.830646]\n",
      "train [Epoch 0/10] [Batch 412/2345] [Iter 412] [G_Loss 1.047711] [D_Loss 0.576602]\n",
      "train [Epoch 0/10] [Batch 413/2345] [Iter 413] [G_Loss 2.365165] [D_Loss 0.812524]\n",
      "train [Epoch 0/10] [Batch 414/2345] [Iter 414] [G_Loss 0.825357] [D_Loss 0.842292]\n",
      "train [Epoch 0/10] [Batch 415/2345] [Iter 415] [G_Loss 2.100225] [D_Loss 0.770430]\n",
      "train [Epoch 0/10] [Batch 416/2345] [Iter 416] [G_Loss 1.279527] [D_Loss 0.761932]\n",
      "train [Epoch 0/10] [Batch 417/2345] [Iter 417] [G_Loss 0.978602] [D_Loss 0.890756]\n",
      "train [Epoch 0/10] [Batch 418/2345] [Iter 418] [G_Loss 1.185473] [D_Loss 1.087427]\n",
      "train [Epoch 0/10] [Batch 419/2345] [Iter 419] [G_Loss 2.018323] [D_Loss 0.930967]\n",
      "train [Epoch 0/10] [Batch 420/2345] [Iter 420] [G_Loss 2.046839] [D_Loss 0.994781]\n",
      "train [Epoch 0/10] [Batch 421/2345] [Iter 421] [G_Loss 0.940960] [D_Loss 1.238821]\n",
      "train [Epoch 0/10] [Batch 422/2345] [Iter 422] [G_Loss 1.814764] [D_Loss 1.363828]\n",
      "train [Epoch 0/10] [Batch 423/2345] [Iter 423] [G_Loss 0.863349] [D_Loss 1.116492]\n",
      "train [Epoch 0/10] [Batch 424/2345] [Iter 424] [G_Loss 0.777737] [D_Loss 1.274461]\n",
      "train [Epoch 0/10] [Batch 425/2345] [Iter 425] [G_Loss 0.936857] [D_Loss 1.184431]\n",
      "train [Epoch 0/10] [Batch 426/2345] [Iter 426] [G_Loss 1.119632] [D_Loss 1.063399]\n",
      "train [Epoch 0/10] [Batch 427/2345] [Iter 427] [G_Loss 1.033938] [D_Loss 1.050004]\n",
      "train [Epoch 0/10] [Batch 428/2345] [Iter 428] [G_Loss 0.816569] [D_Loss 1.249202]\n",
      "train [Epoch 0/10] [Batch 429/2345] [Iter 429] [G_Loss 1.347224] [D_Loss 1.061077]\n",
      "train [Epoch 0/10] [Batch 430/2345] [Iter 430] [G_Loss 1.023553] [D_Loss 1.202486]\n",
      "train [Epoch 0/10] [Batch 431/2345] [Iter 431] [G_Loss 1.304393] [D_Loss 1.244063]\n",
      "train [Epoch 0/10] [Batch 432/2345] [Iter 432] [G_Loss 1.019248] [D_Loss 1.288918]\n",
      "train [Epoch 0/10] [Batch 433/2345] [Iter 433] [G_Loss 0.936245] [D_Loss 1.328909]\n",
      "train [Epoch 0/10] [Batch 434/2345] [Iter 434] [G_Loss 1.162746] [D_Loss 1.281430]\n",
      "train [Epoch 0/10] [Batch 435/2345] [Iter 435] [G_Loss 0.813155] [D_Loss 1.093027]\n",
      "train [Epoch 0/10] [Batch 436/2345] [Iter 436] [G_Loss 0.972200] [D_Loss 1.058314]\n",
      "train [Epoch 0/10] [Batch 437/2345] [Iter 437] [G_Loss 0.963354] [D_Loss 1.038118]\n",
      "train [Epoch 0/10] [Batch 438/2345] [Iter 438] [G_Loss 1.036738] [D_Loss 1.031008]\n",
      "train [Epoch 0/10] [Batch 439/2345] [Iter 439] [G_Loss 1.182886] [D_Loss 0.882765]\n",
      "train [Epoch 0/10] [Batch 440/2345] [Iter 440] [G_Loss 1.309498] [D_Loss 0.658636]\n",
      "train [Epoch 0/10] [Batch 441/2345] [Iter 441] [G_Loss 0.902622] [D_Loss 1.013941]\n",
      "train [Epoch 0/10] [Batch 448/2345] [Iter 448] [G_Loss 1.673228] [D_Loss 1.343114]\n",
      "train [Epoch 0/10] [Batch 449/2345] [Iter 449] [G_Loss 1.021019] [D_Loss 1.379820]\n",
      "train [Epoch 0/10] [Batch 450/2345] [Iter 450] [G_Loss 1.110520] [D_Loss 1.368112]\n",
      "train [Epoch 0/10] [Batch 451/2345] [Iter 451] [G_Loss 1.065683] [D_Loss 1.169572]\n",
      "train [Epoch 0/10] [Batch 452/2345] [Iter 452] [G_Loss 1.372911] [D_Loss 1.113098]\n",
      "train [Epoch 0/10] [Batch 453/2345] [Iter 453] [G_Loss 0.877773] [D_Loss 1.000946]\n",
      "train [Epoch 0/10] [Batch 454/2345] [Iter 454] [G_Loss 1.287931] [D_Loss 0.898075]\n",
      "train [Epoch 0/10] [Batch 455/2345] [Iter 455] [G_Loss 0.846768] [D_Loss 0.961942]\n",
      "train [Epoch 0/10] [Batch 456/2345] [Iter 456] [G_Loss 1.752653] [D_Loss 0.980212]\n",
      "train [Epoch 0/10] [Batch 457/2345] [Iter 457] [G_Loss 1.035798] [D_Loss 1.091966]\n",
      "train [Epoch 0/10] [Batch 458/2345] [Iter 458] [G_Loss 0.917051] [D_Loss 0.921587]\n",
      "train [Epoch 0/10] [Batch 459/2345] [Iter 459] [G_Loss 1.130031] [D_Loss 0.911317]\n",
      "train [Epoch 0/10] [Batch 460/2345] [Iter 460] [G_Loss 1.254546] [D_Loss 0.758751]\n",
      "train [Epoch 0/10] [Batch 461/2345] [Iter 461] [G_Loss 1.112226] [D_Loss 0.945678]\n",
      "train [Epoch 0/10] [Batch 462/2345] [Iter 462] [G_Loss 1.198642] [D_Loss 1.014572]\n",
      "train [Epoch 0/10] [Batch 463/2345] [Iter 463] [G_Loss 1.537911] [D_Loss 0.874769]\n",
      "train [Epoch 0/10] [Batch 464/2345] [Iter 464] [G_Loss 1.751812] [D_Loss 0.709622]\n",
      "train [Epoch 0/10] [Batch 465/2345] [Iter 465] [G_Loss 1.353878] [D_Loss 1.041961]\n",
      "train [Epoch 0/10] [Batch 466/2345] [Iter 466] [G_Loss 1.581354] [D_Loss 0.966379]\n",
      "train [Epoch 0/10] [Batch 467/2345] [Iter 467] [G_Loss 2.278995] [D_Loss 1.519062]\n",
      "train [Epoch 0/10] [Batch 468/2345] [Iter 468] [G_Loss 0.537146] [D_Loss 1.802330]\n",
      "train [Epoch 0/10] [Batch 469/2345] [Iter 469] [G_Loss 1.048577] [D_Loss 1.280306]\n",
      "train [Epoch 0/10] [Batch 470/2345] [Iter 470] [G_Loss 1.440778] [D_Loss 1.246694]\n",
      "train [Epoch 0/10] [Batch 471/2345] [Iter 471] [G_Loss 0.637115] [D_Loss 1.247318]\n",
      "train [Epoch 0/10] [Batch 472/2345] [Iter 472] [G_Loss 0.962177] [D_Loss 1.044007]\n",
      "train [Epoch 0/10] [Batch 473/2345] [Iter 473] [G_Loss 1.428749] [D_Loss 1.003637]\n",
      "train [Epoch 0/10] [Batch 474/2345] [Iter 474] [G_Loss 1.055992] [D_Loss 0.861409]\n",
      "train [Epoch 0/10] [Batch 475/2345] [Iter 475] [G_Loss 1.087072] [D_Loss 0.961123]\n",
      "train [Epoch 0/10] [Batch 476/2345] [Iter 476] [G_Loss 1.250815] [D_Loss 0.681031]\n",
      "train [Epoch 0/10] [Batch 477/2345] [Iter 477] [G_Loss 1.569215] [D_Loss 0.754126]\n",
      "train [Epoch 0/10] [Batch 478/2345] [Iter 478] [G_Loss 1.423826] [D_Loss 0.592884]\n",
      "train [Epoch 0/10] [Batch 479/2345] [Iter 479] [G_Loss 1.507463] [D_Loss 0.734683]\n",
      "train [Epoch 0/10] [Batch 480/2345] [Iter 480] [G_Loss 1.258250] [D_Loss 0.828976]\n",
      "train [Epoch 0/10] [Batch 481/2345] [Iter 481] [G_Loss 2.957034] [D_Loss 2.104439]\n",
      "train [Epoch 0/10] [Batch 482/2345] [Iter 482] [G_Loss 0.641939] [D_Loss 1.134515]\n",
      "train [Epoch 0/10] [Batch 483/2345] [Iter 483] [G_Loss 3.808224] [D_Loss 0.491720]\n",
      "train [Epoch 0/10] [Batch 484/2345] [Iter 484] [G_Loss 6.752700] [D_Loss 0.548588]\n",
      "train [Epoch 0/10] [Batch 485/2345] [Iter 485] [G_Loss 6.122915] [D_Loss 0.884344]\n",
      "train [Epoch 0/10] [Batch 486/2345] [Iter 486] [G_Loss 3.081504] [D_Loss 0.420084]\n",
      "train [Epoch 0/10] [Batch 487/2345] [Iter 487] [G_Loss 2.799988] [D_Loss 0.521471]\n",
      "train [Epoch 0/10] [Batch 488/2345] [Iter 488] [G_Loss 1.796098] [D_Loss 0.642247]\n",
      "train [Epoch 0/10] [Batch 489/2345] [Iter 489] [G_Loss 2.538821] [D_Loss 1.203046]\n",
      "train [Epoch 0/10] [Batch 490/2345] [Iter 490] [G_Loss 1.723503] [D_Loss 1.033884]\n",
      "train [Epoch 0/10] [Batch 491/2345] [Iter 491] [G_Loss 0.880884] [D_Loss 1.154078]\n",
      "train [Epoch 0/10] [Batch 492/2345] [Iter 492] [G_Loss 1.529330] [D_Loss 0.948976]\n",
      "train [Epoch 0/10] [Batch 493/2345] [Iter 493] [G_Loss 0.989878] [D_Loss 1.535292]\n",
      "train [Epoch 0/10] [Batch 494/2345] [Iter 494] [G_Loss 1.493078] [D_Loss 0.793316]\n",
      "train [Epoch 0/10] [Batch 495/2345] [Iter 495] [G_Loss 0.859563] [D_Loss 1.024598]\n",
      "train [Epoch 0/10] [Batch 496/2345] [Iter 496] [G_Loss 1.244026] [D_Loss 1.114038]\n",
      "train [Epoch 0/10] [Batch 497/2345] [Iter 497] [G_Loss 1.514320] [D_Loss 1.018885]\n",
      "train [Epoch 0/10] [Batch 498/2345] [Iter 498] [G_Loss 1.425233] [D_Loss 0.900787]\n",
      "train [Epoch 0/10] [Batch 499/2345] [Iter 499] [G_Loss 0.758365] [D_Loss 1.203335]\n",
      "train [Epoch 0/10] [Batch 500/2345] [Iter 500] [G_Loss 1.522267] [D_Loss 1.037014]\n",
      "train [Epoch 0/10] [Batch 501/2345] [Iter 501] [G_Loss 0.934539] [D_Loss 1.047403]\n",
      "train [Epoch 0/10] [Batch 502/2345] [Iter 502] [G_Loss 0.849097] [D_Loss 1.247910]\n",
      "train [Epoch 0/10] [Batch 503/2345] [Iter 503] [G_Loss 1.287364] [D_Loss 1.554868]\n",
      "train [Epoch 0/10] [Batch 504/2345] [Iter 504] [G_Loss 0.192149] [D_Loss 2.416483]\n",
      "train [Epoch 0/10] [Batch 505/2345] [Iter 505] [G_Loss 1.447345] [D_Loss 1.677626]\n",
      "train [Epoch 0/10] [Batch 506/2345] [Iter 506] [G_Loss 0.849824] [D_Loss 1.294204]\n",
      "train [Epoch 0/10] [Batch 507/2345] [Iter 507] [G_Loss 0.694508] [D_Loss 1.259487]\n",
      "train [Epoch 0/10] [Batch 508/2345] [Iter 508] [G_Loss 0.797738] [D_Loss 1.132962]\n",
      "train [Epoch 0/10] [Batch 509/2345] [Iter 509] [G_Loss 0.763222] [D_Loss 1.288253]\n",
      "train [Epoch 0/10] [Batch 510/2345] [Iter 510] [G_Loss 0.885462] [D_Loss 1.395617]\n",
      "train [Epoch 0/10] [Batch 511/2345] [Iter 511] [G_Loss 1.028267] [D_Loss 1.458323]\n",
      "train [Epoch 0/10] [Batch 512/2345] [Iter 512] [G_Loss 0.853875] [D_Loss 1.307662]\n",
      "train [Epoch 0/10] [Batch 513/2345] [Iter 513] [G_Loss 0.815809] [D_Loss 1.265077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 514/2345] [Iter 514] [G_Loss 0.751123] [D_Loss 1.230248]\n",
      "train [Epoch 0/10] [Batch 515/2345] [Iter 515] [G_Loss 0.814231] [D_Loss 1.296842]\n",
      "train [Epoch 0/10] [Batch 516/2345] [Iter 516] [G_Loss 0.764218] [D_Loss 1.186557]\n",
      "train [Epoch 0/10] [Batch 517/2345] [Iter 517] [G_Loss 0.991719] [D_Loss 1.213651]\n",
      "train [Epoch 0/10] [Batch 518/2345] [Iter 518] [G_Loss 1.420190] [D_Loss 1.041630]\n",
      "train [Epoch 0/10] [Batch 519/2345] [Iter 519] [G_Loss 0.511149] [D_Loss 1.465096]\n",
      "train [Epoch 0/10] [Batch 520/2345] [Iter 520] [G_Loss 1.061451] [D_Loss 1.603263]\n",
      "train [Epoch 0/10] [Batch 521/2345] [Iter 521] [G_Loss 0.983141] [D_Loss 1.286313]\n",
      "train [Epoch 0/10] [Batch 522/2345] [Iter 522] [G_Loss 0.823443] [D_Loss 1.202710]\n",
      "train [Epoch 0/10] [Batch 523/2345] [Iter 523] [G_Loss 0.914082] [D_Loss 1.168693]\n",
      "train [Epoch 0/10] [Batch 524/2345] [Iter 524] [G_Loss 0.892516] [D_Loss 1.122005]\n",
      "train [Epoch 0/10] [Batch 525/2345] [Iter 525] [G_Loss 1.001151] [D_Loss 1.094724]\n",
      "train [Epoch 0/10] [Batch 526/2345] [Iter 526] [G_Loss 0.947977] [D_Loss 1.063141]\n",
      "train [Epoch 0/10] [Batch 527/2345] [Iter 527] [G_Loss 1.085463] [D_Loss 1.027105]\n",
      "train [Epoch 0/10] [Batch 528/2345] [Iter 528] [G_Loss 1.259629] [D_Loss 0.824796]\n",
      "train [Epoch 0/10] [Batch 529/2345] [Iter 529] [G_Loss 1.175952] [D_Loss 0.956114]\n",
      "train [Epoch 0/10] [Batch 530/2345] [Iter 530] [G_Loss 1.002601] [D_Loss 0.916878]\n",
      "train [Epoch 0/10] [Batch 531/2345] [Iter 531] [G_Loss 1.411946] [D_Loss 0.853590]\n",
      "train [Epoch 0/10] [Batch 532/2345] [Iter 532] [G_Loss 1.069659] [D_Loss 0.813340]\n",
      "train [Epoch 0/10] [Batch 533/2345] [Iter 533] [G_Loss 1.498124] [D_Loss 1.027402]\n",
      "train [Epoch 0/10] [Batch 534/2345] [Iter 534] [G_Loss 0.669473] [D_Loss 1.401112]\n",
      "train [Epoch 0/10] [Batch 535/2345] [Iter 535] [G_Loss 1.673956] [D_Loss 1.481185]\n",
      "train [Epoch 0/10] [Batch 536/2345] [Iter 536] [G_Loss 0.849591] [D_Loss 1.051478]\n",
      "train [Epoch 0/10] [Batch 537/2345] [Iter 537] [G_Loss 5.510334] [D_Loss 0.366923]\n",
      "train [Epoch 0/10] [Batch 538/2345] [Iter 538] [G_Loss 1.554580] [D_Loss 1.114392]\n",
      "train [Epoch 0/10] [Batch 539/2345] [Iter 539] [G_Loss 2.704192] [D_Loss 1.615719]\n",
      "train [Epoch 0/10] [Batch 540/2345] [Iter 540] [G_Loss 0.505640] [D_Loss 2.084200]\n",
      "train [Epoch 0/10] [Batch 541/2345] [Iter 541] [G_Loss 1.074394] [D_Loss 1.325062]\n",
      "train [Epoch 0/10] [Batch 542/2345] [Iter 542] [G_Loss 1.585035] [D_Loss 1.250915]\n",
      "train [Epoch 0/10] [Batch 543/2345] [Iter 543] [G_Loss 1.076760] [D_Loss 1.160841]\n",
      "train [Epoch 0/10] [Batch 544/2345] [Iter 544] [G_Loss 0.839194] [D_Loss 1.057781]\n",
      "train [Epoch 0/10] [Batch 545/2345] [Iter 545] [G_Loss 0.981075] [D_Loss 0.992840]\n",
      "train [Epoch 0/10] [Batch 546/2345] [Iter 546] [G_Loss 1.117477] [D_Loss 0.985239]\n",
      "train [Epoch 0/10] [Batch 547/2345] [Iter 547] [G_Loss 1.116894] [D_Loss 0.890368]\n",
      "train [Epoch 0/10] [Batch 548/2345] [Iter 548] [G_Loss 1.295384] [D_Loss 0.815091]\n",
      "train [Epoch 0/10] [Batch 549/2345] [Iter 549] [G_Loss 1.259069] [D_Loss 0.814683]\n",
      "train [Epoch 0/10] [Batch 550/2345] [Iter 550] [G_Loss 1.387189] [D_Loss 0.755134]\n",
      "train [Epoch 0/10] [Batch 551/2345] [Iter 551] [G_Loss 1.372314] [D_Loss 0.730129]\n",
      "train [Epoch 0/10] [Batch 552/2345] [Iter 552] [G_Loss 1.316358] [D_Loss 0.563411]\n",
      "train [Epoch 0/10] [Batch 553/2345] [Iter 553] [G_Loss 1.811327] [D_Loss 0.634404]\n",
      "train [Epoch 0/10] [Batch 554/2345] [Iter 554] [G_Loss 2.100603] [D_Loss 0.518520]\n",
      "train [Epoch 0/10] [Batch 555/2345] [Iter 555] [G_Loss 0.816775] [D_Loss 0.814098]\n",
      "train [Epoch 0/10] [Batch 556/2345] [Iter 556] [G_Loss 3.320460] [D_Loss 1.132409]\n",
      "train [Epoch 0/10] [Batch 557/2345] [Iter 557] [G_Loss 2.457296] [D_Loss 0.961279]\n",
      "train [Epoch 0/10] [Batch 558/2345] [Iter 558] [G_Loss 1.452599] [D_Loss 0.681543]\n",
      "train [Epoch 0/10] [Batch 559/2345] [Iter 559] [G_Loss 1.199563] [D_Loss 0.761734]\n",
      "train [Epoch 0/10] [Batch 560/2345] [Iter 560] [G_Loss 1.115992] [D_Loss 0.707651]\n",
      "train [Epoch 0/10] [Batch 561/2345] [Iter 561] [G_Loss 1.337080] [D_Loss 0.522731]\n",
      "train [Epoch 0/10] [Batch 562/2345] [Iter 562] [G_Loss 1.947648] [D_Loss 0.934803]\n",
      "train [Epoch 0/10] [Batch 563/2345] [Iter 563] [G_Loss 1.585410] [D_Loss 0.879233]\n",
      "train [Epoch 0/10] [Batch 564/2345] [Iter 564] [G_Loss 1.131341] [D_Loss 1.224645]\n",
      "train [Epoch 0/10] [Batch 565/2345] [Iter 565] [G_Loss 1.200471] [D_Loss 1.392544]\n",
      "train [Epoch 0/10] [Batch 566/2345] [Iter 566] [G_Loss 3.708329] [D_Loss 1.951151]\n",
      "train [Epoch 0/10] [Batch 567/2345] [Iter 567] [G_Loss 1.500662] [D_Loss 0.753528]\n",
      "train [Epoch 0/10] [Batch 568/2345] [Iter 568] [G_Loss 1.209339] [D_Loss 0.893656]\n",
      "train [Epoch 0/10] [Batch 569/2345] [Iter 569] [G_Loss 1.291426] [D_Loss 0.967607]\n",
      "train [Epoch 0/10] [Batch 570/2345] [Iter 570] [G_Loss 1.575265] [D_Loss 0.831023]\n",
      "train [Epoch 0/10] [Batch 571/2345] [Iter 571] [G_Loss 1.278112] [D_Loss 1.040293]\n",
      "train [Epoch 0/10] [Batch 572/2345] [Iter 572] [G_Loss 0.909191] [D_Loss 1.250626]\n",
      "train [Epoch 0/10] [Batch 573/2345] [Iter 573] [G_Loss 1.392483] [D_Loss 1.291277]\n",
      "train [Epoch 0/10] [Batch 574/2345] [Iter 574] [G_Loss 0.530224] [D_Loss 1.220747]\n",
      "train [Epoch 0/10] [Batch 575/2345] [Iter 575] [G_Loss 1.355957] [D_Loss 0.757790]\n",
      "train [Epoch 0/10] [Batch 576/2345] [Iter 576] [G_Loss 1.711716] [D_Loss 0.629816]\n",
      "train [Epoch 0/10] [Batch 577/2345] [Iter 577] [G_Loss 1.589442] [D_Loss 0.651693]\n",
      "train [Epoch 0/10] [Batch 578/2345] [Iter 578] [G_Loss 1.402871] [D_Loss 0.551285]\n",
      "train [Epoch 0/10] [Batch 579/2345] [Iter 579] [G_Loss 1.378057] [D_Loss 0.531193]\n",
      "train [Epoch 0/10] [Batch 580/2345] [Iter 580] [G_Loss 0.998306] [D_Loss 1.318752]\n",
      "train [Epoch 0/10] [Batch 581/2345] [Iter 581] [G_Loss 1.459730] [D_Loss 1.735626]\n",
      "train [Epoch 0/10] [Batch 582/2345] [Iter 582] [G_Loss 1.510174] [D_Loss 1.291736]\n",
      "train [Epoch 0/10] [Batch 583/2345] [Iter 583] [G_Loss 5.477306] [D_Loss 0.536543]\n",
      "train [Epoch 0/10] [Batch 584/2345] [Iter 584] [G_Loss 0.418557] [D_Loss 2.837588]\n",
      "train [Epoch 0/10] [Batch 585/2345] [Iter 585] [G_Loss 1.473907] [D_Loss 1.327209]\n",
      "train [Epoch 0/10] [Batch 586/2345] [Iter 586] [G_Loss 1.520212] [D_Loss 1.193080]\n",
      "train [Epoch 0/10] [Batch 587/2345] [Iter 587] [G_Loss 1.598723] [D_Loss 0.884001]\n",
      "train [Epoch 0/10] [Batch 588/2345] [Iter 588] [G_Loss 1.448154] [D_Loss 0.776999]\n",
      "train [Epoch 0/10] [Batch 589/2345] [Iter 589] [G_Loss 1.557052] [D_Loss 0.628526]\n",
      "train [Epoch 0/10] [Batch 590/2345] [Iter 590] [G_Loss 1.716930] [D_Loss 0.638510]\n",
      "train [Epoch 0/10] [Batch 591/2345] [Iter 591] [G_Loss 1.656722] [D_Loss 0.580125]\n",
      "train [Epoch 0/10] [Batch 592/2345] [Iter 592] [G_Loss 1.396109] [D_Loss 0.716938]\n",
      "train [Epoch 0/10] [Batch 593/2345] [Iter 593] [G_Loss 1.729268] [D_Loss 0.618231]\n",
      "train [Epoch 0/10] [Batch 594/2345] [Iter 594] [G_Loss 1.334752] [D_Loss 0.689731]\n",
      "train [Epoch 0/10] [Batch 595/2345] [Iter 595] [G_Loss 1.164413] [D_Loss 0.958447]\n",
      "train [Epoch 0/10] [Batch 596/2345] [Iter 596] [G_Loss 2.353202] [D_Loss 1.255722]\n",
      "train [Epoch 0/10] [Batch 597/2345] [Iter 597] [G_Loss 0.737327] [D_Loss 1.764558]\n",
      "train [Epoch 0/10] [Batch 598/2345] [Iter 598] [G_Loss 6.274525] [D_Loss 1.475722]\n",
      "train [Epoch 0/10] [Batch 599/2345] [Iter 599] [G_Loss 1.242087] [D_Loss 0.956629]\n",
      "train [Epoch 0/10] [Batch 600/2345] [Iter 600] [G_Loss 0.866440] [D_Loss 1.036320]\n",
      "train [Epoch 0/10] [Batch 601/2345] [Iter 601] [G_Loss 1.027557] [D_Loss 0.974937]\n",
      "train [Epoch 0/10] [Batch 602/2345] [Iter 602] [G_Loss 1.182039] [D_Loss 0.880672]\n",
      "train [Epoch 0/10] [Batch 603/2345] [Iter 603] [G_Loss 1.446644] [D_Loss 0.784994]\n",
      "train [Epoch 0/10] [Batch 604/2345] [Iter 604] [G_Loss 1.271726] [D_Loss 0.744791]\n",
      "train [Epoch 0/10] [Batch 605/2345] [Iter 605] [G_Loss 1.022043] [D_Loss 0.990510]\n",
      "train [Epoch 0/10] [Batch 606/2345] [Iter 606] [G_Loss 0.867174] [D_Loss 1.315075]\n",
      "train [Epoch 0/10] [Batch 607/2345] [Iter 607] [G_Loss 1.010579] [D_Loss 1.293155]\n",
      "train [Epoch 0/10] [Batch 608/2345] [Iter 608] [G_Loss 1.825864] [D_Loss 1.056462]\n",
      "train [Epoch 0/10] [Batch 609/2345] [Iter 609] [G_Loss 1.157292] [D_Loss 1.237339]\n",
      "train [Epoch 0/10] [Batch 610/2345] [Iter 610] [G_Loss 1.075543] [D_Loss 1.202684]\n",
      "train [Epoch 0/10] [Batch 611/2345] [Iter 611] [G_Loss 0.902605] [D_Loss 1.384771]\n",
      "train [Epoch 0/10] [Batch 612/2345] [Iter 612] [G_Loss 1.022691] [D_Loss 1.351011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 613/2345] [Iter 613] [G_Loss 1.401470] [D_Loss 1.073365]\n",
      "train [Epoch 0/10] [Batch 614/2345] [Iter 614] [G_Loss 0.808973] [D_Loss 1.210744]\n",
      "train [Epoch 0/10] [Batch 615/2345] [Iter 615] [G_Loss 1.162168] [D_Loss 1.330091]\n",
      "train [Epoch 0/10] [Batch 616/2345] [Iter 616] [G_Loss 1.146418] [D_Loss 1.163998]\n",
      "train [Epoch 0/10] [Batch 617/2345] [Iter 617] [G_Loss 1.179029] [D_Loss 1.158708]\n",
      "train [Epoch 0/10] [Batch 618/2345] [Iter 618] [G_Loss 0.730460] [D_Loss 1.178618]\n",
      "train [Epoch 0/10] [Batch 619/2345] [Iter 619] [G_Loss 0.903497] [D_Loss 1.036279]\n",
      "train [Epoch 0/10] [Batch 620/2345] [Iter 620] [G_Loss 1.212599] [D_Loss 1.016866]\n",
      "train [Epoch 0/10] [Batch 621/2345] [Iter 621] [G_Loss 0.990633] [D_Loss 0.980590]\n",
      "train [Epoch 0/10] [Batch 622/2345] [Iter 622] [G_Loss 0.939247] [D_Loss 0.936558]\n",
      "train [Epoch 0/10] [Batch 623/2345] [Iter 623] [G_Loss 1.203771] [D_Loss 0.735656]\n",
      "train [Epoch 0/10] [Batch 624/2345] [Iter 624] [G_Loss 1.704222] [D_Loss 0.965812]\n",
      "train [Epoch 0/10] [Batch 625/2345] [Iter 625] [G_Loss 0.964813] [D_Loss 0.785180]\n",
      "train [Epoch 0/10] [Batch 626/2345] [Iter 626] [G_Loss 1.406361] [D_Loss 0.779055]\n",
      "train [Epoch 0/10] [Batch 627/2345] [Iter 627] [G_Loss 1.595283] [D_Loss 0.655214]\n",
      "train [Epoch 0/10] [Batch 628/2345] [Iter 628] [G_Loss 1.329296] [D_Loss 0.802392]\n",
      "train [Epoch 0/10] [Batch 629/2345] [Iter 629] [G_Loss 1.230928] [D_Loss 1.003556]\n",
      "train [Epoch 0/10] [Batch 630/2345] [Iter 630] [G_Loss 0.388710] [D_Loss 2.531367]\n",
      "train [Epoch 0/10] [Batch 631/2345] [Iter 631] [G_Loss 3.746964] [D_Loss 2.428474]\n",
      "train [Epoch 0/10] [Batch 632/2345] [Iter 632] [G_Loss 0.990252] [D_Loss 1.438456]\n",
      "train [Epoch 0/10] [Batch 633/2345] [Iter 633] [G_Loss 0.559231] [D_Loss 1.481782]\n",
      "train [Epoch 0/10] [Batch 634/2345] [Iter 634] [G_Loss 0.576413] [D_Loss 1.364907]\n",
      "train [Epoch 0/10] [Batch 635/2345] [Iter 635] [G_Loss 0.746478] [D_Loss 1.263431]\n",
      "train [Epoch 0/10] [Batch 636/2345] [Iter 636] [G_Loss 0.901950] [D_Loss 1.250158]\n",
      "train [Epoch 0/10] [Batch 637/2345] [Iter 637] [G_Loss 0.964772] [D_Loss 1.142184]\n",
      "train [Epoch 0/10] [Batch 638/2345] [Iter 638] [G_Loss 0.932127] [D_Loss 1.077969]\n",
      "train [Epoch 0/10] [Batch 639/2345] [Iter 639] [G_Loss 1.220818] [D_Loss 0.948869]\n",
      "train [Epoch 0/10] [Batch 640/2345] [Iter 640] [G_Loss 1.153460] [D_Loss 0.913428]\n",
      "train [Epoch 0/10] [Batch 641/2345] [Iter 641] [G_Loss 0.999661] [D_Loss 0.893217]\n",
      "train [Epoch 0/10] [Batch 642/2345] [Iter 642] [G_Loss 1.504845] [D_Loss 1.009205]\n",
      "train [Epoch 0/10] [Batch 643/2345] [Iter 643] [G_Loss 0.761745] [D_Loss 1.207201]\n",
      "train [Epoch 0/10] [Batch 644/2345] [Iter 644] [G_Loss 1.307708] [D_Loss 1.207166]\n",
      "train [Epoch 0/10] [Batch 645/2345] [Iter 645] [G_Loss 0.811330] [D_Loss 1.105046]\n",
      "train [Epoch 0/10] [Batch 646/2345] [Iter 646] [G_Loss 0.897996] [D_Loss 1.222773]\n",
      "train [Epoch 0/10] [Batch 647/2345] [Iter 647] [G_Loss 0.933728] [D_Loss 1.319719]\n",
      "train [Epoch 0/10] [Batch 648/2345] [Iter 648] [G_Loss 1.168050] [D_Loss 1.205847]\n",
      "train [Epoch 0/10] [Batch 649/2345] [Iter 649] [G_Loss 0.756032] [D_Loss 1.038513]\n",
      "train [Epoch 0/10] [Batch 650/2345] [Iter 650] [G_Loss 1.060332] [D_Loss 1.004876]\n",
      "train [Epoch 0/10] [Batch 651/2345] [Iter 651] [G_Loss 1.346473] [D_Loss 1.088070]\n",
      "train [Epoch 0/10] [Batch 652/2345] [Iter 652] [G_Loss 1.052295] [D_Loss 0.902114]\n",
      "train [Epoch 0/10] [Batch 653/2345] [Iter 653] [G_Loss 1.074214] [D_Loss 0.862493]\n",
      "train [Epoch 0/10] [Batch 654/2345] [Iter 654] [G_Loss 1.333765] [D_Loss 0.948947]\n",
      "train [Epoch 0/10] [Batch 655/2345] [Iter 655] [G_Loss 1.110954] [D_Loss 0.865789]\n",
      "train [Epoch 0/10] [Batch 656/2345] [Iter 656] [G_Loss 1.247862] [D_Loss 0.717756]\n",
      "train [Epoch 0/10] [Batch 657/2345] [Iter 657] [G_Loss 1.540463] [D_Loss 0.802556]\n",
      "train [Epoch 0/10] [Batch 658/2345] [Iter 658] [G_Loss 1.890242] [D_Loss 0.803102]\n",
      "train [Epoch 0/10] [Batch 659/2345] [Iter 659] [G_Loss 0.925362] [D_Loss 1.007692]\n",
      "train [Epoch 0/10] [Batch 660/2345] [Iter 660] [G_Loss 1.078136] [D_Loss 1.186888]\n",
      "train [Epoch 0/10] [Batch 661/2345] [Iter 661] [G_Loss 1.355560] [D_Loss 1.445004]\n",
      "train [Epoch 0/10] [Batch 662/2345] [Iter 662] [G_Loss 1.121786] [D_Loss 1.154974]\n",
      "train [Epoch 0/10] [Batch 663/2345] [Iter 663] [G_Loss 1.317147] [D_Loss 1.357395]\n",
      "train [Epoch 0/10] [Batch 664/2345] [Iter 664] [G_Loss 1.288297] [D_Loss 1.612051]\n",
      "train [Epoch 0/10] [Batch 665/2345] [Iter 665] [G_Loss 0.815855] [D_Loss 1.572325]\n",
      "train [Epoch 0/10] [Batch 666/2345] [Iter 666] [G_Loss 1.172222] [D_Loss 1.317377]\n",
      "train [Epoch 0/10] [Batch 667/2345] [Iter 667] [G_Loss 0.841016] [D_Loss 1.420336]\n",
      "train [Epoch 0/10] [Batch 668/2345] [Iter 668] [G_Loss 0.840870] [D_Loss 1.426348]\n",
      "train [Epoch 0/10] [Batch 669/2345] [Iter 669] [G_Loss 0.896085] [D_Loss 1.450609]\n",
      "train [Epoch 0/10] [Batch 670/2345] [Iter 670] [G_Loss 0.733028] [D_Loss 1.798264]\n",
      "train [Epoch 0/10] [Batch 671/2345] [Iter 671] [G_Loss 0.872228] [D_Loss 1.465675]\n",
      "train [Epoch 0/10] [Batch 672/2345] [Iter 672] [G_Loss 1.077120] [D_Loss 1.151974]\n",
      "train [Epoch 0/10] [Batch 673/2345] [Iter 673] [G_Loss 0.826274] [D_Loss 1.260390]\n",
      "train [Epoch 0/10] [Batch 674/2345] [Iter 674] [G_Loss 1.068638] [D_Loss 1.014816]\n",
      "train [Epoch 0/10] [Batch 675/2345] [Iter 675] [G_Loss 1.094771] [D_Loss 1.068245]\n",
      "train [Epoch 0/10] [Batch 676/2345] [Iter 676] [G_Loss 1.137228] [D_Loss 0.979563]\n",
      "train [Epoch 0/10] [Batch 677/2345] [Iter 677] [G_Loss 1.540956] [D_Loss 1.113730]\n",
      "train [Epoch 0/10] [Batch 678/2345] [Iter 678] [G_Loss 0.806121] [D_Loss 1.142807]\n",
      "train [Epoch 0/10] [Batch 679/2345] [Iter 679] [G_Loss 1.072258] [D_Loss 1.247645]\n",
      "train [Epoch 0/10] [Batch 680/2345] [Iter 680] [G_Loss 0.963797] [D_Loss 1.350613]\n",
      "train [Epoch 0/10] [Batch 681/2345] [Iter 681] [G_Loss 0.893499] [D_Loss 1.223483]\n",
      "train [Epoch 0/10] [Batch 682/2345] [Iter 682] [G_Loss 0.934312] [D_Loss 1.296648]\n",
      "train [Epoch 0/10] [Batch 683/2345] [Iter 683] [G_Loss 0.678980] [D_Loss 1.175463]\n",
      "train [Epoch 0/10] [Batch 684/2345] [Iter 684] [G_Loss 1.159006] [D_Loss 1.060008]\n",
      "train [Epoch 0/10] [Batch 685/2345] [Iter 685] [G_Loss 1.136667] [D_Loss 0.888171]\n",
      "train [Epoch 0/10] [Batch 686/2345] [Iter 686] [G_Loss 0.818793] [D_Loss 0.859017]\n",
      "train [Epoch 0/10] [Batch 687/2345] [Iter 687] [G_Loss 1.854117] [D_Loss 0.934738]\n",
      "train [Epoch 0/10] [Batch 688/2345] [Iter 688] [G_Loss 0.837900] [D_Loss 0.864743]\n",
      "train [Epoch 0/10] [Batch 689/2345] [Iter 689] [G_Loss 1.820772] [D_Loss 0.856891]\n",
      "train [Epoch 0/10] [Batch 690/2345] [Iter 690] [G_Loss 1.373890] [D_Loss 0.596354]\n",
      "train [Epoch 0/10] [Batch 691/2345] [Iter 691] [G_Loss 1.074259] [D_Loss 0.734470]\n",
      "train [Epoch 0/10] [Batch 692/2345] [Iter 692] [G_Loss 1.978527] [D_Loss 0.744934]\n",
      "train [Epoch 0/10] [Batch 693/2345] [Iter 693] [G_Loss 1.280826] [D_Loss 0.996390]\n",
      "train [Epoch 0/10] [Batch 694/2345] [Iter 694] [G_Loss 1.460704] [D_Loss 0.837709]\n",
      "train [Epoch 0/10] [Batch 695/2345] [Iter 695] [G_Loss 1.415542] [D_Loss 1.037551]\n",
      "train [Epoch 0/10] [Batch 696/2345] [Iter 696] [G_Loss 1.760014] [D_Loss 0.673720]\n",
      "train [Epoch 0/10] [Batch 697/2345] [Iter 697] [G_Loss 1.262881] [D_Loss 1.333978]\n",
      "train [Epoch 0/10] [Batch 698/2345] [Iter 698] [G_Loss 2.079703] [D_Loss 1.453628]\n",
      "train [Epoch 0/10] [Batch 699/2345] [Iter 699] [G_Loss 0.590762] [D_Loss 1.587303]\n",
      "train [Epoch 0/10] [Batch 700/2345] [Iter 700] [G_Loss 0.957754] [D_Loss 1.395908]\n",
      "train [Epoch 0/10] [Batch 701/2345] [Iter 701] [G_Loss 0.994089] [D_Loss 1.446177]\n",
      "train [Epoch 0/10] [Batch 702/2345] [Iter 702] [G_Loss 1.144772] [D_Loss 1.117791]\n",
      "train [Epoch 0/10] [Batch 703/2345] [Iter 703] [G_Loss 1.086925] [D_Loss 1.105931]\n",
      "train [Epoch 0/10] [Batch 704/2345] [Iter 704] [G_Loss 0.793894] [D_Loss 1.148423]\n",
      "train [Epoch 0/10] [Batch 705/2345] [Iter 705] [G_Loss 0.821450] [D_Loss 1.203145]\n",
      "train [Epoch 0/10] [Batch 706/2345] [Iter 706] [G_Loss 0.852067] [D_Loss 1.365591]\n",
      "train [Epoch 0/10] [Batch 707/2345] [Iter 707] [G_Loss 1.101395] [D_Loss 1.188861]\n",
      "train [Epoch 0/10] [Batch 708/2345] [Iter 708] [G_Loss 0.842882] [D_Loss 1.143211]\n",
      "train [Epoch 0/10] [Batch 709/2345] [Iter 709] [G_Loss 0.967730] [D_Loss 0.997126]\n",
      "train [Epoch 0/10] [Batch 710/2345] [Iter 710] [G_Loss 1.459225] [D_Loss 0.772759]\n",
      "train [Epoch 0/10] [Batch 711/2345] [Iter 711] [G_Loss 1.529341] [D_Loss 0.858934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 712/2345] [Iter 712] [G_Loss 1.800008] [D_Loss 0.839441]\n",
      "train [Epoch 0/10] [Batch 713/2345] [Iter 713] [G_Loss 2.132183] [D_Loss 0.953459]\n",
      "train [Epoch 0/10] [Batch 714/2345] [Iter 714] [G_Loss 0.705946] [D_Loss 1.376787]\n",
      "train [Epoch 0/10] [Batch 715/2345] [Iter 715] [G_Loss 2.562547] [D_Loss 1.895815]\n",
      "train [Epoch 0/10] [Batch 716/2345] [Iter 716] [G_Loss 0.233521] [D_Loss 1.819247]\n",
      "train [Epoch 0/10] [Batch 717/2345] [Iter 717] [G_Loss 0.922546] [D_Loss 1.146595]\n",
      "train [Epoch 0/10] [Batch 718/2345] [Iter 718] [G_Loss 1.551358] [D_Loss 1.150194]\n",
      "train [Epoch 0/10] [Batch 719/2345] [Iter 719] [G_Loss 1.216470] [D_Loss 1.068645]\n",
      "train [Epoch 0/10] [Batch 720/2345] [Iter 720] [G_Loss 0.706039] [D_Loss 1.049095]\n",
      "train [Epoch 0/10] [Batch 721/2345] [Iter 721] [G_Loss 1.035904] [D_Loss 1.102333]\n",
      "train [Epoch 0/10] [Batch 722/2345] [Iter 722] [G_Loss 1.203747] [D_Loss 0.896204]\n",
      "train [Epoch 0/10] [Batch 723/2345] [Iter 723] [G_Loss 1.191809] [D_Loss 0.888555]\n",
      "train [Epoch 0/10] [Batch 724/2345] [Iter 724] [G_Loss 1.065803] [D_Loss 0.887224]\n",
      "train [Epoch 0/10] [Batch 725/2345] [Iter 725] [G_Loss 1.217071] [D_Loss 0.812565]\n",
      "train [Epoch 0/10] [Batch 726/2345] [Iter 726] [G_Loss 1.347669] [D_Loss 0.592431]\n",
      "train [Epoch 0/10] [Batch 727/2345] [Iter 727] [G_Loss 1.907598] [D_Loss 0.860875]\n",
      "train [Epoch 0/10] [Batch 728/2345] [Iter 728] [G_Loss 1.265052] [D_Loss 0.631067]\n",
      "train [Epoch 0/10] [Batch 729/2345] [Iter 729] [G_Loss 1.659341] [D_Loss 0.563593]\n",
      "train [Epoch 0/10] [Batch 730/2345] [Iter 730] [G_Loss 1.818184] [D_Loss 0.483676]\n",
      "train [Epoch 0/10] [Batch 731/2345] [Iter 731] [G_Loss 1.857285] [D_Loss 0.626090]\n",
      "train [Epoch 0/10] [Batch 732/2345] [Iter 732] [G_Loss 0.598399] [D_Loss 2.009577]\n",
      "train [Epoch 0/10] [Batch 733/2345] [Iter 733] [G_Loss 2.099783] [D_Loss 2.053036]\n",
      "train [Epoch 0/10] [Batch 734/2345] [Iter 734] [G_Loss 1.297201] [D_Loss 1.238121]\n",
      "train [Epoch 0/10] [Batch 735/2345] [Iter 735] [G_Loss 0.789216] [D_Loss 1.357589]\n",
      "train [Epoch 0/10] [Batch 736/2345] [Iter 736] [G_Loss 0.917142] [D_Loss 1.151808]\n",
      "train [Epoch 0/10] [Batch 737/2345] [Iter 737] [G_Loss 1.161413] [D_Loss 1.129322]\n",
      "train [Epoch 0/10] [Batch 738/2345] [Iter 738] [G_Loss 0.749100] [D_Loss 1.120238]\n",
      "train [Epoch 0/10] [Batch 739/2345] [Iter 739] [G_Loss 0.798202] [D_Loss 1.086466]\n",
      "train [Epoch 0/10] [Batch 740/2345] [Iter 740] [G_Loss 0.978177] [D_Loss 1.272065]\n",
      "train [Epoch 0/10] [Batch 741/2345] [Iter 741] [G_Loss 0.529746] [D_Loss 1.615553]\n",
      "train [Epoch 0/10] [Batch 742/2345] [Iter 742] [G_Loss 1.281846] [D_Loss 1.109807]\n",
      "train [Epoch 0/10] [Batch 743/2345] [Iter 743] [G_Loss 0.905523] [D_Loss 1.396250]\n",
      "train [Epoch 0/10] [Batch 744/2345] [Iter 744] [G_Loss 1.150207] [D_Loss 1.321072]\n",
      "train [Epoch 0/10] [Batch 745/2345] [Iter 745] [G_Loss 0.873649] [D_Loss 1.312899]\n",
      "train [Epoch 0/10] [Batch 746/2345] [Iter 746] [G_Loss 0.952256] [D_Loss 1.289888]\n",
      "train [Epoch 0/10] [Batch 747/2345] [Iter 747] [G_Loss 1.094422] [D_Loss 1.185071]\n",
      "train [Epoch 0/10] [Batch 748/2345] [Iter 748] [G_Loss 1.210848] [D_Loss 1.117224]\n",
      "train [Epoch 0/10] [Batch 749/2345] [Iter 749] [G_Loss 1.010655] [D_Loss 1.018389]\n",
      "train [Epoch 0/10] [Batch 750/2345] [Iter 750] [G_Loss 1.011009] [D_Loss 1.120764]\n",
      "train [Epoch 0/10] [Batch 751/2345] [Iter 751] [G_Loss 1.236092] [D_Loss 0.948711]\n",
      "train [Epoch 0/10] [Batch 752/2345] [Iter 752] [G_Loss 0.917413] [D_Loss 0.929979]\n",
      "train [Epoch 0/10] [Batch 753/2345] [Iter 753] [G_Loss 1.258044] [D_Loss 0.836977]\n",
      "train [Epoch 0/10] [Batch 754/2345] [Iter 754] [G_Loss 1.529685] [D_Loss 0.881103]\n",
      "train [Epoch 0/10] [Batch 755/2345] [Iter 755] [G_Loss 1.009992] [D_Loss 0.809390]\n",
      "train [Epoch 0/10] [Batch 756/2345] [Iter 756] [G_Loss 1.491641] [D_Loss 0.625236]\n",
      "train [Epoch 0/10] [Batch 757/2345] [Iter 757] [G_Loss 1.547752] [D_Loss 0.593608]\n",
      "train [Epoch 0/10] [Batch 758/2345] [Iter 758] [G_Loss 1.114792] [D_Loss 1.011852]\n",
      "train [Epoch 0/10] [Batch 759/2345] [Iter 759] [G_Loss 0.800768] [D_Loss 1.605801]\n",
      "train [Epoch 0/10] [Batch 760/2345] [Iter 760] [G_Loss 0.568870] [D_Loss 2.141204]\n",
      "train [Epoch 0/10] [Batch 761/2345] [Iter 761] [G_Loss 1.798556] [D_Loss 1.169150]\n",
      "train [Epoch 0/10] [Batch 762/2345] [Iter 762] [G_Loss 1.871684] [D_Loss 0.693124]\n",
      "train [Epoch 0/10] [Batch 763/2345] [Iter 763] [G_Loss 1.129686] [D_Loss 1.415199]\n",
      "train [Epoch 0/10] [Batch 764/2345] [Iter 764] [G_Loss 2.244728] [D_Loss 1.494493]\n",
      "train [Epoch 0/10] [Batch 765/2345] [Iter 765] [G_Loss 0.959512] [D_Loss 1.484806]\n",
      "train [Epoch 0/10] [Batch 766/2345] [Iter 766] [G_Loss 0.818753] [D_Loss 1.224635]\n",
      "train [Epoch 0/10] [Batch 767/2345] [Iter 767] [G_Loss 1.032013] [D_Loss 1.144190]\n",
      "train [Epoch 0/10] [Batch 768/2345] [Iter 768] [G_Loss 0.915196] [D_Loss 1.165715]\n",
      "train [Epoch 0/10] [Batch 769/2345] [Iter 769] [G_Loss 1.120983] [D_Loss 1.067216]\n",
      "train [Epoch 0/10] [Batch 770/2345] [Iter 770] [G_Loss 0.985262] [D_Loss 1.107029]\n",
      "train [Epoch 0/10] [Batch 771/2345] [Iter 771] [G_Loss 1.170325] [D_Loss 0.904047]\n",
      "train [Epoch 0/10] [Batch 772/2345] [Iter 772] [G_Loss 1.169259] [D_Loss 0.831567]\n",
      "train [Epoch 0/10] [Batch 773/2345] [Iter 773] [G_Loss 1.659977] [D_Loss 0.741451]\n",
      "train [Epoch 0/10] [Batch 774/2345] [Iter 774] [G_Loss 1.109591] [D_Loss 0.929581]\n",
      "train [Epoch 0/10] [Batch 775/2345] [Iter 775] [G_Loss 1.590805] [D_Loss 0.826632]\n",
      "train [Epoch 0/10] [Batch 776/2345] [Iter 776] [G_Loss 0.871612] [D_Loss 1.186488]\n",
      "train [Epoch 0/10] [Batch 777/2345] [Iter 777] [G_Loss 2.309814] [D_Loss 1.149020]\n",
      "train [Epoch 0/10] [Batch 778/2345] [Iter 778] [G_Loss 1.046054] [D_Loss 0.803004]\n",
      "train [Epoch 0/10] [Batch 779/2345] [Iter 779] [G_Loss 1.070920] [D_Loss 0.694238]\n",
      "train [Epoch 0/10] [Batch 780/2345] [Iter 780] [G_Loss 1.797104] [D_Loss 0.643270]\n",
      "train [Epoch 0/10] [Batch 781/2345] [Iter 781] [G_Loss 2.150519] [D_Loss 0.551471]\n",
      "train [Epoch 0/10] [Batch 782/2345] [Iter 782] [G_Loss 1.211563] [D_Loss 0.575637]\n",
      "train [Epoch 0/10] [Batch 783/2345] [Iter 783] [G_Loss 1.913996] [D_Loss 0.609113]\n",
      "train [Epoch 0/10] [Batch 784/2345] [Iter 784] [G_Loss 1.652922] [D_Loss 0.559348]\n",
      "train [Epoch 0/10] [Batch 785/2345] [Iter 785] [G_Loss 2.684563] [D_Loss 1.022704]\n",
      "train [Epoch 0/10] [Batch 786/2345] [Iter 786] [G_Loss 1.378532] [D_Loss 0.668909]\n",
      "train [Epoch 0/10] [Batch 787/2345] [Iter 787] [G_Loss 0.830477] [D_Loss 1.273725]\n",
      "train [Epoch 0/10] [Batch 788/2345] [Iter 788] [G_Loss 1.438719] [D_Loss 2.625670]\n",
      "train [Epoch 0/10] [Batch 789/2345] [Iter 789] [G_Loss 1.182615] [D_Loss 2.139296]\n",
      "train [Epoch 0/10] [Batch 790/2345] [Iter 790] [G_Loss 5.895960] [D_Loss 0.701280]\n",
      "train [Epoch 0/10] [Batch 791/2345] [Iter 791] [G_Loss 4.769296] [D_Loss 1.253746]\n",
      "train [Epoch 0/10] [Batch 792/2345] [Iter 792] [G_Loss 1.049400] [D_Loss 0.861034]\n",
      "train [Epoch 0/10] [Batch 793/2345] [Iter 793] [G_Loss 4.209469] [D_Loss 0.709782]\n",
      "train [Epoch 0/10] [Batch 794/2345] [Iter 794] [G_Loss 3.066565] [D_Loss 0.566146]\n",
      "train [Epoch 0/10] [Batch 795/2345] [Iter 795] [G_Loss 4.322678] [D_Loss 0.499564]\n",
      "train [Epoch 0/10] [Batch 796/2345] [Iter 796] [G_Loss 0.676347] [D_Loss 1.584982]\n",
      "train [Epoch 0/10] [Batch 797/2345] [Iter 797] [G_Loss 2.371021] [D_Loss 1.176019]\n",
      "train [Epoch 0/10] [Batch 798/2345] [Iter 798] [G_Loss 0.959396] [D_Loss 1.397071]\n",
      "train [Epoch 0/10] [Batch 799/2345] [Iter 799] [G_Loss 0.782857] [D_Loss 1.306817]\n",
      "train [Epoch 0/10] [Batch 800/2345] [Iter 800] [G_Loss 1.015041] [D_Loss 1.156197]\n",
      "train [Epoch 0/10] [Batch 801/2345] [Iter 801] [G_Loss 1.199674] [D_Loss 0.910901]\n",
      "train [Epoch 0/10] [Batch 802/2345] [Iter 802] [G_Loss 1.265459] [D_Loss 0.884370]\n",
      "train [Epoch 0/10] [Batch 803/2345] [Iter 803] [G_Loss 1.146793] [D_Loss 0.939503]\n",
      "train [Epoch 0/10] [Batch 804/2345] [Iter 804] [G_Loss 1.355189] [D_Loss 0.778945]\n",
      "train [Epoch 0/10] [Batch 805/2345] [Iter 805] [G_Loss 1.087492] [D_Loss 0.998901]\n",
      "train [Epoch 0/10] [Batch 806/2345] [Iter 806] [G_Loss 1.348393] [D_Loss 0.927174]\n",
      "train [Epoch 0/10] [Batch 807/2345] [Iter 807] [G_Loss 1.252089] [D_Loss 1.006471]\n",
      "train [Epoch 0/10] [Batch 808/2345] [Iter 808] [G_Loss 0.958213] [D_Loss 1.020370]\n",
      "train [Epoch 0/10] [Batch 809/2345] [Iter 809] [G_Loss 1.090837] [D_Loss 0.881123]\n",
      "train [Epoch 0/10] [Batch 810/2345] [Iter 810] [G_Loss 1.146709] [D_Loss 0.773480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 811/2345] [Iter 811] [G_Loss 1.367402] [D_Loss 0.825623]\n",
      "train [Epoch 0/10] [Batch 812/2345] [Iter 812] [G_Loss 1.375891] [D_Loss 0.809408]\n",
      "train [Epoch 0/10] [Batch 813/2345] [Iter 813] [G_Loss 1.192918] [D_Loss 0.868315]\n",
      "train [Epoch 0/10] [Batch 814/2345] [Iter 814] [G_Loss 0.869865] [D_Loss 1.235805]\n",
      "train [Epoch 0/10] [Batch 815/2345] [Iter 815] [G_Loss 1.970086] [D_Loss 1.246959]\n",
      "train [Epoch 0/10] [Batch 816/2345] [Iter 816] [G_Loss 0.779859] [D_Loss 1.399786]\n",
      "train [Epoch 0/10] [Batch 817/2345] [Iter 817] [G_Loss 1.993181] [D_Loss 0.756950]\n",
      "train [Epoch 0/10] [Batch 818/2345] [Iter 818] [G_Loss 1.020989] [D_Loss 1.300241]\n",
      "train [Epoch 0/10] [Batch 819/2345] [Iter 819] [G_Loss 1.192425] [D_Loss 1.321638]\n",
      "train [Epoch 0/10] [Batch 820/2345] [Iter 820] [G_Loss 3.210922] [D_Loss 0.787305]\n",
      "train [Epoch 0/10] [Batch 821/2345] [Iter 821] [G_Loss 1.105762] [D_Loss 0.931853]\n",
      "train [Epoch 0/10] [Batch 822/2345] [Iter 822] [G_Loss 1.660867] [D_Loss 1.360338]\n",
      "train [Epoch 0/10] [Batch 823/2345] [Iter 823] [G_Loss 1.333785] [D_Loss 1.051158]\n",
      "train [Epoch 0/10] [Batch 824/2345] [Iter 824] [G_Loss 1.114412] [D_Loss 0.874234]\n",
      "train [Epoch 0/10] [Batch 825/2345] [Iter 825] [G_Loss 1.882697] [D_Loss 0.691138]\n",
      "train [Epoch 0/10] [Batch 826/2345] [Iter 826] [G_Loss 1.888150] [D_Loss 0.885477]\n",
      "train [Epoch 0/10] [Batch 827/2345] [Iter 827] [G_Loss 1.281109] [D_Loss 0.679646]\n",
      "train [Epoch 0/10] [Batch 828/2345] [Iter 828] [G_Loss 1.122394] [D_Loss 0.829640]\n",
      "train [Epoch 0/10] [Batch 829/2345] [Iter 829] [G_Loss 1.701171] [D_Loss 0.684124]\n",
      "train [Epoch 0/10] [Batch 830/2345] [Iter 830] [G_Loss 1.930964] [D_Loss 0.797175]\n",
      "train [Epoch 0/10] [Batch 831/2345] [Iter 831] [G_Loss 1.685911] [D_Loss 0.694918]\n",
      "train [Epoch 0/10] [Batch 832/2345] [Iter 832] [G_Loss 2.131015] [D_Loss 0.521881]\n",
      "train [Epoch 0/10] [Batch 833/2345] [Iter 833] [G_Loss 3.068691] [D_Loss 1.205737]\n",
      "train [Epoch 0/10] [Batch 834/2345] [Iter 834] [G_Loss 0.603914] [D_Loss 1.869295]\n",
      "train [Epoch 0/10] [Batch 835/2345] [Iter 835] [G_Loss 16.943287] [D_Loss 2.876875]\n",
      "train [Epoch 0/10] [Batch 836/2345] [Iter 836] [G_Loss 6.037400] [D_Loss 0.821271]\n",
      "train [Epoch 0/10] [Batch 837/2345] [Iter 837] [G_Loss 1.256292] [D_Loss 1.164251]\n",
      "train [Epoch 0/10] [Batch 838/2345] [Iter 838] [G_Loss 2.055266] [D_Loss 0.894582]\n",
      "train [Epoch 0/10] [Batch 839/2345] [Iter 839] [G_Loss 1.647221] [D_Loss 0.677802]\n",
      "train [Epoch 0/10] [Batch 840/2345] [Iter 840] [G_Loss 1.576972] [D_Loss 0.683521]\n",
      "train [Epoch 0/10] [Batch 841/2345] [Iter 841] [G_Loss 1.809603] [D_Loss 0.746918]\n",
      "train [Epoch 0/10] [Batch 842/2345] [Iter 842] [G_Loss 1.854666] [D_Loss 0.641811]\n",
      "train [Epoch 0/10] [Batch 843/2345] [Iter 843] [G_Loss 1.610299] [D_Loss 0.631294]\n",
      "train [Epoch 0/10] [Batch 844/2345] [Iter 844] [G_Loss 2.867574] [D_Loss 0.780998]\n",
      "train [Epoch 0/10] [Batch 845/2345] [Iter 845] [G_Loss 1.921845] [D_Loss 0.549561]\n",
      "train [Epoch 0/10] [Batch 846/2345] [Iter 846] [G_Loss 2.396407] [D_Loss 0.558376]\n",
      "train [Epoch 0/10] [Batch 847/2345] [Iter 847] [G_Loss 1.911940] [D_Loss 0.550813]\n",
      "train [Epoch 0/10] [Batch 848/2345] [Iter 848] [G_Loss 1.964160] [D_Loss 0.442021]\n",
      "train [Epoch 0/10] [Batch 849/2345] [Iter 849] [G_Loss 2.076198] [D_Loss 0.361064]\n",
      "train [Epoch 0/10] [Batch 850/2345] [Iter 850] [G_Loss 2.019336] [D_Loss 0.423955]\n",
      "train [Epoch 0/10] [Batch 851/2345] [Iter 851] [G_Loss 1.738705] [D_Loss 0.554443]\n",
      "train [Epoch 0/10] [Batch 852/2345] [Iter 852] [G_Loss 2.595571] [D_Loss 0.549575]\n",
      "train [Epoch 0/10] [Batch 853/2345] [Iter 853] [G_Loss 1.044436] [D_Loss 1.136372]\n",
      "train [Epoch 0/10] [Batch 854/2345] [Iter 854] [G_Loss 4.828278] [D_Loss 1.327918]\n",
      "train [Epoch 0/10] [Batch 855/2345] [Iter 855] [G_Loss 2.250753] [D_Loss 0.759394]\n",
      "train [Epoch 0/10] [Batch 856/2345] [Iter 856] [G_Loss 0.777496] [D_Loss 1.038433]\n",
      "train [Epoch 0/10] [Batch 857/2345] [Iter 857] [G_Loss 1.656987] [D_Loss 0.785335]\n",
      "train [Epoch 0/10] [Batch 858/2345] [Iter 858] [G_Loss 2.174447] [D_Loss 0.914715]\n",
      "train [Epoch 0/10] [Batch 859/2345] [Iter 859] [G_Loss 1.112595] [D_Loss 0.854188]\n",
      "train [Epoch 0/10] [Batch 860/2345] [Iter 860] [G_Loss 0.934795] [D_Loss 1.781097]\n",
      "train [Epoch 0/10] [Batch 861/2345] [Iter 861] [G_Loss 3.392343] [D_Loss 1.213117]\n",
      "train [Epoch 0/10] [Batch 862/2345] [Iter 862] [G_Loss 1.034241] [D_Loss 0.960999]\n",
      "train [Epoch 0/10] [Batch 863/2345] [Iter 863] [G_Loss 0.807381] [D_Loss 1.159611]\n",
      "train [Epoch 0/10] [Batch 864/2345] [Iter 864] [G_Loss 2.271705] [D_Loss 1.495104]\n",
      "train [Epoch 0/10] [Batch 865/2345] [Iter 865] [G_Loss 1.364592] [D_Loss 0.897625]\n",
      "train [Epoch 0/10] [Batch 866/2345] [Iter 866] [G_Loss 0.929516] [D_Loss 0.961980]\n",
      "train [Epoch 0/10] [Batch 867/2345] [Iter 867] [G_Loss 0.935016] [D_Loss 0.976757]\n",
      "train [Epoch 0/10] [Batch 868/2345] [Iter 868] [G_Loss 0.824938] [D_Loss 1.340929]\n",
      "train [Epoch 0/10] [Batch 869/2345] [Iter 869] [G_Loss 0.904487] [D_Loss 1.235205]\n",
      "train [Epoch 0/10] [Batch 870/2345] [Iter 870] [G_Loss 0.788888] [D_Loss 1.271780]\n",
      "train [Epoch 0/10] [Batch 871/2345] [Iter 871] [G_Loss 1.264605] [D_Loss 0.987086]\n",
      "train [Epoch 0/10] [Batch 872/2345] [Iter 872] [G_Loss 1.071506] [D_Loss 1.176613]\n",
      "train [Epoch 0/10] [Batch 873/2345] [Iter 873] [G_Loss 1.936405] [D_Loss 0.727999]\n",
      "train [Epoch 0/10] [Batch 874/2345] [Iter 874] [G_Loss 0.658537] [D_Loss 1.378154]\n",
      "train [Epoch 0/10] [Batch 875/2345] [Iter 875] [G_Loss 1.163586] [D_Loss 0.931278]\n",
      "train [Epoch 0/10] [Batch 876/2345] [Iter 876] [G_Loss 1.391764] [D_Loss 1.091918]\n",
      "train [Epoch 0/10] [Batch 877/2345] [Iter 877] [G_Loss 1.224492] [D_Loss 0.842201]\n",
      "train [Epoch 0/10] [Batch 878/2345] [Iter 878] [G_Loss 1.538306] [D_Loss 0.638875]\n",
      "train [Epoch 0/10] [Batch 879/2345] [Iter 879] [G_Loss 0.778179] [D_Loss 1.280332]\n",
      "train [Epoch 0/10] [Batch 880/2345] [Iter 880] [G_Loss 1.930440] [D_Loss 1.061467]\n",
      "train [Epoch 0/10] [Batch 881/2345] [Iter 881] [G_Loss 1.500787] [D_Loss 0.866651]\n",
      "train [Epoch 0/10] [Batch 882/2345] [Iter 882] [G_Loss 1.237471] [D_Loss 1.030652]\n",
      "train [Epoch 0/10] [Batch 883/2345] [Iter 883] [G_Loss 1.534599] [D_Loss 0.734240]\n",
      "train [Epoch 0/10] [Batch 884/2345] [Iter 884] [G_Loss 1.790073] [D_Loss 0.783021]\n",
      "train [Epoch 0/10] [Batch 885/2345] [Iter 885] [G_Loss 1.449825] [D_Loss 0.749831]\n",
      "train [Epoch 0/10] [Batch 886/2345] [Iter 886] [G_Loss 1.297629] [D_Loss 0.695416]\n",
      "train [Epoch 0/10] [Batch 887/2345] [Iter 887] [G_Loss 1.575095] [D_Loss 0.843115]\n",
      "train [Epoch 0/10] [Batch 888/2345] [Iter 888] [G_Loss 1.221814] [D_Loss 0.711793]\n",
      "train [Epoch 0/10] [Batch 889/2345] [Iter 889] [G_Loss 1.650874] [D_Loss 0.476426]\n",
      "train [Epoch 0/10] [Batch 890/2345] [Iter 890] [G_Loss 2.100559] [D_Loss 0.327468]\n",
      "train [Epoch 0/10] [Batch 891/2345] [Iter 891] [G_Loss 1.927594] [D_Loss 0.353428]\n",
      "train [Epoch 0/10] [Batch 892/2345] [Iter 892] [G_Loss 3.723007] [D_Loss 1.907981]\n",
      "train [Epoch 0/10] [Batch 893/2345] [Iter 893] [G_Loss 1.546384] [D_Loss 0.536427]\n",
      "train [Epoch 0/10] [Batch 894/2345] [Iter 894] [G_Loss 1.154282] [D_Loss 0.737360]\n",
      "train [Epoch 0/10] [Batch 895/2345] [Iter 895] [G_Loss 1.815631] [D_Loss 0.384667]\n",
      "train [Epoch 0/10] [Batch 896/2345] [Iter 896] [G_Loss 2.943212] [D_Loss 0.377115]\n",
      "train [Epoch 0/10] [Batch 897/2345] [Iter 897] [G_Loss 2.663885] [D_Loss 0.235645]\n",
      "train [Epoch 0/10] [Batch 898/2345] [Iter 898] [G_Loss 1.374335] [D_Loss 0.568321]\n",
      "train [Epoch 0/10] [Batch 899/2345] [Iter 899] [G_Loss 0.352187] [D_Loss 2.482037]\n",
      "train [Epoch 0/10] [Batch 900/2345] [Iter 900] [G_Loss 5.447507] [D_Loss 4.271319]\n",
      "train [Epoch 0/10] [Batch 901/2345] [Iter 901] [G_Loss 2.334546] [D_Loss 1.484017]\n",
      "train [Epoch 0/10] [Batch 902/2345] [Iter 902] [G_Loss 1.005968] [D_Loss 1.064717]\n",
      "train [Epoch 0/10] [Batch 903/2345] [Iter 903] [G_Loss 0.575430] [D_Loss 1.284237]\n",
      "train [Epoch 0/10] [Batch 904/2345] [Iter 904] [G_Loss 0.916273] [D_Loss 0.930021]\n",
      "train [Epoch 0/10] [Batch 905/2345] [Iter 905] [G_Loss 1.262654] [D_Loss 1.044029]\n",
      "train [Epoch 0/10] [Batch 906/2345] [Iter 906] [G_Loss 1.110027] [D_Loss 0.914672]\n",
      "train [Epoch 0/10] [Batch 907/2345] [Iter 907] [G_Loss 1.076002] [D_Loss 0.918643]\n",
      "train [Epoch 0/10] [Batch 908/2345] [Iter 908] [G_Loss 1.352942] [D_Loss 0.956120]\n",
      "train [Epoch 0/10] [Batch 909/2345] [Iter 909] [G_Loss 1.109543] [D_Loss 0.931910]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 910/2345] [Iter 910] [G_Loss 1.225683] [D_Loss 0.907227]\n",
      "train [Epoch 0/10] [Batch 911/2345] [Iter 911] [G_Loss 1.088495] [D_Loss 0.769139]\n",
      "train [Epoch 0/10] [Batch 912/2345] [Iter 912] [G_Loss 1.395156] [D_Loss 0.761896]\n",
      "train [Epoch 0/10] [Batch 913/2345] [Iter 913] [G_Loss 1.581485] [D_Loss 0.844873]\n",
      "train [Epoch 0/10] [Batch 914/2345] [Iter 914] [G_Loss 0.988288] [D_Loss 0.919518]\n",
      "train [Epoch 0/10] [Batch 915/2345] [Iter 915] [G_Loss 0.643476] [D_Loss 1.608690]\n",
      "train [Epoch 0/10] [Batch 916/2345] [Iter 916] [G_Loss 1.124736] [D_Loss 1.490537]\n",
      "train [Epoch 0/10] [Batch 917/2345] [Iter 917] [G_Loss 1.691780] [D_Loss 1.261212]\n",
      "train [Epoch 0/10] [Batch 918/2345] [Iter 918] [G_Loss 0.933551] [D_Loss 1.308557]\n",
      "train [Epoch 0/10] [Batch 919/2345] [Iter 919] [G_Loss 1.240162] [D_Loss 1.271639]\n",
      "train [Epoch 0/10] [Batch 920/2345] [Iter 920] [G_Loss 1.269556] [D_Loss 1.011564]\n",
      "train [Epoch 0/10] [Batch 921/2345] [Iter 921] [G_Loss 1.274144] [D_Loss 1.117266]\n",
      "train [Epoch 0/10] [Batch 922/2345] [Iter 922] [G_Loss 1.129866] [D_Loss 0.977747]\n",
      "train [Epoch 0/10] [Batch 923/2345] [Iter 923] [G_Loss 1.187822] [D_Loss 0.978799]\n",
      "train [Epoch 0/10] [Batch 924/2345] [Iter 924] [G_Loss 1.033004] [D_Loss 0.927164]\n",
      "train [Epoch 0/10] [Batch 925/2345] [Iter 925] [G_Loss 1.300173] [D_Loss 0.904030]\n",
      "train [Epoch 0/10] [Batch 926/2345] [Iter 926] [G_Loss 1.232226] [D_Loss 0.678921]\n",
      "train [Epoch 0/10] [Batch 927/2345] [Iter 927] [G_Loss 1.451110] [D_Loss 0.893059]\n",
      "train [Epoch 0/10] [Batch 928/2345] [Iter 928] [G_Loss 1.310472] [D_Loss 0.767249]\n",
      "train [Epoch 0/10] [Batch 929/2345] [Iter 929] [G_Loss 0.992555] [D_Loss 0.891948]\n",
      "train [Epoch 0/10] [Batch 930/2345] [Iter 930] [G_Loss 0.592494] [D_Loss 1.696791]\n",
      "train [Epoch 0/10] [Batch 931/2345] [Iter 931] [G_Loss 1.652975] [D_Loss 1.354325]\n",
      "train [Epoch 0/10] [Batch 932/2345] [Iter 932] [G_Loss 1.526575] [D_Loss 0.971550]\n",
      "train [Epoch 0/10] [Batch 933/2345] [Iter 933] [G_Loss 0.629983] [D_Loss 1.782370]\n",
      "train [Epoch 0/10] [Batch 934/2345] [Iter 934] [G_Loss 1.196836] [D_Loss 1.211257]\n",
      "train [Epoch 0/10] [Batch 935/2345] [Iter 935] [G_Loss 1.046707] [D_Loss 1.226118]\n",
      "train [Epoch 0/10] [Batch 936/2345] [Iter 936] [G_Loss 1.021979] [D_Loss 1.062655]\n",
      "train [Epoch 0/10] [Batch 937/2345] [Iter 937] [G_Loss 1.032025] [D_Loss 1.093876]\n",
      "train [Epoch 0/10] [Batch 938/2345] [Iter 938] [G_Loss 1.241566] [D_Loss 0.965545]\n",
      "train [Epoch 0/10] [Batch 939/2345] [Iter 939] [G_Loss 1.192187] [D_Loss 1.022076]\n",
      "train [Epoch 0/10] [Batch 940/2345] [Iter 940] [G_Loss 1.360058] [D_Loss 0.951924]\n",
      "train [Epoch 0/10] [Batch 941/2345] [Iter 941] [G_Loss 1.287663] [D_Loss 1.052098]\n",
      "train [Epoch 0/10] [Batch 942/2345] [Iter 942] [G_Loss 1.180621] [D_Loss 1.007042]\n",
      "train [Epoch 0/10] [Batch 943/2345] [Iter 943] [G_Loss 1.335677] [D_Loss 0.939434]\n",
      "train [Epoch 0/10] [Batch 944/2345] [Iter 944] [G_Loss 1.088121] [D_Loss 1.146278]\n",
      "train [Epoch 0/10] [Batch 945/2345] [Iter 945] [G_Loss 1.055179] [D_Loss 0.954466]\n",
      "train [Epoch 0/10] [Batch 946/2345] [Iter 946] [G_Loss 1.136687] [D_Loss 0.874541]\n",
      "train [Epoch 0/10] [Batch 947/2345] [Iter 947] [G_Loss 1.598370] [D_Loss 0.765140]\n",
      "train [Epoch 0/10] [Batch 948/2345] [Iter 948] [G_Loss 1.039967] [D_Loss 0.943228]\n",
      "train [Epoch 0/10] [Batch 949/2345] [Iter 949] [G_Loss 1.513987] [D_Loss 0.846957]\n",
      "train [Epoch 0/10] [Batch 950/2345] [Iter 950] [G_Loss 1.599556] [D_Loss 0.805902]\n",
      "train [Epoch 0/10] [Batch 951/2345] [Iter 951] [G_Loss 1.607828] [D_Loss 0.758075]\n",
      "train [Epoch 0/10] [Batch 952/2345] [Iter 952] [G_Loss 1.580733] [D_Loss 0.740902]\n",
      "train [Epoch 0/10] [Batch 953/2345] [Iter 953] [G_Loss 1.416165] [D_Loss 0.686489]\n",
      "train [Epoch 0/10] [Batch 954/2345] [Iter 954] [G_Loss 1.678004] [D_Loss 0.651740]\n",
      "train [Epoch 0/10] [Batch 955/2345] [Iter 955] [G_Loss 1.973155] [D_Loss 0.748942]\n",
      "train [Epoch 0/10] [Batch 956/2345] [Iter 956] [G_Loss 2.185559] [D_Loss 0.935905]\n",
      "train [Epoch 0/10] [Batch 957/2345] [Iter 957] [G_Loss 1.398320] [D_Loss 0.772934]\n",
      "train [Epoch 0/10] [Batch 958/2345] [Iter 958] [G_Loss 2.278388] [D_Loss 0.682868]\n",
      "train [Epoch 0/10] [Batch 959/2345] [Iter 959] [G_Loss 2.326167] [D_Loss 0.518426]\n",
      "train [Epoch 0/10] [Batch 960/2345] [Iter 960] [G_Loss 1.337718] [D_Loss 0.628780]\n",
      "train [Epoch 0/10] [Batch 961/2345] [Iter 961] [G_Loss 2.156604] [D_Loss 0.469519]\n",
      "train [Epoch 0/10] [Batch 962/2345] [Iter 962] [G_Loss 2.170112] [D_Loss 0.456903]\n",
      "train [Epoch 0/10] [Batch 963/2345] [Iter 963] [G_Loss 1.762607] [D_Loss 0.608839]\n",
      "train [Epoch 0/10] [Batch 964/2345] [Iter 964] [G_Loss 1.813772] [D_Loss 0.706043]\n",
      "train [Epoch 0/10] [Batch 965/2345] [Iter 965] [G_Loss 2.270006] [D_Loss 1.128115]\n",
      "train [Epoch 0/10] [Batch 966/2345] [Iter 966] [G_Loss 0.588810] [D_Loss 2.539173]\n",
      "train [Epoch 0/10] [Batch 967/2345] [Iter 967] [G_Loss 2.002202] [D_Loss 2.349297]\n",
      "train [Epoch 0/10] [Batch 968/2345] [Iter 968] [G_Loss 1.561840] [D_Loss 1.431453]\n",
      "train [Epoch 0/10] [Batch 969/2345] [Iter 969] [G_Loss 0.658638] [D_Loss 1.791738]\n",
      "train [Epoch 0/10] [Batch 970/2345] [Iter 970] [G_Loss 1.166362] [D_Loss 1.161646]\n",
      "train [Epoch 0/10] [Batch 971/2345] [Iter 971] [G_Loss 0.971051] [D_Loss 1.382677]\n",
      "train [Epoch 0/10] [Batch 972/2345] [Iter 972] [G_Loss 1.026962] [D_Loss 1.188038]\n",
      "train [Epoch 0/10] [Batch 973/2345] [Iter 973] [G_Loss 1.230241] [D_Loss 1.287134]\n",
      "train [Epoch 0/10] [Batch 974/2345] [Iter 974] [G_Loss 0.889429] [D_Loss 1.211558]\n",
      "train [Epoch 0/10] [Batch 975/2345] [Iter 975] [G_Loss 1.111297] [D_Loss 1.110453]\n",
      "train [Epoch 0/10] [Batch 976/2345] [Iter 976] [G_Loss 1.341909] [D_Loss 1.114259]\n",
      "train [Epoch 0/10] [Batch 977/2345] [Iter 977] [G_Loss 1.103160] [D_Loss 0.987020]\n",
      "train [Epoch 0/10] [Batch 978/2345] [Iter 978] [G_Loss 1.118964] [D_Loss 0.911003]\n",
      "train [Epoch 0/10] [Batch 979/2345] [Iter 979] [G_Loss 2.638648] [D_Loss 1.277534]\n",
      "train [Epoch 0/10] [Batch 980/2345] [Iter 980] [G_Loss 0.053941] [D_Loss 3.882023]\n",
      "train [Epoch 0/10] [Batch 981/2345] [Iter 981] [G_Loss 0.311583] [D_Loss 2.152916]\n",
      "train [Epoch 0/10] [Batch 982/2345] [Iter 982] [G_Loss 1.140214] [D_Loss 1.230847]\n",
      "train [Epoch 0/10] [Batch 983/2345] [Iter 983] [G_Loss 1.716904] [D_Loss 1.426698]\n",
      "train [Epoch 0/10] [Batch 984/2345] [Iter 984] [G_Loss 0.846084] [D_Loss 1.299556]\n",
      "train [Epoch 0/10] [Batch 985/2345] [Iter 985] [G_Loss 0.591880] [D_Loss 1.486845]\n",
      "train [Epoch 0/10] [Batch 986/2345] [Iter 986] [G_Loss 0.809587] [D_Loss 1.357259]\n",
      "train [Epoch 0/10] [Batch 987/2345] [Iter 987] [G_Loss 1.104092] [D_Loss 1.157013]\n",
      "train [Epoch 0/10] [Batch 988/2345] [Iter 988] [G_Loss 0.995398] [D_Loss 1.159907]\n",
      "train [Epoch 0/10] [Batch 989/2345] [Iter 989] [G_Loss 0.868570] [D_Loss 1.281529]\n",
      "train [Epoch 0/10] [Batch 990/2345] [Iter 990] [G_Loss 0.931598] [D_Loss 1.195235]\n",
      "train [Epoch 0/10] [Batch 991/2345] [Iter 991] [G_Loss 0.883844] [D_Loss 1.199405]\n",
      "train [Epoch 0/10] [Batch 992/2345] [Iter 992] [G_Loss 0.984237] [D_Loss 1.152538]\n",
      "train [Epoch 0/10] [Batch 993/2345] [Iter 993] [G_Loss 0.804196] [D_Loss 1.248724]\n",
      "train [Epoch 0/10] [Batch 994/2345] [Iter 994] [G_Loss 0.992269] [D_Loss 1.143729]\n",
      "train [Epoch 0/10] [Batch 995/2345] [Iter 995] [G_Loss 0.924534] [D_Loss 1.183021]\n",
      "train [Epoch 0/10] [Batch 996/2345] [Iter 996] [G_Loss 1.072723] [D_Loss 1.181858]\n",
      "train [Epoch 0/10] [Batch 997/2345] [Iter 997] [G_Loss 1.153237] [D_Loss 1.083491]\n",
      "train [Epoch 0/10] [Batch 998/2345] [Iter 998] [G_Loss 1.025482] [D_Loss 1.092472]\n",
      "train [Epoch 0/10] [Batch 999/2345] [Iter 999] [G_Loss 1.030319] [D_Loss 1.006038]\n",
      "train [Epoch 0/10] [Batch 1000/2345] [Iter 1000] [G_Loss 1.554697] [D_Loss 1.111316]\n",
      "train [Epoch 0/10] [Batch 1001/2345] [Iter 1001] [G_Loss 0.600644] [D_Loss 1.137902]\n",
      "train [Epoch 0/10] [Batch 1002/2345] [Iter 1002] [G_Loss 1.643679] [D_Loss 1.035951]\n",
      "train [Epoch 0/10] [Batch 1003/2345] [Iter 1003] [G_Loss 1.093876] [D_Loss 0.867512]\n",
      "train [Epoch 0/10] [Batch 1004/2345] [Iter 1004] [G_Loss 1.298349] [D_Loss 0.781546]\n",
      "train [Epoch 0/10] [Batch 1005/2345] [Iter 1005] [G_Loss 1.398488] [D_Loss 0.557948]\n",
      "train [Epoch 0/10] [Batch 1006/2345] [Iter 1006] [G_Loss 1.730176] [D_Loss 0.702175]\n",
      "train [Epoch 0/10] [Batch 1007/2345] [Iter 1007] [G_Loss 1.361168] [D_Loss 0.991532]\n",
      "train [Epoch 0/10] [Batch 1008/2345] [Iter 1008] [G_Loss 1.716758] [D_Loss 1.179746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 1009/2345] [Iter 1009] [G_Loss 0.730410] [D_Loss 1.115590]\n",
      "train [Epoch 0/10] [Batch 1010/2345] [Iter 1010] [G_Loss 1.664168] [D_Loss 0.946118]\n",
      "train [Epoch 0/10] [Batch 1011/2345] [Iter 1011] [G_Loss 1.507249] [D_Loss 1.181693]\n",
      "train [Epoch 0/10] [Batch 1012/2345] [Iter 1012] [G_Loss 0.769055] [D_Loss 1.240191]\n",
      "train [Epoch 0/10] [Batch 1013/2345] [Iter 1013] [G_Loss 1.366730] [D_Loss 1.103008]\n",
      "train [Epoch 0/10] [Batch 1014/2345] [Iter 1014] [G_Loss 3.842514] [D_Loss 0.906776]\n",
      "train [Epoch 0/10] [Batch 1015/2345] [Iter 1015] [G_Loss 2.792632] [D_Loss 0.650463]\n",
      "train [Epoch 0/10] [Batch 1016/2345] [Iter 1016] [G_Loss 2.431350] [D_Loss 0.640589]\n",
      "train [Epoch 0/10] [Batch 1017/2345] [Iter 1017] [G_Loss 2.169079] [D_Loss 0.950889]\n",
      "train [Epoch 0/10] [Batch 1018/2345] [Iter 1018] [G_Loss 1.064803] [D_Loss 1.280001]\n",
      "train [Epoch 0/10] [Batch 1019/2345] [Iter 1019] [G_Loss 1.101610] [D_Loss 1.243395]\n",
      "train [Epoch 0/10] [Batch 1020/2345] [Iter 1020] [G_Loss 1.538345] [D_Loss 1.433537]\n",
      "train [Epoch 0/10] [Batch 1021/2345] [Iter 1021] [G_Loss 1.293001] [D_Loss 0.913178]\n",
      "train [Epoch 0/10] [Batch 1022/2345] [Iter 1022] [G_Loss 1.268338] [D_Loss 0.883234]\n",
      "train [Epoch 0/10] [Batch 1023/2345] [Iter 1023] [G_Loss 1.331007] [D_Loss 0.874009]\n",
      "train [Epoch 0/10] [Batch 1024/2345] [Iter 1024] [G_Loss 1.318475] [D_Loss 0.787980]\n",
      "train [Epoch 0/10] [Batch 1025/2345] [Iter 1025] [G_Loss 1.102940] [D_Loss 0.820917]\n",
      "train [Epoch 0/10] [Batch 1026/2345] [Iter 1026] [G_Loss 1.388128] [D_Loss 0.772396]\n",
      "train [Epoch 0/10] [Batch 1027/2345] [Iter 1027] [G_Loss 1.519240] [D_Loss 0.819522]\n",
      "train [Epoch 0/10] [Batch 1028/2345] [Iter 1028] [G_Loss 1.575220] [D_Loss 0.511853]\n",
      "train [Epoch 0/10] [Batch 1029/2345] [Iter 1029] [G_Loss 1.153077] [D_Loss 0.711505]\n",
      "train [Epoch 0/10] [Batch 1030/2345] [Iter 1030] [G_Loss 1.721711] [D_Loss 0.548065]\n",
      "train [Epoch 0/10] [Batch 1031/2345] [Iter 1031] [G_Loss 0.631580] [D_Loss 1.348132]\n",
      "train [Epoch 0/10] [Batch 1032/2345] [Iter 1032] [G_Loss 3.035445] [D_Loss 1.678173]\n",
      "train [Epoch 0/10] [Batch 1033/2345] [Iter 1033] [G_Loss 0.730814] [D_Loss 2.078430]\n",
      "train [Epoch 0/10] [Batch 1034/2345] [Iter 1034] [G_Loss 2.042958] [D_Loss 1.013777]\n",
      "train [Epoch 0/10] [Batch 1035/2345] [Iter 1035] [G_Loss 3.745980] [D_Loss 0.650773]\n",
      "train [Epoch 0/10] [Batch 1036/2345] [Iter 1036] [G_Loss 2.713164] [D_Loss 0.624260]\n",
      "train [Epoch 0/10] [Batch 1037/2345] [Iter 1037] [G_Loss 0.582337] [D_Loss 2.517398]\n",
      "train [Epoch 0/10] [Batch 1038/2345] [Iter 1038] [G_Loss 1.776044] [D_Loss 1.345858]\n",
      "train [Epoch 0/10] [Batch 1039/2345] [Iter 1039] [G_Loss 1.586498] [D_Loss 1.117281]\n",
      "train [Epoch 0/10] [Batch 1040/2345] [Iter 1040] [G_Loss 1.401063] [D_Loss 1.033352]\n",
      "train [Epoch 0/10] [Batch 1041/2345] [Iter 1041] [G_Loss 1.149197] [D_Loss 0.941228]\n",
      "train [Epoch 0/10] [Batch 1042/2345] [Iter 1042] [G_Loss 1.046087] [D_Loss 1.036359]\n",
      "train [Epoch 0/10] [Batch 1043/2345] [Iter 1043] [G_Loss 0.719661] [D_Loss 1.311884]\n",
      "train [Epoch 0/10] [Batch 1044/2345] [Iter 1044] [G_Loss 0.692380] [D_Loss 1.534247]\n",
      "train [Epoch 0/10] [Batch 1045/2345] [Iter 1045] [G_Loss 0.652561] [D_Loss 1.756244]\n",
      "train [Epoch 0/10] [Batch 1046/2345] [Iter 1046] [G_Loss 1.084947] [D_Loss 1.505268]\n",
      "train [Epoch 0/10] [Batch 1047/2345] [Iter 1047] [G_Loss 0.972735] [D_Loss 1.279683]\n",
      "train [Epoch 0/10] [Batch 1048/2345] [Iter 1048] [G_Loss 1.105876] [D_Loss 1.219582]\n",
      "train [Epoch 0/10] [Batch 1049/2345] [Iter 1049] [G_Loss 1.240059] [D_Loss 1.229270]\n",
      "train [Epoch 0/10] [Batch 1050/2345] [Iter 1050] [G_Loss 1.107928] [D_Loss 1.226921]\n",
      "train [Epoch 0/10] [Batch 1051/2345] [Iter 1051] [G_Loss 0.947875] [D_Loss 1.270655]\n",
      "train [Epoch 0/10] [Batch 1052/2345] [Iter 1052] [G_Loss 0.928422] [D_Loss 1.216925]\n",
      "train [Epoch 0/10] [Batch 1053/2345] [Iter 1053] [G_Loss 0.989062] [D_Loss 1.324798]\n",
      "train [Epoch 0/10] [Batch 1054/2345] [Iter 1054] [G_Loss 1.035855] [D_Loss 1.190444]\n",
      "train [Epoch 0/10] [Batch 1055/2345] [Iter 1055] [G_Loss 0.637820] [D_Loss 1.491301]\n",
      "train [Epoch 0/10] [Batch 1056/2345] [Iter 1056] [G_Loss 0.751941] [D_Loss 1.384139]\n",
      "train [Epoch 0/10] [Batch 1057/2345] [Iter 1057] [G_Loss 1.041670] [D_Loss 1.255769]\n",
      "train [Epoch 0/10] [Batch 1058/2345] [Iter 1058] [G_Loss 0.786973] [D_Loss 1.406255]\n",
      "train [Epoch 0/10] [Batch 1059/2345] [Iter 1059] [G_Loss 1.088995] [D_Loss 1.135486]\n",
      "train [Epoch 0/10] [Batch 1060/2345] [Iter 1060] [G_Loss 0.790070] [D_Loss 1.327575]\n",
      "train [Epoch 0/10] [Batch 1061/2345] [Iter 1061] [G_Loss 1.003896] [D_Loss 1.289114]\n",
      "train [Epoch 0/10] [Batch 1062/2345] [Iter 1062] [G_Loss 0.843512] [D_Loss 1.139538]\n",
      "train [Epoch 0/10] [Batch 1063/2345] [Iter 1063] [G_Loss 1.192489] [D_Loss 1.033989]\n",
      "train [Epoch 0/10] [Batch 1064/2345] [Iter 1064] [G_Loss 0.746260] [D_Loss 1.327276]\n",
      "train [Epoch 0/10] [Batch 1065/2345] [Iter 1065] [G_Loss 1.009076] [D_Loss 1.188212]\n",
      "train [Epoch 0/10] [Batch 1066/2345] [Iter 1066] [G_Loss 0.943641] [D_Loss 1.175347]\n",
      "train [Epoch 0/10] [Batch 1067/2345] [Iter 1067] [G_Loss 0.872832] [D_Loss 1.394051]\n",
      "train [Epoch 0/10] [Batch 1068/2345] [Iter 1068] [G_Loss 0.900752] [D_Loss 1.337504]\n",
      "train [Epoch 0/10] [Batch 1069/2345] [Iter 1069] [G_Loss 0.843781] [D_Loss 1.259380]\n",
      "train [Epoch 0/10] [Batch 1070/2345] [Iter 1070] [G_Loss 0.805950] [D_Loss 1.228256]\n",
      "train [Epoch 0/10] [Batch 1071/2345] [Iter 1071] [G_Loss 1.153484] [D_Loss 1.275472]\n",
      "train [Epoch 0/10] [Batch 1072/2345] [Iter 1072] [G_Loss 0.880058] [D_Loss 1.152977]\n",
      "train [Epoch 0/10] [Batch 1073/2345] [Iter 1073] [G_Loss 0.816194] [D_Loss 1.176041]\n",
      "train [Epoch 0/10] [Batch 1074/2345] [Iter 1074] [G_Loss 1.252967] [D_Loss 1.138503]\n",
      "train [Epoch 0/10] [Batch 1075/2345] [Iter 1075] [G_Loss 0.935691] [D_Loss 1.078851]\n",
      "train [Epoch 0/10] [Batch 1076/2345] [Iter 1076] [G_Loss 0.870239] [D_Loss 1.104957]\n",
      "train [Epoch 0/10] [Batch 1077/2345] [Iter 1077] [G_Loss 1.124207] [D_Loss 0.983588]\n",
      "train [Epoch 0/10] [Batch 1078/2345] [Iter 1078] [G_Loss 0.929161] [D_Loss 1.179265]\n",
      "train [Epoch 0/10] [Batch 1079/2345] [Iter 1079] [G_Loss 0.990378] [D_Loss 1.193232]\n",
      "train [Epoch 0/10] [Batch 1080/2345] [Iter 1080] [G_Loss 0.766190] [D_Loss 1.235762]\n",
      "train [Epoch 0/10] [Batch 1081/2345] [Iter 1081] [G_Loss 1.873286] [D_Loss 1.763600]\n",
      "train [Epoch 0/10] [Batch 1082/2345] [Iter 1082] [G_Loss 0.759137] [D_Loss 1.600987]\n",
      "train [Epoch 0/10] [Batch 1083/2345] [Iter 1083] [G_Loss 0.946421] [D_Loss 1.038060]\n",
      "train [Epoch 0/10] [Batch 1084/2345] [Iter 1084] [G_Loss 0.917275] [D_Loss 1.245700]\n",
      "train [Epoch 0/10] [Batch 1085/2345] [Iter 1085] [G_Loss 1.058786] [D_Loss 1.266950]\n",
      "train [Epoch 0/10] [Batch 1086/2345] [Iter 1086] [G_Loss 1.019170] [D_Loss 1.278690]\n",
      "train [Epoch 0/10] [Batch 1087/2345] [Iter 1087] [G_Loss 1.217077] [D_Loss 1.315719]\n",
      "train [Epoch 0/10] [Batch 1088/2345] [Iter 1088] [G_Loss 1.063094] [D_Loss 1.219994]\n",
      "train [Epoch 0/10] [Batch 1089/2345] [Iter 1089] [G_Loss 1.028550] [D_Loss 1.212835]\n",
      "train [Epoch 0/10] [Batch 1090/2345] [Iter 1090] [G_Loss 0.845948] [D_Loss 1.257772]\n",
      "train [Epoch 0/10] [Batch 1091/2345] [Iter 1091] [G_Loss 0.958464] [D_Loss 1.195050]\n",
      "train [Epoch 0/10] [Batch 1092/2345] [Iter 1092] [G_Loss 1.079216] [D_Loss 1.250531]\n",
      "train [Epoch 0/10] [Batch 1093/2345] [Iter 1093] [G_Loss 0.805879] [D_Loss 1.183149]\n",
      "train [Epoch 0/10] [Batch 1094/2345] [Iter 1094] [G_Loss 0.900345] [D_Loss 1.201478]\n",
      "train [Epoch 0/10] [Batch 1095/2345] [Iter 1095] [G_Loss 0.972548] [D_Loss 1.157180]\n",
      "train [Epoch 0/10] [Batch 1096/2345] [Iter 1096] [G_Loss 1.216218] [D_Loss 1.196343]\n",
      "train [Epoch 0/10] [Batch 1097/2345] [Iter 1097] [G_Loss 0.929291] [D_Loss 1.046070]\n",
      "train [Epoch 0/10] [Batch 1098/2345] [Iter 1098] [G_Loss 0.857295] [D_Loss 1.039119]\n",
      "train [Epoch 0/10] [Batch 1099/2345] [Iter 1099] [G_Loss 0.940970] [D_Loss 1.061453]\n",
      "train [Epoch 0/10] [Batch 1100/2345] [Iter 1100] [G_Loss 1.033479] [D_Loss 0.897048]\n",
      "train [Epoch 0/10] [Batch 1101/2345] [Iter 1101] [G_Loss 1.072691] [D_Loss 1.125644]\n",
      "train [Epoch 0/10] [Batch 1102/2345] [Iter 1102] [G_Loss 0.939639] [D_Loss 1.091644]\n",
      "train [Epoch 0/10] [Batch 1103/2345] [Iter 1103] [G_Loss 1.115809] [D_Loss 1.120903]\n",
      "train [Epoch 0/10] [Batch 1104/2345] [Iter 1104] [G_Loss 1.280685] [D_Loss 0.939351]\n",
      "train [Epoch 0/10] [Batch 1105/2345] [Iter 1105] [G_Loss 0.984461] [D_Loss 1.026980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 1106/2345] [Iter 1106] [G_Loss 2.093707] [D_Loss 1.544147]\n",
      "train [Epoch 0/10] [Batch 1107/2345] [Iter 1107] [G_Loss 0.830258] [D_Loss 1.161013]\n",
      "train [Epoch 0/10] [Batch 1108/2345] [Iter 1108] [G_Loss 0.696276] [D_Loss 1.280792]\n",
      "train [Epoch 0/10] [Batch 1109/2345] [Iter 1109] [G_Loss 0.908347] [D_Loss 1.087331]\n",
      "train [Epoch 0/10] [Batch 1110/2345] [Iter 1110] [G_Loss 1.097542] [D_Loss 0.998530]\n",
      "train [Epoch 0/10] [Batch 1111/2345] [Iter 1111] [G_Loss 1.046138] [D_Loss 1.042374]\n",
      "train [Epoch 0/10] [Batch 1112/2345] [Iter 1112] [G_Loss 0.934621] [D_Loss 1.122895]\n",
      "train [Epoch 0/10] [Batch 1113/2345] [Iter 1113] [G_Loss 1.094148] [D_Loss 1.034296]\n",
      "train [Epoch 0/10] [Batch 1114/2345] [Iter 1114] [G_Loss 1.279360] [D_Loss 0.996682]\n",
      "train [Epoch 0/10] [Batch 1115/2345] [Iter 1115] [G_Loss 0.945276] [D_Loss 1.358512]\n",
      "train [Epoch 0/10] [Batch 1116/2345] [Iter 1116] [G_Loss 1.014031] [D_Loss 1.415836]\n",
      "train [Epoch 0/10] [Batch 1117/2345] [Iter 1117] [G_Loss 1.016656] [D_Loss 1.378160]\n",
      "train [Epoch 0/10] [Batch 1118/2345] [Iter 1118] [G_Loss 1.557129] [D_Loss 1.073626]\n",
      "train [Epoch 0/10] [Batch 1119/2345] [Iter 1119] [G_Loss 1.278155] [D_Loss 0.866112]\n",
      "train [Epoch 0/10] [Batch 1120/2345] [Iter 1120] [G_Loss 1.055954] [D_Loss 0.933752]\n",
      "train [Epoch 0/10] [Batch 1121/2345] [Iter 1121] [G_Loss 1.611651] [D_Loss 0.789147]\n",
      "train [Epoch 0/10] [Batch 1122/2345] [Iter 1122] [G_Loss 1.629913] [D_Loss 0.836467]\n",
      "train [Epoch 0/10] [Batch 1123/2345] [Iter 1123] [G_Loss 1.012320] [D_Loss 0.970670]\n",
      "train [Epoch 0/10] [Batch 1124/2345] [Iter 1124] [G_Loss 1.293962] [D_Loss 0.843080]\n",
      "train [Epoch 0/10] [Batch 1125/2345] [Iter 1125] [G_Loss 1.373980] [D_Loss 0.763274]\n",
      "train [Epoch 0/10] [Batch 1126/2345] [Iter 1126] [G_Loss 1.367216] [D_Loss 0.776854]\n",
      "train [Epoch 0/10] [Batch 1127/2345] [Iter 1127] [G_Loss 1.316055] [D_Loss 0.677079]\n",
      "train [Epoch 0/10] [Batch 1128/2345] [Iter 1128] [G_Loss 1.516924] [D_Loss 0.666951]\n",
      "train [Epoch 0/10] [Batch 1129/2345] [Iter 1129] [G_Loss 1.651373] [D_Loss 0.892038]\n",
      "train [Epoch 0/10] [Batch 1130/2345] [Iter 1130] [G_Loss 0.882907] [D_Loss 1.057152]\n",
      "train [Epoch 0/10] [Batch 1131/2345] [Iter 1131] [G_Loss 2.698904] [D_Loss 1.123564]\n",
      "train [Epoch 0/10] [Batch 1132/2345] [Iter 1132] [G_Loss 1.360023] [D_Loss 0.649271]\n",
      "train [Epoch 0/10] [Batch 1133/2345] [Iter 1133] [G_Loss 0.890250] [D_Loss 0.884164]\n",
      "train [Epoch 0/10] [Batch 1134/2345] [Iter 1134] [G_Loss 0.883660] [D_Loss 2.577526]\n",
      "train [Epoch 0/10] [Batch 1135/2345] [Iter 1135] [G_Loss 2.053215] [D_Loss 1.379106]\n",
      "train [Epoch 0/10] [Batch 1136/2345] [Iter 1136] [G_Loss 2.275581] [D_Loss 0.696629]\n",
      "train [Epoch 0/10] [Batch 1137/2345] [Iter 1137] [G_Loss 1.468367] [D_Loss 0.897992]\n",
      "train [Epoch 0/10] [Batch 1138/2345] [Iter 1138] [G_Loss 12.188713] [D_Loss 0.774058]\n",
      "train [Epoch 0/10] [Batch 1139/2345] [Iter 1139] [G_Loss 5.193972] [D_Loss 0.734826]\n",
      "train [Epoch 0/10] [Batch 1140/2345] [Iter 1140] [G_Loss 5.224703] [D_Loss 1.128319]\n",
      "train [Epoch 0/10] [Batch 1141/2345] [Iter 1141] [G_Loss 12.728739] [D_Loss 0.681968]\n",
      "train [Epoch 0/10] [Batch 1142/2345] [Iter 1142] [G_Loss 3.533525] [D_Loss 0.848221]\n",
      "train [Epoch 0/10] [Batch 1143/2345] [Iter 1143] [G_Loss 2.449981] [D_Loss 0.548713]\n",
      "train [Epoch 0/10] [Batch 1144/2345] [Iter 1144] [G_Loss 2.099539] [D_Loss 0.565724]\n",
      "train [Epoch 0/10] [Batch 1145/2345] [Iter 1145] [G_Loss 2.675475] [D_Loss 0.434603]\n",
      "train [Epoch 0/10] [Batch 1146/2345] [Iter 1146] [G_Loss 2.404939] [D_Loss 0.475914]\n",
      "train [Epoch 0/10] [Batch 1147/2345] [Iter 1147] [G_Loss 2.790500] [D_Loss 0.915556]\n",
      "train [Epoch 0/10] [Batch 1148/2345] [Iter 1148] [G_Loss 1.893703] [D_Loss 0.624193]\n",
      "train [Epoch 0/10] [Batch 1149/2345] [Iter 1149] [G_Loss 1.811928] [D_Loss 0.559519]\n",
      "train [Epoch 0/10] [Batch 1150/2345] [Iter 1150] [G_Loss 2.573978] [D_Loss 0.425967]\n",
      "train [Epoch 0/10] [Batch 1151/2345] [Iter 1151] [G_Loss 1.582037] [D_Loss 0.660997]\n",
      "train [Epoch 0/10] [Batch 1152/2345] [Iter 1152] [G_Loss 3.018707] [D_Loss 0.984711]\n",
      "train [Epoch 0/10] [Batch 1153/2345] [Iter 1153] [G_Loss 1.429321] [D_Loss 0.623846]\n",
      "train [Epoch 0/10] [Batch 1154/2345] [Iter 1154] [G_Loss 2.370769] [D_Loss 0.671726]\n",
      "train [Epoch 0/10] [Batch 1155/2345] [Iter 1155] [G_Loss 2.088168] [D_Loss 0.512737]\n",
      "train [Epoch 0/10] [Batch 1156/2345] [Iter 1156] [G_Loss 1.305349] [D_Loss 0.756394]\n",
      "train [Epoch 0/10] [Batch 1157/2345] [Iter 1157] [G_Loss 1.536970] [D_Loss 0.852331]\n",
      "train [Epoch 0/10] [Batch 1158/2345] [Iter 1158] [G_Loss 1.752130] [D_Loss 0.552840]\n",
      "train [Epoch 0/10] [Batch 1159/2345] [Iter 1159] [G_Loss 0.970620] [D_Loss 0.832969]\n",
      "train [Epoch 0/10] [Batch 1160/2345] [Iter 1160] [G_Loss 3.333924] [D_Loss 1.314309]\n",
      "train [Epoch 0/10] [Batch 1161/2345] [Iter 1161] [G_Loss 2.134131] [D_Loss 0.616271]\n",
      "train [Epoch 0/10] [Batch 1162/2345] [Iter 1162] [G_Loss 1.208478] [D_Loss 0.660916]\n",
      "train [Epoch 0/10] [Batch 1163/2345] [Iter 1163] [G_Loss 1.370939] [D_Loss 0.583864]\n",
      "train [Epoch 0/10] [Batch 1164/2345] [Iter 1164] [G_Loss 1.809053] [D_Loss 0.504773]\n",
      "train [Epoch 0/10] [Batch 1165/2345] [Iter 1165] [G_Loss 2.148782] [D_Loss 0.399852]\n",
      "train [Epoch 0/10] [Batch 1166/2345] [Iter 1166] [G_Loss 2.432564] [D_Loss 0.326318]\n",
      "train [Epoch 0/10] [Batch 1167/2345] [Iter 1167] [G_Loss 2.511488] [D_Loss 0.394684]\n",
      "train [Epoch 0/10] [Batch 1168/2345] [Iter 1168] [G_Loss 1.755076] [D_Loss 0.353584]\n",
      "train [Epoch 0/10] [Batch 1169/2345] [Iter 1169] [G_Loss 1.897923] [D_Loss 0.722281]\n",
      "train [Epoch 0/10] [Batch 1170/2345] [Iter 1170] [G_Loss 1.303402] [D_Loss 1.356781]\n",
      "train [Epoch 0/10] [Batch 1171/2345] [Iter 1171] [G_Loss 1.947930] [D_Loss 1.494448]\n",
      "train [Epoch 0/10] [Batch 1172/2345] [Iter 1172] [G_Loss 2.069242] [D_Loss 0.591312]\n",
      "train [Epoch 0/10] [Batch 1173/2345] [Iter 1173] [G_Loss 2.233672] [D_Loss 0.408778]\n",
      "train [Epoch 0/10] [Batch 1174/2345] [Iter 1174] [G_Loss 2.113968] [D_Loss 0.463680]\n",
      "train [Epoch 0/10] [Batch 1175/2345] [Iter 1175] [G_Loss 1.552301] [D_Loss 0.427311]\n",
      "train [Epoch 0/10] [Batch 1176/2345] [Iter 1176] [G_Loss 1.878090] [D_Loss 0.353315]\n",
      "train [Epoch 0/10] [Batch 1177/2345] [Iter 1177] [G_Loss 1.938199] [D_Loss 0.377220]\n",
      "train [Epoch 0/10] [Batch 1178/2345] [Iter 1178] [G_Loss 1.995882] [D_Loss 0.289500]\n",
      "train [Epoch 0/10] [Batch 1179/2345] [Iter 1179] [G_Loss 2.191365] [D_Loss 0.354483]\n",
      "train [Epoch 0/10] [Batch 1180/2345] [Iter 1180] [G_Loss 2.008332] [D_Loss 0.480721]\n",
      "train [Epoch 0/10] [Batch 1181/2345] [Iter 1181] [G_Loss 1.328536] [D_Loss 1.235771]\n",
      "train [Epoch 0/10] [Batch 1182/2345] [Iter 1182] [G_Loss 1.733847] [D_Loss 1.005570]\n",
      "train [Epoch 0/10] [Batch 1183/2345] [Iter 1183] [G_Loss 2.580805] [D_Loss 0.568605]\n",
      "train [Epoch 0/10] [Batch 1184/2345] [Iter 1184] [G_Loss 4.546516] [D_Loss 0.278997]\n",
      "train [Epoch 0/10] [Batch 1185/2345] [Iter 1185] [G_Loss 1.162967] [D_Loss 0.908961]\n",
      "train [Epoch 0/10] [Batch 1186/2345] [Iter 1186] [G_Loss 2.322126] [D_Loss 0.603948]\n",
      "train [Epoch 0/10] [Batch 1187/2345] [Iter 1187] [G_Loss 2.465289] [D_Loss 0.680391]\n",
      "train [Epoch 0/10] [Batch 1188/2345] [Iter 1188] [G_Loss 2.422794] [D_Loss 0.524877]\n",
      "train [Epoch 0/10] [Batch 1189/2345] [Iter 1189] [G_Loss 1.184281] [D_Loss 1.254944]\n",
      "train [Epoch 0/10] [Batch 1190/2345] [Iter 1190] [G_Loss 2.604941] [D_Loss 0.697583]\n",
      "train [Epoch 0/10] [Batch 1191/2345] [Iter 1191] [G_Loss 3.372114] [D_Loss 0.435083]\n",
      "train [Epoch 0/10] [Batch 1192/2345] [Iter 1192] [G_Loss 1.569034] [D_Loss 0.540446]\n",
      "train [Epoch 0/10] [Batch 1193/2345] [Iter 1193] [G_Loss 2.240977] [D_Loss 0.383999]\n",
      "train [Epoch 0/10] [Batch 1194/2345] [Iter 1194] [G_Loss 2.940138] [D_Loss 0.370451]\n",
      "train [Epoch 0/10] [Batch 1195/2345] [Iter 1195] [G_Loss 0.912024] [D_Loss 1.552946]\n",
      "train [Epoch 0/10] [Batch 1196/2345] [Iter 1196] [G_Loss 2.453458] [D_Loss 1.604052]\n",
      "train [Epoch 0/10] [Batch 1197/2345] [Iter 1197] [G_Loss 2.703153] [D_Loss 0.647217]\n",
      "train [Epoch 0/10] [Batch 1198/2345] [Iter 1198] [G_Loss 1.019304] [D_Loss 1.068705]\n",
      "train [Epoch 0/10] [Batch 1199/2345] [Iter 1199] [G_Loss 2.043258] [D_Loss 0.925590]\n",
      "train [Epoch 0/10] [Batch 1200/2345] [Iter 1200] [G_Loss 2.203623] [D_Loss 0.742730]\n",
      "train [Epoch 0/10] [Batch 1201/2345] [Iter 1201] [G_Loss 1.951512] [D_Loss 0.560675]\n",
      "train [Epoch 0/10] [Batch 1202/2345] [Iter 1202] [G_Loss 1.732973] [D_Loss 0.545404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 1203/2345] [Iter 1203] [G_Loss 2.041769] [D_Loss 0.625051]\n",
      "train [Epoch 0/10] [Batch 1204/2345] [Iter 1204] [G_Loss 2.032548] [D_Loss 0.516094]\n",
      "train [Epoch 0/10] [Batch 1205/2345] [Iter 1205] [G_Loss 1.484222] [D_Loss 0.571554]\n",
      "train [Epoch 0/10] [Batch 1206/2345] [Iter 1206] [G_Loss 2.272410] [D_Loss 0.546885]\n",
      "train [Epoch 0/10] [Batch 1207/2345] [Iter 1207] [G_Loss 1.596835] [D_Loss 0.476830]\n",
      "train [Epoch 0/10] [Batch 1208/2345] [Iter 1208] [G_Loss 1.812124] [D_Loss 0.670621]\n",
      "train [Epoch 0/10] [Batch 1209/2345] [Iter 1209] [G_Loss 1.357813] [D_Loss 0.956234]\n",
      "train [Epoch 0/10] [Batch 1210/2345] [Iter 1210] [G_Loss 2.621870] [D_Loss 1.057586]\n",
      "train [Epoch 0/10] [Batch 1211/2345] [Iter 1211] [G_Loss 1.002512] [D_Loss 1.345656]\n",
      "train [Epoch 0/10] [Batch 1212/2345] [Iter 1212] [G_Loss 7.795066] [D_Loss 1.717058]\n",
      "train [Epoch 0/10] [Batch 1213/2345] [Iter 1213] [G_Loss 1.268586] [D_Loss 1.005058]\n",
      "train [Epoch 0/10] [Batch 1214/2345] [Iter 1214] [G_Loss 2.134971] [D_Loss 0.585386]\n",
      "train [Epoch 0/10] [Batch 1215/2345] [Iter 1215] [G_Loss 2.447913] [D_Loss 0.507141]\n",
      "train [Epoch 0/10] [Batch 1216/2345] [Iter 1216] [G_Loss 1.722509] [D_Loss 0.448034]\n",
      "train [Epoch 0/10] [Batch 1217/2345] [Iter 1217] [G_Loss 2.089132] [D_Loss 0.501950]\n",
      "train [Epoch 0/10] [Batch 1218/2345] [Iter 1218] [G_Loss 2.150759] [D_Loss 0.554352]\n",
      "train [Epoch 0/10] [Batch 1219/2345] [Iter 1219] [G_Loss 1.921623] [D_Loss 0.477379]\n",
      "train [Epoch 0/10] [Batch 1220/2345] [Iter 1220] [G_Loss 2.781518] [D_Loss 0.271828]\n",
      "train [Epoch 0/10] [Batch 1221/2345] [Iter 1221] [G_Loss 3.124372] [D_Loss 0.305062]\n",
      "train [Epoch 0/10] [Batch 1222/2345] [Iter 1222] [G_Loss 2.451574] [D_Loss 0.358147]\n",
      "train [Epoch 0/10] [Batch 1223/2345] [Iter 1223] [G_Loss 1.786140] [D_Loss 0.424457]\n",
      "train [Epoch 0/10] [Batch 1224/2345] [Iter 1224] [G_Loss 1.727056] [D_Loss 0.517941]\n",
      "train [Epoch 0/10] [Batch 1225/2345] [Iter 1225] [G_Loss 3.852185] [D_Loss 0.610193]\n",
      "train [Epoch 0/10] [Batch 1226/2345] [Iter 1226] [G_Loss 2.691626] [D_Loss 0.652552]\n",
      "train [Epoch 0/10] [Batch 1227/2345] [Iter 1227] [G_Loss 1.145006] [D_Loss 0.991225]\n",
      "train [Epoch 0/10] [Batch 1228/2345] [Iter 1228] [G_Loss 3.902816] [D_Loss 1.211267]\n",
      "train [Epoch 0/10] [Batch 1229/2345] [Iter 1229] [G_Loss 3.331066] [D_Loss 0.229574]\n",
      "train [Epoch 0/10] [Batch 1230/2345] [Iter 1230] [G_Loss 2.477906] [D_Loss 0.372510]\n",
      "train [Epoch 0/10] [Batch 1231/2345] [Iter 1231] [G_Loss 2.958450] [D_Loss 0.153549]\n",
      "train [Epoch 0/10] [Batch 1232/2345] [Iter 1232] [G_Loss 2.349636] [D_Loss 0.389983]\n",
      "train [Epoch 0/10] [Batch 1233/2345] [Iter 1233] [G_Loss 2.325107] [D_Loss 0.559943]\n",
      "train [Epoch 0/10] [Batch 1234/2345] [Iter 1234] [G_Loss 2.618171] [D_Loss 0.664845]\n",
      "train [Epoch 0/10] [Batch 1235/2345] [Iter 1235] [G_Loss 1.981005] [D_Loss 0.728061]\n",
      "train [Epoch 0/10] [Batch 1236/2345] [Iter 1236] [G_Loss 1.683363] [D_Loss 0.375424]\n",
      "train [Epoch 0/10] [Batch 1237/2345] [Iter 1237] [G_Loss 2.360318] [D_Loss 0.454607]\n",
      "train [Epoch 0/10] [Batch 1238/2345] [Iter 1238] [G_Loss 2.693166] [D_Loss 0.360088]\n",
      "train [Epoch 0/10] [Batch 1239/2345] [Iter 1239] [G_Loss 2.279955] [D_Loss 0.290102]\n",
      "train [Epoch 0/10] [Batch 1240/2345] [Iter 1240] [G_Loss 1.682507] [D_Loss 0.644793]\n",
      "train [Epoch 0/10] [Batch 1241/2345] [Iter 1241] [G_Loss 1.671570] [D_Loss 0.931692]\n",
      "train [Epoch 0/10] [Batch 1242/2345] [Iter 1242] [G_Loss 3.549563] [D_Loss 1.179251]\n",
      "train [Epoch 0/10] [Batch 1243/2345] [Iter 1243] [G_Loss 3.490274] [D_Loss 0.359074]\n",
      "train [Epoch 0/10] [Batch 1244/2345] [Iter 1244] [G_Loss 2.349131] [D_Loss 0.272667]\n",
      "train [Epoch 0/10] [Batch 1245/2345] [Iter 1245] [G_Loss 3.895456] [D_Loss 0.103914]\n",
      "train [Epoch 0/10] [Batch 1246/2345] [Iter 1246] [G_Loss 3.577294] [D_Loss 0.255541]\n",
      "train [Epoch 0/10] [Batch 1247/2345] [Iter 1247] [G_Loss 4.528778] [D_Loss 0.094703]\n",
      "train [Epoch 0/10] [Batch 1248/2345] [Iter 1248] [G_Loss 4.778095] [D_Loss 0.221615]\n",
      "train [Epoch 0/10] [Batch 1249/2345] [Iter 1249] [G_Loss 3.462180] [D_Loss 0.413042]\n",
      "train [Epoch 0/10] [Batch 1250/2345] [Iter 1250] [G_Loss 2.762111] [D_Loss 0.491633]\n",
      "train [Epoch 0/10] [Batch 1251/2345] [Iter 1251] [G_Loss 2.610972] [D_Loss 0.537586]\n",
      "train [Epoch 0/10] [Batch 1252/2345] [Iter 1252] [G_Loss 2.496128] [D_Loss 0.323102]\n",
      "train [Epoch 0/10] [Batch 1253/2345] [Iter 1253] [G_Loss 2.692511] [D_Loss 0.326668]\n",
      "train [Epoch 0/10] [Batch 1254/2345] [Iter 1254] [G_Loss 2.647869] [D_Loss 0.218306]\n",
      "train [Epoch 0/10] [Batch 1255/2345] [Iter 1255] [G_Loss 2.887839] [D_Loss 0.261325]\n",
      "train [Epoch 0/10] [Batch 1256/2345] [Iter 1256] [G_Loss 2.737181] [D_Loss 0.197032]\n",
      "train [Epoch 0/10] [Batch 1257/2345] [Iter 1257] [G_Loss 3.256059] [D_Loss 0.136179]\n",
      "train [Epoch 0/10] [Batch 1258/2345] [Iter 1258] [G_Loss 3.240627] [D_Loss 0.199023]\n",
      "train [Epoch 0/10] [Batch 1259/2345] [Iter 1259] [G_Loss 2.507584] [D_Loss 0.371861]\n",
      "train [Epoch 0/10] [Batch 1260/2345] [Iter 1260] [G_Loss 2.260095] [D_Loss 0.253092]\n",
      "train [Epoch 0/10] [Batch 1261/2345] [Iter 1261] [G_Loss 1.821441] [D_Loss 0.938846]\n",
      "train [Epoch 0/10] [Batch 1262/2345] [Iter 1262] [G_Loss 2.751185] [D_Loss 0.909660]\n",
      "train [Epoch 0/10] [Batch 1263/2345] [Iter 1263] [G_Loss 4.099159] [D_Loss 0.243525]\n",
      "train [Epoch 0/10] [Batch 1264/2345] [Iter 1264] [G_Loss 1.795881] [D_Loss 0.641393]\n",
      "train [Epoch 0/10] [Batch 1265/2345] [Iter 1265] [G_Loss 3.425200] [D_Loss 0.404795]\n",
      "train [Epoch 0/10] [Batch 1266/2345] [Iter 1266] [G_Loss 2.859147] [D_Loss 0.648009]\n",
      "train [Epoch 0/10] [Batch 1267/2345] [Iter 1267] [G_Loss 2.043876] [D_Loss 0.624207]\n",
      "train [Epoch 0/10] [Batch 1268/2345] [Iter 1268] [G_Loss 2.206552] [D_Loss 0.473331]\n",
      "train [Epoch 0/10] [Batch 1269/2345] [Iter 1269] [G_Loss 2.281821] [D_Loss 0.628073]\n",
      "train [Epoch 0/10] [Batch 1270/2345] [Iter 1270] [G_Loss 2.168423] [D_Loss 0.588466]\n",
      "train [Epoch 0/10] [Batch 1271/2345] [Iter 1271] [G_Loss 1.909145] [D_Loss 0.593349]\n",
      "train [Epoch 0/10] [Batch 1272/2345] [Iter 1272] [G_Loss 2.159078] [D_Loss 0.644223]\n",
      "train [Epoch 0/10] [Batch 1273/2345] [Iter 1273] [G_Loss 2.419181] [D_Loss 0.375285]\n",
      "train [Epoch 0/10] [Batch 1274/2345] [Iter 1274] [G_Loss 2.470312] [D_Loss 0.245314]\n",
      "train [Epoch 0/10] [Batch 1275/2345] [Iter 1275] [G_Loss 1.811554] [D_Loss 0.514536]\n",
      "train [Epoch 0/10] [Batch 1276/2345] [Iter 1276] [G_Loss 2.710189] [D_Loss 0.620684]\n",
      "train [Epoch 0/10] [Batch 1277/2345] [Iter 1277] [G_Loss 1.333436] [D_Loss 1.745157]\n",
      "train [Epoch 0/10] [Batch 1278/2345] [Iter 1278] [G_Loss 2.489576] [D_Loss 0.958811]\n",
      "train [Epoch 0/10] [Batch 1279/2345] [Iter 1279] [G_Loss 4.764297] [D_Loss 0.517527]\n",
      "train [Epoch 0/10] [Batch 1280/2345] [Iter 1280] [G_Loss 2.809115] [D_Loss 0.404360]\n",
      "train [Epoch 0/10] [Batch 1281/2345] [Iter 1281] [G_Loss 2.096000] [D_Loss 0.719624]\n",
      "train [Epoch 0/10] [Batch 1282/2345] [Iter 1282] [G_Loss 1.951290] [D_Loss 0.886638]\n",
      "train [Epoch 0/10] [Batch 1283/2345] [Iter 1283] [G_Loss 1.992563] [D_Loss 1.014620]\n",
      "train [Epoch 0/10] [Batch 1284/2345] [Iter 1284] [G_Loss 2.032185] [D_Loss 0.883192]\n",
      "train [Epoch 0/10] [Batch 1285/2345] [Iter 1285] [G_Loss 2.538279] [D_Loss 0.623582]\n",
      "train [Epoch 0/10] [Batch 1286/2345] [Iter 1286] [G_Loss 1.376286] [D_Loss 0.615389]\n",
      "train [Epoch 0/10] [Batch 1287/2345] [Iter 1287] [G_Loss 3.120432] [D_Loss 1.117840]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_gan import *\n",
    "\n",
    "train_model(generator, discriminator, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_gan import *\n",
    "\n",
    "train_model(generator, discriminator, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
