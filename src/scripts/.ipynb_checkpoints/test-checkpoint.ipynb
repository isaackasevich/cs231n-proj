{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "loading annotations into memory...\n",
      "Done (t=0.33s)\n",
      "creating index...\n",
      "index created!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[Epoch 0/5] [Batch 0/988] [loss: 0.002188]\n",
      "[Epoch 0/5] [Batch 1/988] [loss: 0.002053]\n",
      "[Epoch 0/5] [Batch 2/988] [loss: 0.002039]\n",
      "[Epoch 0/5] [Batch 3/988] [loss: 0.002236]\n",
      "[Epoch 0/5] [Batch 4/988] [loss: 0.002075]\n",
      "[Epoch 0/5] [Batch 5/988] [loss: 0.002102]\n",
      "[Epoch 0/5] [Batch 6/988] [loss: 0.002259]\n",
      "[Epoch 0/5] [Batch 7/988] [loss: 0.002200]\n",
      "[Epoch 0/5] [Batch 8/988] [loss: 0.002199]\n",
      "[Epoch 0/5] [Batch 9/988] [loss: 0.001944]\n",
      "[Epoch 0/5] [Batch 10/988] [loss: 0.002274]\n",
      "[Epoch 0/5] [Batch 11/988] [loss: 0.002102]\n",
      "[Epoch 0/5] [Batch 12/988] [loss: 0.002003]\n",
      "[Epoch 0/5] [Batch 13/988] [loss: 0.001908]\n",
      "[Epoch 0/5] [Batch 14/988] [loss: 0.002443]\n",
      "[Epoch 0/5] [Batch 15/988] [loss: 0.002176]\n",
      "[Epoch 0/5] [Batch 16/988] [loss: 0.002230]\n",
      "[Epoch 0/5] [Batch 17/988] [loss: 0.002096]\n",
      "[Epoch 0/5] [Batch 18/988] [loss: 0.002245]\n",
      "[Epoch 0/5] [Batch 19/988] [loss: 0.002075]\n",
      "[Epoch 0/5] [Batch 20/988] [loss: 0.002006]\n",
      "[Epoch 0/5] [Batch 21/988] [loss: 0.002175]\n",
      "[Epoch 0/5] [Batch 22/988] [loss: 0.002078]\n",
      "[Epoch 0/5] [Batch 23/988] [loss: 0.002243]\n",
      "[Epoch 0/5] [Batch 24/988] [loss: 0.002016]\n",
      "[Epoch 0/5] [Batch 25/988] [loss: 0.002340]\n",
      "[Epoch 0/5] [Batch 26/988] [loss: 0.002217]\n",
      "[Epoch 0/5] [Batch 27/988] [loss: 0.002185]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_loop_conv import *\n",
    "\n",
    "model = torch.load('model_state.pt')\n",
    "model.eval()\n",
    "\n",
    "#After first round of training model made it to epoch 1 iteration ~800 loss ~.002\n",
    "\n",
    "\n",
    "train_model(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CIFAR10', 'CIFAR100', 'Cityscapes', 'CocoCaptions', 'CocoDetection', 'DatasetFolder', 'EMNIST', 'FakeData', 'FashionMNIST', 'Flickr30k', 'Flickr8k', 'ImageFolder', 'KMNIST', 'LSUN', 'LSUNClass', 'MNIST', 'Omniglot', 'PhotoTour', 'SBU', 'SEMEION', 'STL10', 'SVHN', 'VOCDetection', 'VOCSegmentation', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'cifar', 'cityscapes', 'coco', 'fakedata', 'flickr', 'folder', 'lsun', 'mnist', 'omniglot', 'phototour', 'sbu', 'semeion', 'stl10', 'svhn', 'utils', 'voc']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.append('../dataprocessing')\n",
    "from dataloader import BlurDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F0C70666DA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F0C70666710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path='~/cs231n-proj/data/cifar'\n",
    "\n",
    "data = BlurDataset.from_single_dataset(path)\n",
    "blurred, tgt = data.train[0]\n",
    "display(blurred)\n",
    "display(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-92391269448a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_state'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "model = torch.load('model_state')\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.31s)\n",
      "creating index...\n",
      "index created!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "train [Epoch 0/10] [Batch 0/1173] [Iter 0] [G_Loss 0.493833] [D_Loss 2.203641]\n",
      "train [Epoch 0/10] [Batch 1/1173] [Iter 1] [G_Loss 674.883240] [D_Loss 125.520073]\n",
      "train [Epoch 0/10] [Batch 2/1173] [Iter 2] [G_Loss 301.344299] [D_Loss 46.836433]\n",
      "train [Epoch 0/10] [Batch 3/1173] [Iter 3] [G_Loss 69.886955] [D_Loss 0.052057]\n",
      "train [Epoch 0/10] [Batch 4/1173] [Iter 4] [G_Loss 0.510172] [D_Loss 16.354734]\n",
      "train [Epoch 0/10] [Batch 5/1173] [Iter 5] [G_Loss 113.438980] [D_Loss 2.373063]\n",
      "train [Epoch 0/10] [Batch 6/1173] [Iter 6] [G_Loss 134.948532] [D_Loss 5.490053]\n",
      "train [Epoch 0/10] [Batch 7/1173] [Iter 7] [G_Loss 114.567993] [D_Loss 0.466100]\n",
      "train [Epoch 0/10] [Batch 8/1173] [Iter 8] [G_Loss 88.668175] [D_Loss 0.152591]\n",
      "train [Epoch 0/10] [Batch 9/1173] [Iter 9] [G_Loss 74.638626] [D_Loss 0.009849]\n",
      "train [Epoch 0/10] [Batch 10/1173] [Iter 10] [G_Loss 66.055412] [D_Loss 0.001357]\n",
      "train [Epoch 0/10] [Batch 11/1173] [Iter 11] [G_Loss 54.559677] [D_Loss 0.000671]\n",
      "train [Epoch 0/10] [Batch 12/1173] [Iter 12] [G_Loss 49.660896] [D_Loss 0.000611]\n",
      "train [Epoch 0/10] [Batch 13/1173] [Iter 13] [G_Loss 42.606277] [D_Loss 0.001275]\n",
      "train [Epoch 0/10] [Batch 14/1173] [Iter 14] [G_Loss 36.368149] [D_Loss 0.000927]\n",
      "train [Epoch 0/10] [Batch 15/1173] [Iter 15] [G_Loss 29.939405] [D_Loss 0.194802]\n",
      "train [Epoch 0/10] [Batch 16/1173] [Iter 16] [G_Loss 28.472900] [D_Loss 0.395836]\n",
      "train [Epoch 0/10] [Batch 17/1173] [Iter 17] [G_Loss 36.078133] [D_Loss 0.005085]\n",
      "train [Epoch 0/10] [Batch 18/1173] [Iter 18] [G_Loss 40.660179] [D_Loss 0.011676]\n",
      "train [Epoch 0/10] [Batch 19/1173] [Iter 19] [G_Loss 39.008965] [D_Loss 0.009847]\n",
      "train [Epoch 0/10] [Batch 20/1173] [Iter 20] [G_Loss 35.704945] [D_Loss 0.030245]\n",
      "train [Epoch 0/10] [Batch 21/1173] [Iter 21] [G_Loss 31.137524] [D_Loss 0.109420]\n",
      "train [Epoch 0/10] [Batch 22/1173] [Iter 22] [G_Loss 28.655024] [D_Loss 0.144223]\n",
      "train [Epoch 0/10] [Batch 23/1173] [Iter 23] [G_Loss 29.362431] [D_Loss 0.095637]\n",
      "train [Epoch 0/10] [Batch 24/1173] [Iter 24] [G_Loss 32.539139] [D_Loss 0.022792]\n",
      "train [Epoch 0/10] [Batch 28/1173] [Iter 28] [G_Loss 34.021927] [D_Loss 0.045531]\n",
      "train [Epoch 0/10] [Batch 29/1173] [Iter 29] [G_Loss 26.682611] [D_Loss 0.066918]\n",
      "train [Epoch 0/10] [Batch 30/1173] [Iter 30] [G_Loss 24.352364] [D_Loss 0.064115]\n",
      "train [Epoch 0/10] [Batch 31/1173] [Iter 31] [G_Loss 29.502714] [D_Loss 0.013755]\n",
      "train [Epoch 0/10] [Batch 32/1173] [Iter 32] [G_Loss 25.093300] [D_Loss 0.137542]\n",
      "train [Epoch 0/10] [Batch 33/1173] [Iter 33] [G_Loss 34.945335] [D_Loss 0.061793]\n",
      "train [Epoch 0/10] [Batch 34/1173] [Iter 34] [G_Loss 35.885639] [D_Loss 0.133826]\n",
      "train [Epoch 0/10] [Batch 35/1173] [Iter 35] [G_Loss 31.989681] [D_Loss 0.014515]\n",
      "train [Epoch 0/10] [Batch 36/1173] [Iter 36] [G_Loss 27.577915] [D_Loss 0.003049]\n",
      "train [Epoch 0/10] [Batch 37/1173] [Iter 37] [G_Loss 25.722031] [D_Loss 0.024684]\n",
      "train [Epoch 0/10] [Batch 38/1173] [Iter 38] [G_Loss 23.078606] [D_Loss 0.015526]\n",
      "train [Epoch 0/10] [Batch 39/1173] [Iter 39] [G_Loss 18.107021] [D_Loss 0.078417]\n",
      "train [Epoch 0/10] [Batch 40/1173] [Iter 40] [G_Loss 28.436308] [D_Loss 0.056261]\n",
      "train [Epoch 0/10] [Batch 41/1173] [Iter 41] [G_Loss 29.550442] [D_Loss 0.003768]\n",
      "train [Epoch 0/10] [Batch 42/1173] [Iter 42] [G_Loss 26.042814] [D_Loss 0.041510]\n",
      "train [Epoch 0/10] [Batch 43/1173] [Iter 43] [G_Loss 27.387913] [D_Loss 0.000886]\n",
      "train [Epoch 0/10] [Batch 44/1173] [Iter 44] [G_Loss 22.257515] [D_Loss 0.001380]\n",
      "train [Epoch 0/10] [Batch 45/1173] [Iter 45] [G_Loss 18.639555] [D_Loss 0.026396]\n",
      "train [Epoch 0/10] [Batch 46/1173] [Iter 46] [G_Loss 24.987617] [D_Loss 0.033011]\n",
      "train [Epoch 0/10] [Batch 47/1173] [Iter 47] [G_Loss 24.382910] [D_Loss 0.007350]\n",
      "train [Epoch 0/10] [Batch 48/1173] [Iter 48] [G_Loss 23.873701] [D_Loss 0.002454]\n",
      "train [Epoch 0/10] [Batch 49/1173] [Iter 49] [G_Loss 18.070724] [D_Loss 0.009465]\n",
      "train [Epoch 0/10] [Batch 50/1173] [Iter 50] [G_Loss 19.772228] [D_Loss 0.003191]\n",
      "train [Epoch 0/10] [Batch 51/1173] [Iter 51] [G_Loss 21.349613] [D_Loss 0.005034]\n",
      "train [Epoch 0/10] [Batch 52/1173] [Iter 52] [G_Loss 19.993479] [D_Loss 0.017909]\n",
      "train [Epoch 0/10] [Batch 53/1173] [Iter 53] [G_Loss 18.164583] [D_Loss 0.106479]\n",
      "train [Epoch 0/10] [Batch 54/1173] [Iter 54] [G_Loss 16.030895] [D_Loss 0.003659]\n",
      "train [Epoch 0/10] [Batch 55/1173] [Iter 55] [G_Loss 13.778596] [D_Loss 0.015373]\n",
      "train [Epoch 0/10] [Batch 56/1173] [Iter 56] [G_Loss 20.206188] [D_Loss 0.000832]\n",
      "train [Epoch 0/10] [Batch 57/1173] [Iter 57] [G_Loss 20.257839] [D_Loss 0.148618]\n",
      "train [Epoch 0/10] [Batch 58/1173] [Iter 58] [G_Loss 16.247236] [D_Loss 0.001240]\n",
      "train [Epoch 0/10] [Batch 59/1173] [Iter 59] [G_Loss 13.493039] [D_Loss 0.003348]\n",
      "train [Epoch 0/10] [Batch 60/1173] [Iter 60] [G_Loss 14.656669] [D_Loss 0.003408]\n",
      "train [Epoch 0/10] [Batch 61/1173] [Iter 61] [G_Loss 17.072779] [D_Loss 0.000207]\n",
      "train [Epoch 0/10] [Batch 62/1173] [Iter 62] [G_Loss 16.065317] [D_Loss 0.003428]\n",
      "train [Epoch 0/10] [Batch 63/1173] [Iter 63] [G_Loss 15.763718] [D_Loss 0.000872]\n",
      "train [Epoch 0/10] [Batch 64/1173] [Iter 64] [G_Loss 17.874569] [D_Loss 0.032297]\n",
      "train [Epoch 0/10] [Batch 65/1173] [Iter 65] [G_Loss 13.217429] [D_Loss 0.032919]\n",
      "train [Epoch 0/10] [Batch 66/1173] [Iter 66] [G_Loss 11.362195] [D_Loss 0.026790]\n",
      "train [Epoch 0/10] [Batch 67/1173] [Iter 67] [G_Loss 22.966343] [D_Loss 0.040338]\n",
      "train [Epoch 0/10] [Batch 68/1173] [Iter 68] [G_Loss 25.222891] [D_Loss 0.008066]\n",
      "train [Epoch 0/10] [Batch 69/1173] [Iter 69] [G_Loss 25.036684] [D_Loss 0.025094]\n",
      "train [Epoch 0/10] [Batch 70/1173] [Iter 70] [G_Loss 21.496077] [D_Loss 0.000153]\n",
      "train [Epoch 0/10] [Batch 71/1173] [Iter 71] [G_Loss 20.432549] [D_Loss 0.004430]\n",
      "train [Epoch 0/10] [Batch 72/1173] [Iter 72] [G_Loss 18.047041] [D_Loss 0.000236]\n",
      "train [Epoch 0/10] [Batch 73/1173] [Iter 73] [G_Loss 16.177620] [D_Loss 0.002426]\n",
      "train [Epoch 0/10] [Batch 74/1173] [Iter 74] [G_Loss 14.616003] [D_Loss 0.000393]\n",
      "train [Epoch 0/10] [Batch 75/1173] [Iter 75] [G_Loss 14.968057] [D_Loss 0.003701]\n",
      "train [Epoch 0/10] [Batch 76/1173] [Iter 76] [G_Loss 14.490449] [D_Loss 0.000301]\n",
      "train [Epoch 0/10] [Batch 77/1173] [Iter 77] [G_Loss 15.657353] [D_Loss 0.000157]\n",
      "train [Epoch 0/10] [Batch 78/1173] [Iter 78] [G_Loss 12.047318] [D_Loss 0.036272]\n",
      "train [Epoch 0/10] [Batch 79/1173] [Iter 79] [G_Loss 9.778085] [D_Loss 0.014007]\n",
      "train [Epoch 0/10] [Batch 80/1173] [Iter 80] [G_Loss 14.650275] [D_Loss 0.000534]\n",
      "train [Epoch 0/10] [Batch 81/1173] [Iter 81] [G_Loss 14.464670] [D_Loss 0.000640]\n",
      "train [Epoch 0/10] [Batch 82/1173] [Iter 82] [G_Loss 14.118337] [D_Loss 0.000740]\n",
      "train [Epoch 0/10] [Batch 83/1173] [Iter 83] [G_Loss 17.569084] [D_Loss 0.000047]\n",
      "train [Epoch 0/10] [Batch 84/1173] [Iter 84] [G_Loss 18.577702] [D_Loss 0.000181]\n",
      "train [Epoch 0/10] [Batch 85/1173] [Iter 85] [G_Loss 15.094489] [D_Loss 0.002835]\n",
      "train [Epoch 0/10] [Batch 86/1173] [Iter 86] [G_Loss 13.071340] [D_Loss 0.003116]\n",
      "train [Epoch 0/10] [Batch 87/1173] [Iter 87] [G_Loss 12.351211] [D_Loss 0.004100]\n",
      "train [Epoch 0/10] [Batch 88/1173] [Iter 88] [G_Loss 11.266454] [D_Loss 0.039223]\n",
      "train [Epoch 0/10] [Batch 89/1173] [Iter 89] [G_Loss 10.516628] [D_Loss 0.013534]\n",
      "train [Epoch 0/10] [Batch 90/1173] [Iter 90] [G_Loss 13.629787] [D_Loss 0.008700]\n",
      "train [Epoch 0/10] [Batch 91/1173] [Iter 91] [G_Loss 14.345196] [D_Loss 0.000948]\n",
      "train [Epoch 0/10] [Batch 92/1173] [Iter 92] [G_Loss 13.522466] [D_Loss 0.000323]\n",
      "train [Epoch 0/10] [Batch 93/1173] [Iter 93] [G_Loss 14.671781] [D_Loss 0.001108]\n",
      "train [Epoch 0/10] [Batch 94/1173] [Iter 94] [G_Loss 17.679117] [D_Loss 0.001330]\n",
      "train [Epoch 0/10] [Batch 95/1173] [Iter 95] [G_Loss 12.952890] [D_Loss 0.000248]\n",
      "train [Epoch 0/10] [Batch 96/1173] [Iter 96] [G_Loss 12.541500] [D_Loss 0.000717]\n",
      "train [Epoch 0/10] [Batch 97/1173] [Iter 97] [G_Loss 11.131610] [D_Loss 0.050323]\n",
      "train [Epoch 0/10] [Batch 98/1173] [Iter 98] [G_Loss 23.852516] [D_Loss 0.075292]\n",
      "train [Epoch 0/10] [Batch 99/1173] [Iter 99] [G_Loss 25.773151] [D_Loss 0.158360]\n",
      "train [Epoch 0/10] [Batch 100/1173] [Iter 100] [G_Loss 19.147488] [D_Loss 0.011909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 101/1173] [Iter 101] [G_Loss 16.375139] [D_Loss 0.002723]\n",
      "train [Epoch 0/10] [Batch 102/1173] [Iter 102] [G_Loss 14.346645] [D_Loss 0.001791]\n",
      "train [Epoch 0/10] [Batch 103/1173] [Iter 103] [G_Loss 12.451850] [D_Loss 0.008482]\n",
      "train [Epoch 0/10] [Batch 104/1173] [Iter 104] [G_Loss 14.753071] [D_Loss 0.002745]\n",
      "train [Epoch 0/10] [Batch 105/1173] [Iter 105] [G_Loss 14.870546] [D_Loss 0.001495]\n",
      "train [Epoch 0/10] [Batch 106/1173] [Iter 106] [G_Loss 15.203244] [D_Loss 0.003486]\n",
      "train [Epoch 0/10] [Batch 107/1173] [Iter 107] [G_Loss 14.650886] [D_Loss 0.076077]\n",
      "train [Epoch 0/10] [Batch 108/1173] [Iter 108] [G_Loss 11.966388] [D_Loss 0.098818]\n",
      "train [Epoch 0/10] [Batch 109/1173] [Iter 109] [G_Loss 18.054235] [D_Loss 0.042267]\n",
      "train [Epoch 0/10] [Batch 110/1173] [Iter 110] [G_Loss 18.117941] [D_Loss 0.020115]\n",
      "train [Epoch 0/10] [Batch 111/1173] [Iter 111] [G_Loss 14.929398] [D_Loss 0.041182]\n",
      "train [Epoch 0/10] [Batch 112/1173] [Iter 112] [G_Loss 11.562938] [D_Loss 0.032316]\n",
      "train [Epoch 0/10] [Batch 113/1173] [Iter 113] [G_Loss 7.095781] [D_Loss 0.054444]\n",
      "train [Epoch 0/10] [Batch 114/1173] [Iter 114] [G_Loss 9.650575] [D_Loss 0.002023]\n",
      "train [Epoch 0/10] [Batch 115/1173] [Iter 115] [G_Loss 10.173660] [D_Loss 0.005581]\n",
      "train [Epoch 0/10] [Batch 116/1173] [Iter 116] [G_Loss 10.939243] [D_Loss 0.009973]\n",
      "train [Epoch 0/10] [Batch 117/1173] [Iter 117] [G_Loss 11.572419] [D_Loss 0.000350]\n",
      "train [Epoch 0/10] [Batch 118/1173] [Iter 118] [G_Loss 11.049182] [D_Loss 0.011739]\n",
      "train [Epoch 0/10] [Batch 119/1173] [Iter 119] [G_Loss 11.329189] [D_Loss 0.001319]\n",
      "train [Epoch 0/10] [Batch 120/1173] [Iter 120] [G_Loss 9.751713] [D_Loss 0.003699]\n",
      "train [Epoch 0/10] [Batch 121/1173] [Iter 121] [G_Loss 9.929332] [D_Loss 0.001983]\n",
      "train [Epoch 0/10] [Batch 122/1173] [Iter 122] [G_Loss 9.019070] [D_Loss 0.004732]\n",
      "train [Epoch 0/10] [Batch 123/1173] [Iter 123] [G_Loss 9.140446] [D_Loss 0.005532]\n",
      "train [Epoch 0/10] [Batch 124/1173] [Iter 124] [G_Loss 11.251077] [D_Loss 0.006005]\n",
      "train [Epoch 0/10] [Batch 125/1173] [Iter 125] [G_Loss 10.323828] [D_Loss 0.001198]\n",
      "train [Epoch 0/10] [Batch 126/1173] [Iter 126] [G_Loss 8.547989] [D_Loss 0.002235]\n",
      "train [Epoch 0/10] [Batch 127/1173] [Iter 127] [G_Loss 8.954485] [D_Loss 0.003243]\n",
      "train [Epoch 0/10] [Batch 128/1173] [Iter 128] [G_Loss 9.470411] [D_Loss 0.002493]\n",
      "train [Epoch 0/10] [Batch 129/1173] [Iter 129] [G_Loss 9.441149] [D_Loss 0.002052]\n",
      "train [Epoch 0/10] [Batch 130/1173] [Iter 130] [G_Loss 10.538243] [D_Loss 0.001739]\n",
      "train [Epoch 0/10] [Batch 131/1173] [Iter 131] [G_Loss 9.722578] [D_Loss 0.001767]\n",
      "train [Epoch 0/10] [Batch 132/1173] [Iter 132] [G_Loss 11.558187] [D_Loss 0.057035]\n",
      "train [Epoch 0/10] [Batch 133/1173] [Iter 133] [G_Loss 8.720489] [D_Loss 0.018122]\n",
      "train [Epoch 0/10] [Batch 134/1173] [Iter 134] [G_Loss 4.587414] [D_Loss 0.122100]\n",
      "train [Epoch 0/10] [Batch 135/1173] [Iter 135] [G_Loss 23.354462] [D_Loss 0.129157]\n",
      "train [Epoch 0/10] [Batch 136/1173] [Iter 136] [G_Loss 23.544418] [D_Loss 0.025291]\n",
      "train [Epoch 0/10] [Batch 137/1173] [Iter 137] [G_Loss 19.900881] [D_Loss 0.053758]\n",
      "train [Epoch 0/10] [Batch 138/1173] [Iter 138] [G_Loss 17.547894] [D_Loss 0.000161]\n",
      "train [Epoch 0/10] [Batch 139/1173] [Iter 139] [G_Loss 15.983893] [D_Loss 0.000279]\n",
      "train [Epoch 0/10] [Batch 140/1173] [Iter 140] [G_Loss 14.920711] [D_Loss 0.000896]\n",
      "train [Epoch 0/10] [Batch 141/1173] [Iter 141] [G_Loss 14.783335] [D_Loss 0.001013]\n",
      "train [Epoch 0/10] [Batch 142/1173] [Iter 142] [G_Loss 13.951438] [D_Loss 0.002031]\n",
      "train [Epoch 0/10] [Batch 143/1173] [Iter 143] [G_Loss 12.710536] [D_Loss 0.140441]\n",
      "train [Epoch 0/10] [Batch 144/1173] [Iter 144] [G_Loss 27.626455] [D_Loss 0.048194]\n",
      "train [Epoch 0/10] [Batch 145/1173] [Iter 145] [G_Loss 24.766972] [D_Loss 0.018040]\n",
      "train [Epoch 0/10] [Batch 146/1173] [Iter 146] [G_Loss 22.849161] [D_Loss 0.180218]\n",
      "train [Epoch 0/10] [Batch 147/1173] [Iter 147] [G_Loss 14.725278] [D_Loss 0.009956]\n",
      "train [Epoch 0/10] [Batch 148/1173] [Iter 148] [G_Loss 15.547597] [D_Loss 0.000048]\n",
      "train [Epoch 0/10] [Batch 149/1173] [Iter 149] [G_Loss 13.307736] [D_Loss 0.000444]\n",
      "train [Epoch 0/10] [Batch 150/1173] [Iter 150] [G_Loss 8.689032] [D_Loss 0.001370]\n",
      "train [Epoch 0/10] [Batch 151/1173] [Iter 151] [G_Loss 4.541499] [D_Loss 0.251056]\n",
      "train [Epoch 0/10] [Batch 152/1173] [Iter 152] [G_Loss 36.671566] [D_Loss 0.782954]\n",
      "train [Epoch 0/10] [Batch 153/1173] [Iter 153] [G_Loss 23.469625] [D_Loss 0.017858]\n",
      "train [Epoch 0/10] [Batch 154/1173] [Iter 154] [G_Loss 14.384545] [D_Loss 0.000218]\n",
      "train [Epoch 0/10] [Batch 155/1173] [Iter 155] [G_Loss 11.082197] [D_Loss 0.001278]\n",
      "train [Epoch 0/10] [Batch 156/1173] [Iter 156] [G_Loss 8.993929] [D_Loss 0.002480]\n",
      "train [Epoch 0/10] [Batch 157/1173] [Iter 157] [G_Loss 8.368836] [D_Loss 0.006689]\n",
      "train [Epoch 0/10] [Batch 158/1173] [Iter 158] [G_Loss 7.063937] [D_Loss 0.006850]\n",
      "train [Epoch 0/10] [Batch 159/1173] [Iter 159] [G_Loss 7.416465] [D_Loss 0.004096]\n",
      "train [Epoch 0/10] [Batch 160/1173] [Iter 160] [G_Loss 7.047956] [D_Loss 0.006275]\n",
      "train [Epoch 0/10] [Batch 161/1173] [Iter 161] [G_Loss 7.063539] [D_Loss 0.004743]\n",
      "train [Epoch 0/10] [Batch 162/1173] [Iter 162] [G_Loss 8.029637] [D_Loss 0.001738]\n",
      "train [Epoch 0/10] [Batch 163/1173] [Iter 163] [G_Loss 8.300643] [D_Loss 0.001456]\n",
      "train [Epoch 0/10] [Batch 164/1173] [Iter 164] [G_Loss 8.586360] [D_Loss 0.011565]\n",
      "train [Epoch 0/10] [Batch 165/1173] [Iter 165] [G_Loss 8.254032] [D_Loss 0.002510]\n",
      "train [Epoch 0/10] [Batch 166/1173] [Iter 166] [G_Loss 7.605236] [D_Loss 0.004342]\n",
      "train [Epoch 0/10] [Batch 167/1173] [Iter 167] [G_Loss 7.426723] [D_Loss 0.003894]\n",
      "train [Epoch 0/10] [Batch 168/1173] [Iter 168] [G_Loss 7.821157] [D_Loss 0.004920]\n",
      "train [Epoch 0/10] [Batch 169/1173] [Iter 169] [G_Loss 7.779600] [D_Loss 0.002421]\n",
      "train [Epoch 0/10] [Batch 170/1173] [Iter 170] [G_Loss 8.532163] [D_Loss 0.001457]\n",
      "train [Epoch 0/10] [Batch 171/1173] [Iter 171] [G_Loss 8.488072] [D_Loss 0.000763]\n",
      "train [Epoch 0/10] [Batch 172/1173] [Iter 172] [G_Loss 8.001223] [D_Loss 0.001864]\n",
      "train [Epoch 0/10] [Batch 173/1173] [Iter 173] [G_Loss 10.213850] [D_Loss 0.001696]\n",
      "train [Epoch 0/10] [Batch 174/1173] [Iter 174] [G_Loss 9.037488] [D_Loss 0.001564]\n",
      "train [Epoch 0/10] [Batch 175/1173] [Iter 175] [G_Loss 8.527789] [D_Loss 0.000868]\n",
      "train [Epoch 0/10] [Batch 176/1173] [Iter 176] [G_Loss 8.745566] [D_Loss 0.003820]\n",
      "train [Epoch 0/10] [Batch 177/1173] [Iter 177] [G_Loss 8.352148] [D_Loss 0.003789]\n",
      "train [Epoch 0/10] [Batch 178/1173] [Iter 178] [G_Loss 8.347116] [D_Loss 0.007135]\n",
      "train [Epoch 0/10] [Batch 179/1173] [Iter 179] [G_Loss 7.564906] [D_Loss 0.005523]\n",
      "train [Epoch 0/10] [Batch 180/1173] [Iter 180] [G_Loss 7.606180] [D_Loss 0.018389]\n",
      "train [Epoch 0/10] [Batch 181/1173] [Iter 181] [G_Loss 8.756359] [D_Loss 0.006386]\n",
      "train [Epoch 0/10] [Batch 182/1173] [Iter 182] [G_Loss 9.360872] [D_Loss 0.000533]\n",
      "train [Epoch 0/10] [Batch 183/1173] [Iter 183] [G_Loss 9.338165] [D_Loss 0.001431]\n",
      "train [Epoch 0/10] [Batch 184/1173] [Iter 184] [G_Loss 8.816184] [D_Loss 0.014826]\n",
      "train [Epoch 0/10] [Batch 185/1173] [Iter 185] [G_Loss 8.852843] [D_Loss 0.005287]\n",
      "train [Epoch 0/10] [Batch 186/1173] [Iter 186] [G_Loss 8.602194] [D_Loss 0.005255]\n",
      "train [Epoch 0/10] [Batch 187/1173] [Iter 187] [G_Loss 8.959701] [D_Loss 0.008198]\n",
      "train [Epoch 0/10] [Batch 188/1173] [Iter 188] [G_Loss 7.688233] [D_Loss 0.023519]\n",
      "train [Epoch 0/10] [Batch 189/1173] [Iter 189] [G_Loss 9.632919] [D_Loss 0.008579]\n",
      "train [Epoch 0/10] [Batch 190/1173] [Iter 190] [G_Loss 10.457003] [D_Loss 0.000278]\n",
      "train [Epoch 0/10] [Batch 191/1173] [Iter 191] [G_Loss 10.312341] [D_Loss 0.001663]\n",
      "train [Epoch 0/10] [Batch 192/1173] [Iter 192] [G_Loss 11.405070] [D_Loss 0.000613]\n",
      "train [Epoch 0/10] [Batch 193/1173] [Iter 193] [G_Loss 9.424427] [D_Loss 0.011041]\n",
      "train [Epoch 0/10] [Batch 194/1173] [Iter 194] [G_Loss 9.989724] [D_Loss 0.000774]\n",
      "train [Epoch 0/10] [Batch 195/1173] [Iter 195] [G_Loss 7.847937] [D_Loss 0.005975]\n",
      "train [Epoch 0/10] [Batch 196/1173] [Iter 196] [G_Loss 10.096628] [D_Loss 0.001083]\n",
      "train [Epoch 0/10] [Batch 197/1173] [Iter 197] [G_Loss 8.318398] [D_Loss 0.022647]\n",
      "train [Epoch 0/10] [Batch 198/1173] [Iter 198] [G_Loss 11.396534] [D_Loss 0.000821]\n",
      "train [Epoch 0/10] [Batch 199/1173] [Iter 199] [G_Loss 12.103160] [D_Loss 0.039311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 200/1173] [Iter 200] [G_Loss 8.911354] [D_Loss 0.004652]\n",
      "train [Epoch 0/10] [Batch 201/1173] [Iter 201] [G_Loss 8.534307] [D_Loss 0.020572]\n",
      "train [Epoch 0/10] [Batch 202/1173] [Iter 202] [G_Loss 11.020671] [D_Loss 0.011239]\n",
      "train [Epoch 0/10] [Batch 203/1173] [Iter 203] [G_Loss 12.005119] [D_Loss 0.088761]\n",
      "train [Epoch 0/10] [Batch 204/1173] [Iter 204] [G_Loss 11.104124] [D_Loss 0.070004]\n",
      "train [Epoch 0/10] [Batch 205/1173] [Iter 205] [G_Loss 7.012040] [D_Loss 0.028299]\n",
      "train [Epoch 0/10] [Batch 206/1173] [Iter 206] [G_Loss 8.310213] [D_Loss 0.006347]\n",
      "train [Epoch 0/10] [Batch 207/1173] [Iter 207] [G_Loss 8.296386] [D_Loss 0.010993]\n",
      "train [Epoch 0/10] [Batch 208/1173] [Iter 208] [G_Loss 9.059128] [D_Loss 0.004006]\n",
      "train [Epoch 0/10] [Batch 209/1173] [Iter 209] [G_Loss 10.039147] [D_Loss 0.050091]\n",
      "train [Epoch 0/10] [Batch 210/1173] [Iter 210] [G_Loss 8.151641] [D_Loss 0.019703]\n",
      "train [Epoch 0/10] [Batch 211/1173] [Iter 211] [G_Loss 7.921334] [D_Loss 0.020384]\n",
      "train [Epoch 0/10] [Batch 212/1173] [Iter 212] [G_Loss 9.618491] [D_Loss 0.022278]\n",
      "train [Epoch 0/10] [Batch 213/1173] [Iter 213] [G_Loss 8.047652] [D_Loss 0.008687]\n",
      "train [Epoch 0/10] [Batch 214/1173] [Iter 214] [G_Loss 8.444828] [D_Loss 0.031409]\n",
      "train [Epoch 0/10] [Batch 215/1173] [Iter 215] [G_Loss 13.997474] [D_Loss 0.052856]\n",
      "train [Epoch 0/10] [Batch 216/1173] [Iter 216] [G_Loss 8.589205] [D_Loss 0.099298]\n",
      "train [Epoch 0/10] [Batch 217/1173] [Iter 217] [G_Loss 8.398984] [D_Loss 0.008595]\n",
      "train [Epoch 0/10] [Batch 218/1173] [Iter 218] [G_Loss 10.215121] [D_Loss 0.002610]\n",
      "train [Epoch 0/10] [Batch 219/1173] [Iter 219] [G_Loss 11.570611] [D_Loss 0.001842]\n",
      "train [Epoch 0/10] [Batch 220/1173] [Iter 220] [G_Loss 9.461491] [D_Loss 0.001372]\n",
      "train [Epoch 0/10] [Batch 221/1173] [Iter 221] [G_Loss 10.545447] [D_Loss 0.022304]\n",
      "train [Epoch 0/10] [Batch 222/1173] [Iter 222] [G_Loss 10.032074] [D_Loss 0.001641]\n",
      "train [Epoch 0/10] [Batch 223/1173] [Iter 223] [G_Loss 7.103053] [D_Loss 0.007719]\n",
      "train [Epoch 0/10] [Batch 224/1173] [Iter 224] [G_Loss 7.127667] [D_Loss 0.025745]\n",
      "train [Epoch 0/10] [Batch 225/1173] [Iter 225] [G_Loss 8.841865] [D_Loss 0.006519]\n",
      "train [Epoch 0/10] [Batch 226/1173] [Iter 226] [G_Loss 11.281540] [D_Loss 0.035532]\n",
      "train [Epoch 0/10] [Batch 227/1173] [Iter 227] [G_Loss 5.690552] [D_Loss 0.060660]\n",
      "train [Epoch 0/10] [Batch 228/1173] [Iter 228] [G_Loss 15.081691] [D_Loss 0.353646]\n",
      "train [Epoch 0/10] [Batch 229/1173] [Iter 229] [G_Loss 0.324092] [D_Loss 10.870398]\n",
      "train [Epoch 0/10] [Batch 230/1173] [Iter 230] [G_Loss 40.790760] [D_Loss 34.687046]\n",
      "train [Epoch 0/10] [Batch 231/1173] [Iter 231] [G_Loss 4.401018] [D_Loss 0.527458]\n",
      "train [Epoch 0/10] [Batch 232/1173] [Iter 232] [G_Loss 0.806033] [D_Loss 0.711339]\n",
      "train [Epoch 0/10] [Batch 233/1173] [Iter 233] [G_Loss 0.756982] [D_Loss 0.717826]\n",
      "train [Epoch 0/10] [Batch 234/1173] [Iter 234] [G_Loss 2.101019] [D_Loss 0.324669]\n",
      "train [Epoch 0/10] [Batch 235/1173] [Iter 235] [G_Loss 3.598639] [D_Loss 0.129267]\n",
      "train [Epoch 0/10] [Batch 236/1173] [Iter 236] [G_Loss 6.550705] [D_Loss 0.224899]\n",
      "train [Epoch 0/10] [Batch 237/1173] [Iter 237] [G_Loss 2.758888] [D_Loss 0.146867]\n",
      "train [Epoch 0/10] [Batch 238/1173] [Iter 238] [G_Loss 4.217233] [D_Loss 0.038590]\n",
      "train [Epoch 0/10] [Batch 239/1173] [Iter 239] [G_Loss 5.158587] [D_Loss 0.016336]\n",
      "train [Epoch 0/10] [Batch 240/1173] [Iter 240] [G_Loss 6.207601] [D_Loss 0.011111]\n",
      "train [Epoch 0/10] [Batch 241/1173] [Iter 241] [G_Loss 6.316996] [D_Loss 0.010707]\n",
      "train [Epoch 0/10] [Batch 242/1173] [Iter 242] [G_Loss 6.125718] [D_Loss 0.010042]\n",
      "train [Epoch 0/10] [Batch 243/1173] [Iter 243] [G_Loss 6.048086] [D_Loss 0.015811]\n",
      "train [Epoch 0/10] [Batch 244/1173] [Iter 244] [G_Loss 6.489137] [D_Loss 0.051853]\n",
      "train [Epoch 0/10] [Batch 245/1173] [Iter 245] [G_Loss 5.822195] [D_Loss 0.025339]\n",
      "train [Epoch 0/10] [Batch 246/1173] [Iter 246] [G_Loss 6.074644] [D_Loss 0.009531]\n",
      "train [Epoch 0/10] [Batch 247/1173] [Iter 247] [G_Loss 6.841683] [D_Loss 0.046710]\n",
      "train [Epoch 0/10] [Batch 248/1173] [Iter 248] [G_Loss 5.896709] [D_Loss 0.133819]\n",
      "train [Epoch 0/10] [Batch 249/1173] [Iter 249] [G_Loss 9.778298] [D_Loss 0.587230]\n",
      "train [Epoch 0/10] [Batch 250/1173] [Iter 250] [G_Loss 0.748266] [D_Loss 3.590557]\n",
      "train [Epoch 0/10] [Batch 251/1173] [Iter 251] [G_Loss 6.466605] [D_Loss 0.276128]\n",
      "train [Epoch 0/10] [Batch 252/1173] [Iter 252] [G_Loss 6.318599] [D_Loss 0.031061]\n",
      "train [Epoch 0/10] [Batch 253/1173] [Iter 253] [G_Loss 5.580228] [D_Loss 0.023717]\n",
      "train [Epoch 0/10] [Batch 254/1173] [Iter 254] [G_Loss 5.163952] [D_Loss 0.072343]\n",
      "train [Epoch 0/10] [Batch 255/1173] [Iter 255] [G_Loss 6.151568] [D_Loss 0.090317]\n",
      "train [Epoch 0/10] [Batch 256/1173] [Iter 256] [G_Loss 5.958211] [D_Loss 0.070266]\n",
      "train [Epoch 0/10] [Batch 257/1173] [Iter 257] [G_Loss 6.580637] [D_Loss 0.187940]\n",
      "train [Epoch 0/10] [Batch 258/1173] [Iter 258] [G_Loss 4.793533] [D_Loss 0.041494]\n",
      "train [Epoch 0/10] [Batch 259/1173] [Iter 259] [G_Loss 6.470911] [D_Loss 0.007906]\n",
      "train [Epoch 0/10] [Batch 260/1173] [Iter 260] [G_Loss 6.577343] [D_Loss 0.064390]\n",
      "train [Epoch 0/10] [Batch 261/1173] [Iter 261] [G_Loss 5.793898] [D_Loss 0.017784]\n",
      "train [Epoch 0/10] [Batch 262/1173] [Iter 262] [G_Loss 4.779802] [D_Loss 0.023890]\n",
      "train [Epoch 0/10] [Batch 263/1173] [Iter 263] [G_Loss 5.235600] [D_Loss 0.017182]\n",
      "train [Epoch 0/10] [Batch 264/1173] [Iter 264] [G_Loss 6.151830] [D_Loss 0.026031]\n",
      "train [Epoch 0/10] [Batch 265/1173] [Iter 265] [G_Loss 5.533805] [D_Loss 0.026060]\n",
      "train [Epoch 0/10] [Batch 266/1173] [Iter 266] [G_Loss 6.055663] [D_Loss 0.041182]\n",
      "train [Epoch 0/10] [Batch 267/1173] [Iter 267] [G_Loss 5.109761] [D_Loss 0.146697]\n",
      "train [Epoch 0/10] [Batch 268/1173] [Iter 268] [G_Loss 9.550879] [D_Loss 0.360478]\n",
      "train [Epoch 0/10] [Batch 269/1173] [Iter 269] [G_Loss 6.736756] [D_Loss 0.177347]\n",
      "train [Epoch 0/10] [Batch 270/1173] [Iter 270] [G_Loss 5.689977] [D_Loss 0.257571]\n",
      "train [Epoch 0/10] [Batch 271/1173] [Iter 271] [G_Loss 5.850863] [D_Loss 0.136126]\n",
      "train [Epoch 0/10] [Batch 272/1173] [Iter 272] [G_Loss 6.717287] [D_Loss 0.100340]\n",
      "train [Epoch 0/10] [Batch 273/1173] [Iter 273] [G_Loss 5.615702] [D_Loss 0.153206]\n",
      "train [Epoch 0/10] [Batch 274/1173] [Iter 274] [G_Loss 4.098744] [D_Loss 0.184468]\n",
      "train [Epoch 0/10] [Batch 275/1173] [Iter 275] [G_Loss 5.852423] [D_Loss 0.060816]\n",
      "train [Epoch 0/10] [Batch 276/1173] [Iter 276] [G_Loss 5.195735] [D_Loss 0.136752]\n",
      "train [Epoch 0/10] [Batch 277/1173] [Iter 277] [G_Loss 5.799579] [D_Loss 0.076137]\n",
      "train [Epoch 0/10] [Batch 278/1173] [Iter 278] [G_Loss 4.213767] [D_Loss 0.065247]\n",
      "train [Epoch 0/10] [Batch 279/1173] [Iter 279] [G_Loss 6.075961] [D_Loss 0.048199]\n",
      "train [Epoch 0/10] [Batch 280/1173] [Iter 280] [G_Loss 6.016119] [D_Loss 0.084173]\n",
      "train [Epoch 0/10] [Batch 281/1173] [Iter 281] [G_Loss 2.836951] [D_Loss 1.327034]\n",
      "train [Epoch 0/10] [Batch 282/1173] [Iter 282] [G_Loss 16.956360] [D_Loss 8.080203]\n",
      "train [Epoch 0/10] [Batch 283/1173] [Iter 283] [G_Loss 0.291584] [D_Loss 19.008310]\n",
      "train [Epoch 0/10] [Batch 284/1173] [Iter 284] [G_Loss 17.003914] [D_Loss 28.139765]\n",
      "train [Epoch 0/10] [Batch 285/1173] [Iter 285] [G_Loss 1.532961] [D_Loss 19.642565]\n",
      "train [Epoch 0/10] [Batch 286/1173] [Iter 286] [G_Loss 10.740038] [D_Loss 0.864224]\n",
      "train [Epoch 0/10] [Batch 287/1173] [Iter 287] [G_Loss 10.893683] [D_Loss 1.529550]\n",
      "train [Epoch 0/10] [Batch 288/1173] [Iter 288] [G_Loss 5.129844] [D_Loss 0.722379]\n",
      "train [Epoch 0/10] [Batch 289/1173] [Iter 289] [G_Loss 2.155705] [D_Loss 0.437158]\n",
      "train [Epoch 0/10] [Batch 290/1173] [Iter 290] [G_Loss 2.329245] [D_Loss 0.319606]\n",
      "train [Epoch 0/10] [Batch 291/1173] [Iter 291] [G_Loss 3.718125] [D_Loss 0.332244]\n",
      "train [Epoch 0/10] [Batch 292/1173] [Iter 292] [G_Loss 2.495378] [D_Loss 0.370782]\n",
      "train [Epoch 0/10] [Batch 293/1173] [Iter 293] [G_Loss 3.242620] [D_Loss 0.380429]\n",
      "train [Epoch 0/10] [Batch 294/1173] [Iter 294] [G_Loss 3.485649] [D_Loss 0.254400]\n",
      "train [Epoch 0/10] [Batch 295/1173] [Iter 295] [G_Loss 3.692875] [D_Loss 0.459704]\n",
      "train [Epoch 0/10] [Batch 296/1173] [Iter 296] [G_Loss 3.254429] [D_Loss 0.370586]\n",
      "train [Epoch 0/10] [Batch 297/1173] [Iter 297] [G_Loss 3.101141] [D_Loss 0.296615]\n",
      "train [Epoch 0/10] [Batch 298/1173] [Iter 298] [G_Loss 3.360204] [D_Loss 0.264131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 299/1173] [Iter 299] [G_Loss 3.745106] [D_Loss 0.352218]\n",
      "train [Epoch 0/10] [Batch 300/1173] [Iter 300] [G_Loss 3.680326] [D_Loss 0.220975]\n",
      "train [Epoch 0/10] [Batch 301/1173] [Iter 301] [G_Loss 3.767313] [D_Loss 0.225635]\n",
      "train [Epoch 0/10] [Batch 302/1173] [Iter 302] [G_Loss 3.969647] [D_Loss 0.321067]\n",
      "train [Epoch 0/10] [Batch 303/1173] [Iter 303] [G_Loss 3.954273] [D_Loss 0.198600]\n",
      "train [Epoch 0/10] [Batch 304/1173] [Iter 304] [G_Loss 4.247560] [D_Loss 0.171020]\n",
      "train [Epoch 0/10] [Batch 305/1173] [Iter 305] [G_Loss 3.963195] [D_Loss 0.250815]\n",
      "train [Epoch 0/10] [Batch 306/1173] [Iter 306] [G_Loss 3.561711] [D_Loss 0.281733]\n",
      "train [Epoch 0/10] [Batch 307/1173] [Iter 307] [G_Loss 2.430889] [D_Loss 0.240634]\n",
      "train [Epoch 0/10] [Batch 308/1173] [Iter 308] [G_Loss 4.597533] [D_Loss 0.313526]\n",
      "train [Epoch 0/10] [Batch 309/1173] [Iter 309] [G_Loss 3.928946] [D_Loss 0.112523]\n",
      "train [Epoch 0/10] [Batch 310/1173] [Iter 310] [G_Loss 3.814043] [D_Loss 0.158724]\n",
      "train [Epoch 0/10] [Batch 311/1173] [Iter 311] [G_Loss 3.833890] [D_Loss 0.119871]\n",
      "train [Epoch 0/10] [Batch 312/1173] [Iter 312] [G_Loss 4.323869] [D_Loss 0.124893]\n",
      "train [Epoch 0/10] [Batch 313/1173] [Iter 313] [G_Loss 4.102058] [D_Loss 0.146935]\n",
      "train [Epoch 0/10] [Batch 317/1173] [Iter 317] [G_Loss 4.057517] [D_Loss 0.089276]\n",
      "train [Epoch 0/10] [Batch 318/1173] [Iter 318] [G_Loss 5.820610] [D_Loss 0.041333]\n",
      "train [Epoch 0/10] [Batch 319/1173] [Iter 319] [G_Loss 5.995920] [D_Loss 0.111199]\n",
      "train [Epoch 0/10] [Batch 320/1173] [Iter 320] [G_Loss 5.792278] [D_Loss 0.076659]\n",
      "train [Epoch 0/10] [Batch 321/1173] [Iter 321] [G_Loss 4.796927] [D_Loss 0.054323]\n",
      "train [Epoch 0/10] [Batch 322/1173] [Iter 322] [G_Loss 5.408373] [D_Loss 0.064203]\n",
      "train [Epoch 0/10] [Batch 323/1173] [Iter 323] [G_Loss 4.627058] [D_Loss 0.087921]\n",
      "train [Epoch 0/10] [Batch 324/1173] [Iter 324] [G_Loss 4.376064] [D_Loss 0.129601]\n",
      "train [Epoch 0/10] [Batch 325/1173] [Iter 325] [G_Loss 4.812062] [D_Loss 0.079481]\n",
      "train [Epoch 0/10] [Batch 326/1173] [Iter 326] [G_Loss 5.183519] [D_Loss 0.041880]\n",
      "train [Epoch 0/10] [Batch 327/1173] [Iter 327] [G_Loss 5.247722] [D_Loss 0.111560]\n",
      "train [Epoch 0/10] [Batch 328/1173] [Iter 328] [G_Loss 2.858813] [D_Loss 0.141660]\n",
      "train [Epoch 0/10] [Batch 329/1173] [Iter 329] [G_Loss 10.412123] [D_Loss 0.174203]\n",
      "train [Epoch 0/10] [Batch 330/1173] [Iter 330] [G_Loss 8.964044] [D_Loss 0.155183]\n",
      "train [Epoch 0/10] [Batch 331/1173] [Iter 331] [G_Loss 5.633670] [D_Loss 0.094508]\n",
      "train [Epoch 0/10] [Batch 332/1173] [Iter 332] [G_Loss 4.647562] [D_Loss 0.059190]\n",
      "train [Epoch 0/10] [Batch 333/1173] [Iter 333] [G_Loss 6.387661] [D_Loss 0.050850]\n",
      "train [Epoch 0/10] [Batch 334/1173] [Iter 334] [G_Loss 7.327721] [D_Loss 0.027849]\n",
      "train [Epoch 0/10] [Batch 335/1173] [Iter 335] [G_Loss 6.906880] [D_Loss 0.071627]\n",
      "train [Epoch 0/10] [Batch 336/1173] [Iter 336] [G_Loss 5.900218] [D_Loss 0.019753]\n",
      "train [Epoch 0/10] [Batch 337/1173] [Iter 337] [G_Loss 5.310549] [D_Loss 0.068313]\n",
      "train [Epoch 0/10] [Batch 338/1173] [Iter 338] [G_Loss 4.539547] [D_Loss 0.048406]\n",
      "train [Epoch 0/10] [Batch 339/1173] [Iter 339] [G_Loss 7.555967] [D_Loss 0.074318]\n",
      "train [Epoch 0/10] [Batch 340/1173] [Iter 340] [G_Loss 5.779572] [D_Loss 0.023542]\n",
      "train [Epoch 0/10] [Batch 341/1173] [Iter 341] [G_Loss 6.044384] [D_Loss 0.048917]\n",
      "train [Epoch 0/10] [Batch 342/1173] [Iter 342] [G_Loss 6.205529] [D_Loss 0.041646]\n",
      "train [Epoch 0/10] [Batch 343/1173] [Iter 343] [G_Loss 7.971379] [D_Loss 0.123533]\n",
      "train [Epoch 0/10] [Batch 344/1173] [Iter 344] [G_Loss 4.522961] [D_Loss 0.091419]\n",
      "train [Epoch 0/10] [Batch 345/1173] [Iter 345] [G_Loss 8.171365] [D_Loss 0.106579]\n",
      "train [Epoch 0/10] [Batch 346/1173] [Iter 346] [G_Loss 7.768909] [D_Loss 0.008605]\n",
      "train [Epoch 0/10] [Batch 347/1173] [Iter 347] [G_Loss 7.236195] [D_Loss 0.038524]\n",
      "train [Epoch 0/10] [Batch 348/1173] [Iter 348] [G_Loss 5.344797] [D_Loss 0.077868]\n",
      "train [Epoch 0/10] [Batch 349/1173] [Iter 349] [G_Loss 4.077341] [D_Loss 0.106181]\n",
      "train [Epoch 0/10] [Batch 350/1173] [Iter 350] [G_Loss 9.215562] [D_Loss 0.033974]\n",
      "train [Epoch 0/10] [Batch 351/1173] [Iter 351] [G_Loss 9.408903] [D_Loss 0.162736]\n",
      "train [Epoch 0/10] [Batch 352/1173] [Iter 352] [G_Loss 3.772932] [D_Loss 0.189940]\n",
      "train [Epoch 0/10] [Batch 353/1173] [Iter 353] [G_Loss 11.319507] [D_Loss 0.321207]\n",
      "train [Epoch 0/10] [Batch 354/1173] [Iter 354] [G_Loss 10.038542] [D_Loss 0.022305]\n",
      "train [Epoch 0/10] [Batch 355/1173] [Iter 355] [G_Loss 9.216770] [D_Loss 0.097422]\n",
      "train [Epoch 0/10] [Batch 356/1173] [Iter 356] [G_Loss 7.575281] [D_Loss 0.020559]\n",
      "train [Epoch 0/10] [Batch 357/1173] [Iter 357] [G_Loss 6.973928] [D_Loss 0.017925]\n",
      "train [Epoch 0/10] [Batch 358/1173] [Iter 358] [G_Loss 6.300001] [D_Loss 0.040175]\n",
      "train [Epoch 0/10] [Batch 359/1173] [Iter 359] [G_Loss 6.491825] [D_Loss 0.055111]\n",
      "train [Epoch 0/10] [Batch 360/1173] [Iter 360] [G_Loss 6.478708] [D_Loss 0.017640]\n",
      "train [Epoch 0/10] [Batch 361/1173] [Iter 361] [G_Loss 7.669348] [D_Loss 0.013838]\n",
      "train [Epoch 0/10] [Batch 362/1173] [Iter 362] [G_Loss 8.568443] [D_Loss 0.052670]\n",
      "train [Epoch 0/10] [Batch 363/1173] [Iter 363] [G_Loss 6.979662] [D_Loss 0.019911]\n",
      "train [Epoch 0/10] [Batch 364/1173] [Iter 364] [G_Loss 5.885005] [D_Loss 0.070758]\n",
      "train [Epoch 0/10] [Batch 365/1173] [Iter 365] [G_Loss 5.113124] [D_Loss 0.057584]\n",
      "train [Epoch 0/10] [Batch 366/1173] [Iter 366] [G_Loss 10.365827] [D_Loss 0.081513]\n",
      "train [Epoch 0/10] [Batch 367/1173] [Iter 367] [G_Loss 9.742448] [D_Loss 0.106106]\n",
      "train [Epoch 0/10] [Batch 368/1173] [Iter 368] [G_Loss 6.686110] [D_Loss 0.043858]\n",
      "train [Epoch 0/10] [Batch 369/1173] [Iter 369] [G_Loss 5.819212] [D_Loss 0.047406]\n",
      "train [Epoch 0/10] [Batch 370/1173] [Iter 370] [G_Loss 6.388508] [D_Loss 0.032315]\n",
      "train [Epoch 0/10] [Batch 371/1173] [Iter 371] [G_Loss 8.360720] [D_Loss 0.075396]\n",
      "train [Epoch 0/10] [Batch 372/1173] [Iter 372] [G_Loss 8.234663] [D_Loss 0.024256]\n",
      "train [Epoch 0/10] [Batch 373/1173] [Iter 373] [G_Loss 7.046629] [D_Loss 0.023778]\n",
      "train [Epoch 0/10] [Batch 374/1173] [Iter 374] [G_Loss 7.001958] [D_Loss 0.042000]\n",
      "train [Epoch 0/10] [Batch 375/1173] [Iter 375] [G_Loss 7.748214] [D_Loss 0.044857]\n",
      "train [Epoch 0/10] [Batch 376/1173] [Iter 376] [G_Loss 6.596014] [D_Loss 0.037243]\n",
      "train [Epoch 0/10] [Batch 377/1173] [Iter 377] [G_Loss 5.141661] [D_Loss 0.074092]\n",
      "train [Epoch 0/10] [Batch 378/1173] [Iter 378] [G_Loss 11.291339] [D_Loss 0.125896]\n",
      "train [Epoch 0/10] [Batch 379/1173] [Iter 379] [G_Loss 9.269796] [D_Loss 0.035465]\n",
      "train [Epoch 0/10] [Batch 380/1173] [Iter 380] [G_Loss 6.679215] [D_Loss 0.050342]\n",
      "train [Epoch 0/10] [Batch 381/1173] [Iter 381] [G_Loss 7.151797] [D_Loss 0.087673]\n",
      "train [Epoch 0/10] [Batch 382/1173] [Iter 382] [G_Loss 11.339159] [D_Loss 0.189245]\n",
      "train [Epoch 0/10] [Batch 383/1173] [Iter 383] [G_Loss 2.254561] [D_Loss 1.330336]\n",
      "train [Epoch 0/10] [Batch 384/1173] [Iter 384] [G_Loss 22.145260] [D_Loss 7.152514]\n",
      "train [Epoch 0/10] [Batch 385/1173] [Iter 385] [G_Loss 9.417948] [D_Loss 0.310510]\n",
      "train [Epoch 0/10] [Batch 386/1173] [Iter 386] [G_Loss 5.192095] [D_Loss 1.066683]\n",
      "train [Epoch 0/10] [Batch 387/1173] [Iter 387] [G_Loss 5.409936] [D_Loss 0.918496]\n",
      "train [Epoch 0/10] [Batch 388/1173] [Iter 388] [G_Loss 6.511800] [D_Loss 0.337780]\n",
      "train [Epoch 0/10] [Batch 389/1173] [Iter 389] [G_Loss 7.069777] [D_Loss 0.358010]\n",
      "train [Epoch 0/10] [Batch 390/1173] [Iter 390] [G_Loss 6.801248] [D_Loss 0.341931]\n",
      "train [Epoch 0/10] [Batch 391/1173] [Iter 391] [G_Loss 6.166028] [D_Loss 0.198875]\n",
      "train [Epoch 0/10] [Batch 392/1173] [Iter 392] [G_Loss 5.374861] [D_Loss 0.206564]\n",
      "train [Epoch 0/10] [Batch 393/1173] [Iter 393] [G_Loss 5.337062] [D_Loss 0.339593]\n",
      "train [Epoch 0/10] [Batch 394/1173] [Iter 394] [G_Loss 4.310293] [D_Loss 0.216853]\n",
      "train [Epoch 0/10] [Batch 395/1173] [Iter 395] [G_Loss 4.572304] [D_Loss 0.154597]\n",
      "train [Epoch 0/10] [Batch 396/1173] [Iter 396] [G_Loss 4.485250] [D_Loss 0.214632]\n",
      "train [Epoch 0/10] [Batch 397/1173] [Iter 397] [G_Loss 3.724296] [D_Loss 0.350712]\n",
      "train [Epoch 0/10] [Batch 398/1173] [Iter 398] [G_Loss 4.785056] [D_Loss 0.366441]\n",
      "train [Epoch 0/10] [Batch 399/1173] [Iter 399] [G_Loss 4.068624] [D_Loss 0.288248]\n",
      "train [Epoch 0/10] [Batch 400/1173] [Iter 400] [G_Loss 5.010943] [D_Loss 0.283922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 401/1173] [Iter 401] [G_Loss 4.431911] [D_Loss 0.184866]\n",
      "train [Epoch 0/10] [Batch 402/1173] [Iter 402] [G_Loss 4.714238] [D_Loss 0.151776]\n",
      "train [Epoch 0/10] [Batch 403/1173] [Iter 403] [G_Loss 4.462899] [D_Loss 0.101024]\n",
      "train [Epoch 0/10] [Batch 404/1173] [Iter 404] [G_Loss 4.526664] [D_Loss 0.125413]\n",
      "train [Epoch 0/10] [Batch 405/1173] [Iter 405] [G_Loss 4.434633] [D_Loss 0.102941]\n",
      "train [Epoch 0/10] [Batch 406/1173] [Iter 406] [G_Loss 4.804996] [D_Loss 0.068827]\n",
      "train [Epoch 0/10] [Batch 407/1173] [Iter 407] [G_Loss 4.698168] [D_Loss 0.119272]\n",
      "train [Epoch 0/10] [Batch 408/1173] [Iter 408] [G_Loss 4.693053] [D_Loss 0.185635]\n",
      "train [Epoch 0/10] [Batch 409/1173] [Iter 409] [G_Loss 4.766107] [D_Loss 0.067783]\n",
      "train [Epoch 0/10] [Batch 410/1173] [Iter 410] [G_Loss 5.000703] [D_Loss 0.108786]\n",
      "train [Epoch 0/10] [Batch 411/1173] [Iter 411] [G_Loss 4.538275] [D_Loss 0.045660]\n",
      "train [Epoch 0/10] [Batch 412/1173] [Iter 412] [G_Loss 4.790104] [D_Loss 0.126760]\n",
      "train [Epoch 0/10] [Batch 413/1173] [Iter 413] [G_Loss 4.332966] [D_Loss 0.088697]\n",
      "train [Epoch 0/10] [Batch 414/1173] [Iter 414] [G_Loss 4.407137] [D_Loss 0.065870]\n",
      "train [Epoch 0/10] [Batch 415/1173] [Iter 415] [G_Loss 4.814325] [D_Loss 0.034429]\n",
      "train [Epoch 0/10] [Batch 416/1173] [Iter 416] [G_Loss 5.073462] [D_Loss 0.088242]\n",
      "train [Epoch 0/10] [Batch 417/1173] [Iter 417] [G_Loss 3.999001] [D_Loss 0.123153]\n",
      "train [Epoch 0/10] [Batch 418/1173] [Iter 418] [G_Loss 5.156525] [D_Loss 0.173712]\n",
      "train [Epoch 0/10] [Batch 419/1173] [Iter 419] [G_Loss 3.704276] [D_Loss 0.161329]\n",
      "train [Epoch 0/10] [Batch 420/1173] [Iter 420] [G_Loss 5.264580] [D_Loss 0.385979]\n",
      "train [Epoch 0/10] [Batch 421/1173] [Iter 421] [G_Loss 1.479522] [D_Loss 1.280775]\n",
      "train [Epoch 0/10] [Batch 422/1173] [Iter 422] [G_Loss 9.478685] [D_Loss 4.681139]\n",
      "train [Epoch 0/10] [Batch 423/1173] [Iter 423] [G_Loss 6.249105] [D_Loss 0.148658]\n",
      "train [Epoch 0/10] [Batch 424/1173] [Iter 424] [G_Loss 3.331283] [D_Loss 0.319787]\n",
      "train [Epoch 0/10] [Batch 425/1173] [Iter 425] [G_Loss 5.243836] [D_Loss 0.078746]\n",
      "train [Epoch 0/10] [Batch 426/1173] [Iter 426] [G_Loss 6.097316] [D_Loss 0.159770]\n",
      "train [Epoch 0/10] [Batch 427/1173] [Iter 427] [G_Loss 6.085451] [D_Loss 0.076957]\n",
      "train [Epoch 0/10] [Batch 428/1173] [Iter 428] [G_Loss 5.362007] [D_Loss 0.297676]\n",
      "train [Epoch 0/10] [Batch 429/1173] [Iter 429] [G_Loss 4.153491] [D_Loss 0.182143]\n",
      "train [Epoch 0/10] [Batch 430/1173] [Iter 430] [G_Loss 4.974999] [D_Loss 0.144043]\n",
      "train [Epoch 0/10] [Batch 431/1173] [Iter 431] [G_Loss 4.364342] [D_Loss 0.115458]\n",
      "train [Epoch 0/10] [Batch 432/1173] [Iter 432] [G_Loss 3.991685] [D_Loss 0.072632]\n",
      "train [Epoch 0/10] [Batch 433/1173] [Iter 433] [G_Loss 3.704458] [D_Loss 0.088281]\n",
      "train [Epoch 0/10] [Batch 434/1173] [Iter 434] [G_Loss 3.500210] [D_Loss 0.117103]\n",
      "train [Epoch 0/10] [Batch 435/1173] [Iter 435] [G_Loss 3.412207] [D_Loss 0.059834]\n",
      "train [Epoch 0/10] [Batch 436/1173] [Iter 436] [G_Loss 3.773484] [D_Loss 0.069180]\n",
      "train [Epoch 0/10] [Batch 437/1173] [Iter 437] [G_Loss 4.260672] [D_Loss 0.054407]\n",
      "train [Epoch 0/10] [Batch 438/1173] [Iter 438] [G_Loss 4.225552] [D_Loss 0.067405]\n",
      "train [Epoch 0/10] [Batch 439/1173] [Iter 439] [G_Loss 3.980860] [D_Loss 0.053319]\n",
      "train [Epoch 0/10] [Batch 440/1173] [Iter 440] [G_Loss 4.172999] [D_Loss 0.069733]\n",
      "train [Epoch 0/10] [Batch 441/1173] [Iter 441] [G_Loss 4.622528] [D_Loss 0.026542]\n",
      "train [Epoch 0/10] [Batch 442/1173] [Iter 442] [G_Loss 4.359950] [D_Loss 0.050998]\n",
      "train [Epoch 0/10] [Batch 443/1173] [Iter 443] [G_Loss 4.825619] [D_Loss 0.039346]\n",
      "train [Epoch 0/10] [Batch 444/1173] [Iter 444] [G_Loss 5.115527] [D_Loss 0.101176]\n",
      "train [Epoch 0/10] [Batch 445/1173] [Iter 445] [G_Loss 4.742634] [D_Loss 0.112016]\n",
      "train [Epoch 0/10] [Batch 446/1173] [Iter 446] [G_Loss 4.696273] [D_Loss 0.065860]\n",
      "train [Epoch 0/10] [Batch 447/1173] [Iter 447] [G_Loss 4.184486] [D_Loss 0.075710]\n",
      "train [Epoch 0/10] [Batch 448/1173] [Iter 448] [G_Loss 5.158720] [D_Loss 0.085515]\n",
      "train [Epoch 0/10] [Batch 449/1173] [Iter 449] [G_Loss 3.465942] [D_Loss 0.220681]\n",
      "train [Epoch 0/10] [Batch 450/1173] [Iter 450] [G_Loss 8.483070] [D_Loss 0.942007]\n",
      "train [Epoch 0/10] [Batch 451/1173] [Iter 451] [G_Loss 2.183971] [D_Loss 1.820021]\n",
      "train [Epoch 0/10] [Batch 452/1173] [Iter 452] [G_Loss 7.697154] [D_Loss 1.153379]\n",
      "train [Epoch 0/10] [Batch 453/1173] [Iter 453] [G_Loss 6.074123] [D_Loss 0.326703]\n",
      "train [Epoch 0/10] [Batch 454/1173] [Iter 454] [G_Loss 3.738908] [D_Loss 0.119313]\n",
      "train [Epoch 0/10] [Batch 455/1173] [Iter 455] [G_Loss 2.791002] [D_Loss 0.183555]\n",
      "train [Epoch 0/10] [Batch 456/1173] [Iter 456] [G_Loss 3.896162] [D_Loss 0.144622]\n",
      "train [Epoch 0/10] [Batch 457/1173] [Iter 457] [G_Loss 4.772864] [D_Loss 0.059546]\n",
      "train [Epoch 0/10] [Batch 458/1173] [Iter 458] [G_Loss 5.191182] [D_Loss 0.042215]\n",
      "train [Epoch 0/10] [Batch 459/1173] [Iter 459] [G_Loss 5.261355] [D_Loss 0.058743]\n",
      "train [Epoch 0/10] [Batch 460/1173] [Iter 460] [G_Loss 5.142383] [D_Loss 0.053073]\n",
      "train [Epoch 0/10] [Batch 461/1173] [Iter 461] [G_Loss 5.083322] [D_Loss 0.042496]\n",
      "train [Epoch 0/10] [Batch 462/1173] [Iter 462] [G_Loss 4.896710] [D_Loss 0.074483]\n",
      "train [Epoch 0/10] [Batch 463/1173] [Iter 463] [G_Loss 4.818425] [D_Loss 0.076407]\n",
      "train [Epoch 0/10] [Batch 464/1173] [Iter 464] [G_Loss 4.310556] [D_Loss 0.070322]\n",
      "train [Epoch 0/10] [Batch 465/1173] [Iter 465] [G_Loss 4.790581] [D_Loss 0.042263]\n",
      "train [Epoch 0/10] [Batch 466/1173] [Iter 466] [G_Loss 4.918614] [D_Loss 0.074427]\n",
      "train [Epoch 0/10] [Batch 467/1173] [Iter 467] [G_Loss 4.672972] [D_Loss 0.041830]\n",
      "train [Epoch 0/10] [Batch 468/1173] [Iter 468] [G_Loss 4.796256] [D_Loss 0.044987]\n",
      "train [Epoch 0/10] [Batch 469/1173] [Iter 469] [G_Loss 5.012573] [D_Loss 0.111211]\n",
      "train [Epoch 0/10] [Batch 470/1173] [Iter 470] [G_Loss 4.830403] [D_Loss 0.039998]\n",
      "train [Epoch 0/10] [Batch 471/1173] [Iter 471] [G_Loss 4.520292] [D_Loss 0.044820]\n",
      "train [Epoch 0/10] [Batch 472/1173] [Iter 472] [G_Loss 5.073722] [D_Loss 0.121690]\n",
      "train [Epoch 0/10] [Batch 473/1173] [Iter 473] [G_Loss 5.134444] [D_Loss 0.036868]\n",
      "train [Epoch 0/10] [Batch 474/1173] [Iter 474] [G_Loss 5.707844] [D_Loss 0.030659]\n",
      "train [Epoch 0/10] [Batch 475/1173] [Iter 475] [G_Loss 5.610367] [D_Loss 0.027668]\n",
      "train [Epoch 0/10] [Batch 476/1173] [Iter 476] [G_Loss 5.475780] [D_Loss 0.037626]\n",
      "train [Epoch 0/10] [Batch 477/1173] [Iter 477] [G_Loss 5.364496] [D_Loss 0.034636]\n",
      "train [Epoch 0/10] [Batch 478/1173] [Iter 478] [G_Loss 5.282878] [D_Loss 0.050011]\n",
      "train [Epoch 0/10] [Batch 479/1173] [Iter 479] [G_Loss 5.110241] [D_Loss 0.040458]\n",
      "train [Epoch 0/10] [Batch 480/1173] [Iter 480] [G_Loss 5.189803] [D_Loss 0.042565]\n",
      "train [Epoch 0/10] [Batch 481/1173] [Iter 481] [G_Loss 5.373956] [D_Loss 0.027203]\n",
      "train [Epoch 0/10] [Batch 482/1173] [Iter 482] [G_Loss 5.518171] [D_Loss 0.104401]\n",
      "train [Epoch 0/10] [Batch 483/1173] [Iter 483] [G_Loss 4.673907] [D_Loss 0.080438]\n",
      "train [Epoch 0/10] [Batch 484/1173] [Iter 484] [G_Loss 4.600941] [D_Loss 0.111078]\n",
      "train [Epoch 0/10] [Batch 485/1173] [Iter 485] [G_Loss 5.292586] [D_Loss 0.110142]\n",
      "train [Epoch 0/10] [Batch 486/1173] [Iter 486] [G_Loss 5.170856] [D_Loss 0.039067]\n",
      "train [Epoch 0/10] [Batch 487/1173] [Iter 487] [G_Loss 4.846807] [D_Loss 0.035467]\n",
      "train [Epoch 0/10] [Batch 488/1173] [Iter 488] [G_Loss 5.381212] [D_Loss 0.129359]\n",
      "train [Epoch 0/10] [Batch 489/1173] [Iter 489] [G_Loss 5.613173] [D_Loss 0.049109]\n",
      "train [Epoch 0/10] [Batch 490/1173] [Iter 490] [G_Loss 5.283789] [D_Loss 0.046781]\n",
      "train [Epoch 0/10] [Batch 491/1173] [Iter 491] [G_Loss 5.412199] [D_Loss 0.025787]\n",
      "train [Epoch 0/10] [Batch 492/1173] [Iter 492] [G_Loss 5.624826] [D_Loss 0.015496]\n",
      "train [Epoch 0/10] [Batch 493/1173] [Iter 493] [G_Loss 6.037304] [D_Loss 0.016782]\n",
      "train [Epoch 0/10] [Batch 494/1173] [Iter 494] [G_Loss 6.262583] [D_Loss 0.145402]\n",
      "train [Epoch 0/10] [Batch 495/1173] [Iter 495] [G_Loss 5.559501] [D_Loss 0.019811]\n",
      "train [Epoch 0/10] [Batch 496/1173] [Iter 496] [G_Loss 4.986208] [D_Loss 0.023943]\n",
      "train [Epoch 0/10] [Batch 497/1173] [Iter 497] [G_Loss 5.091240] [D_Loss 0.052427]\n",
      "train [Epoch 0/10] [Batch 498/1173] [Iter 498] [G_Loss 5.191199] [D_Loss 0.053428]\n",
      "train [Epoch 0/10] [Batch 499/1173] [Iter 499] [G_Loss 5.369989] [D_Loss 0.024054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 500/1173] [Iter 500] [G_Loss 5.627161] [D_Loss 0.028684]\n",
      "train [Epoch 0/10] [Batch 501/1173] [Iter 501] [G_Loss 5.561035] [D_Loss 0.057582]\n",
      "train [Epoch 0/10] [Batch 502/1173] [Iter 502] [G_Loss 5.652781] [D_Loss 0.062978]\n",
      "train [Epoch 0/10] [Batch 503/1173] [Iter 503] [G_Loss 5.335539] [D_Loss 0.050799]\n",
      "train [Epoch 0/10] [Batch 504/1173] [Iter 504] [G_Loss 4.254573] [D_Loss 0.098439]\n",
      "train [Epoch 0/10] [Batch 505/1173] [Iter 505] [G_Loss 4.020305] [D_Loss 0.241347]\n",
      "train [Epoch 0/10] [Batch 506/1173] [Iter 506] [G_Loss 4.289789] [D_Loss 0.286270]\n",
      "train [Epoch 0/10] [Batch 507/1173] [Iter 507] [G_Loss 8.119051] [D_Loss 0.359231]\n",
      "train [Epoch 0/10] [Batch 508/1173] [Iter 508] [G_Loss 6.055717] [D_Loss 0.032122]\n",
      "train [Epoch 0/10] [Batch 509/1173] [Iter 509] [G_Loss 3.131901] [D_Loss 0.109895]\n",
      "train [Epoch 0/10] [Batch 510/1173] [Iter 510] [G_Loss 6.086740] [D_Loss 0.062333]\n",
      "train [Epoch 0/10] [Batch 511/1173] [Iter 511] [G_Loss 6.810036] [D_Loss 0.072846]\n",
      "train [Epoch 0/10] [Batch 512/1173] [Iter 512] [G_Loss 6.312502] [D_Loss 0.062347]\n",
      "train [Epoch 0/10] [Batch 513/1173] [Iter 513] [G_Loss 4.817284] [D_Loss 0.070623]\n",
      "train [Epoch 0/10] [Batch 514/1173] [Iter 514] [G_Loss 4.892557] [D_Loss 0.056109]\n",
      "train [Epoch 0/10] [Batch 515/1173] [Iter 515] [G_Loss 5.362070] [D_Loss 0.021478]\n",
      "train [Epoch 0/10] [Batch 516/1173] [Iter 516] [G_Loss 6.136837] [D_Loss 0.026728]\n",
      "train [Epoch 0/10] [Batch 517/1173] [Iter 517] [G_Loss 6.257101] [D_Loss 0.048835]\n",
      "train [Epoch 0/10] [Batch 518/1173] [Iter 518] [G_Loss 6.446936] [D_Loss 0.022431]\n",
      "train [Epoch 0/10] [Batch 519/1173] [Iter 519] [G_Loss 6.331585] [D_Loss 0.035261]\n",
      "train [Epoch 0/10] [Batch 520/1173] [Iter 520] [G_Loss 4.879702] [D_Loss 0.080347]\n",
      "train [Epoch 0/10] [Batch 521/1173] [Iter 521] [G_Loss 7.061606] [D_Loss 0.036321]\n",
      "train [Epoch 0/10] [Batch 522/1173] [Iter 522] [G_Loss 7.171215] [D_Loss 0.135207]\n",
      "train [Epoch 0/10] [Batch 523/1173] [Iter 523] [G_Loss 4.353421] [D_Loss 0.085598]\n",
      "train [Epoch 0/10] [Batch 524/1173] [Iter 524] [G_Loss 8.651211] [D_Loss 0.041818]\n",
      "train [Epoch 0/10] [Batch 525/1173] [Iter 525] [G_Loss 8.769061] [D_Loss 0.144374]\n",
      "train [Epoch 0/10] [Batch 526/1173] [Iter 526] [G_Loss 4.378579] [D_Loss 0.086276]\n",
      "train [Epoch 0/10] [Batch 527/1173] [Iter 527] [G_Loss 6.886475] [D_Loss 0.023781]\n",
      "train [Epoch 0/10] [Batch 528/1173] [Iter 528] [G_Loss 7.239022] [D_Loss 0.039200]\n",
      "train [Epoch 0/10] [Batch 529/1173] [Iter 529] [G_Loss 7.281711] [D_Loss 0.034839]\n",
      "train [Epoch 0/10] [Batch 530/1173] [Iter 530] [G_Loss 5.348217] [D_Loss 0.031342]\n",
      "train [Epoch 0/10] [Batch 531/1173] [Iter 531] [G_Loss 6.397540] [D_Loss 0.061949]\n",
      "train [Epoch 0/10] [Batch 532/1173] [Iter 532] [G_Loss 6.823280] [D_Loss 0.067949]\n",
      "train [Epoch 0/10] [Batch 533/1173] [Iter 533] [G_Loss 6.063841] [D_Loss 0.032714]\n",
      "train [Epoch 0/10] [Batch 534/1173] [Iter 534] [G_Loss 5.065392] [D_Loss 0.038013]\n",
      "train [Epoch 0/10] [Batch 535/1173] [Iter 535] [G_Loss 6.994352] [D_Loss 0.016799]\n",
      "train [Epoch 0/10] [Batch 536/1173] [Iter 536] [G_Loss 7.015663] [D_Loss 0.104325]\n",
      "train [Epoch 0/10] [Batch 537/1173] [Iter 537] [G_Loss 6.088097] [D_Loss 0.029994]\n",
      "train [Epoch 0/10] [Batch 538/1173] [Iter 538] [G_Loss 4.015913] [D_Loss 0.092089]\n",
      "train [Epoch 0/10] [Batch 539/1173] [Iter 539] [G_Loss 8.484270] [D_Loss 0.086901]\n",
      "train [Epoch 0/10] [Batch 540/1173] [Iter 540] [G_Loss 8.345803] [D_Loss 0.213168]\n",
      "train [Epoch 0/10] [Batch 541/1173] [Iter 541] [G_Loss 6.377578] [D_Loss 0.125715]\n",
      "train [Epoch 0/10] [Batch 542/1173] [Iter 542] [G_Loss 3.877507] [D_Loss 0.080485]\n",
      "train [Epoch 0/10] [Batch 543/1173] [Iter 543] [G_Loss 6.265242] [D_Loss 0.017252]\n",
      "train [Epoch 0/10] [Batch 544/1173] [Iter 544] [G_Loss 6.566428] [D_Loss 0.033459]\n",
      "train [Epoch 0/10] [Batch 545/1173] [Iter 545] [G_Loss 6.439001] [D_Loss 0.027785]\n",
      "train [Epoch 0/10] [Batch 546/1173] [Iter 546] [G_Loss 5.601633] [D_Loss 0.012847]\n",
      "train [Epoch 0/10] [Batch 547/1173] [Iter 547] [G_Loss 4.980395] [D_Loss 0.077715]\n",
      "train [Epoch 0/10] [Batch 548/1173] [Iter 548] [G_Loss 4.901198] [D_Loss 0.044883]\n",
      "train [Epoch 0/10] [Batch 549/1173] [Iter 549] [G_Loss 5.385035] [D_Loss 0.106434]\n",
      "train [Epoch 0/10] [Batch 550/1173] [Iter 550] [G_Loss 4.383127] [D_Loss 0.035504]\n",
      "train [Epoch 0/10] [Batch 551/1173] [Iter 551] [G_Loss 5.067141] [D_Loss 0.034255]\n",
      "train [Epoch 0/10] [Batch 552/1173] [Iter 552] [G_Loss 6.048120] [D_Loss 0.022685]\n",
      "train [Epoch 0/10] [Batch 553/1173] [Iter 553] [G_Loss 6.366318] [D_Loss 0.014734]\n",
      "train [Epoch 0/10] [Batch 554/1173] [Iter 554] [G_Loss 6.021845] [D_Loss 0.014225]\n",
      "train [Epoch 0/10] [Batch 555/1173] [Iter 555] [G_Loss 5.990354] [D_Loss 0.010064]\n",
      "train [Epoch 0/10] [Batch 556/1173] [Iter 556] [G_Loss 5.864744] [D_Loss 0.013969]\n",
      "train [Epoch 0/10] [Batch 557/1173] [Iter 557] [G_Loss 6.004322] [D_Loss 0.012608]\n",
      "train [Epoch 0/10] [Batch 558/1173] [Iter 558] [G_Loss 5.898090] [D_Loss 0.048432]\n",
      "train [Epoch 0/10] [Batch 559/1173] [Iter 559] [G_Loss 4.822371] [D_Loss 0.031964]\n",
      "train [Epoch 0/10] [Batch 560/1173] [Iter 560] [G_Loss 5.598937] [D_Loss 0.010147]\n",
      "train [Epoch 0/10] [Batch 561/1173] [Iter 561] [G_Loss 6.397833] [D_Loss 0.011125]\n",
      "train [Epoch 0/10] [Batch 562/1173] [Iter 562] [G_Loss 6.532303] [D_Loss 0.063570]\n",
      "train [Epoch 0/10] [Batch 563/1173] [Iter 563] [G_Loss 5.469934] [D_Loss 0.012912]\n",
      "train [Epoch 0/10] [Batch 564/1173] [Iter 564] [G_Loss 5.199903] [D_Loss 0.040372]\n",
      "train [Epoch 0/10] [Batch 565/1173] [Iter 565] [G_Loss 5.513010] [D_Loss 0.034515]\n",
      "train [Epoch 0/10] [Batch 566/1173] [Iter 566] [G_Loss 5.466318] [D_Loss 0.048548]\n",
      "train [Epoch 0/10] [Batch 567/1173] [Iter 567] [G_Loss 4.873473] [D_Loss 0.034510]\n",
      "train [Epoch 0/10] [Batch 568/1173] [Iter 568] [G_Loss 5.220398] [D_Loss 0.029096]\n",
      "train [Epoch 0/10] [Batch 569/1173] [Iter 569] [G_Loss 6.158714] [D_Loss 0.008446]\n",
      "train [Epoch 0/10] [Batch 570/1173] [Iter 570] [G_Loss 6.624197] [D_Loss 0.015063]\n",
      "train [Epoch 0/10] [Batch 571/1173] [Iter 571] [G_Loss 6.473684] [D_Loss 0.026229]\n",
      "train [Epoch 0/10] [Batch 572/1173] [Iter 572] [G_Loss 6.373043] [D_Loss 0.066957]\n",
      "train [Epoch 0/10] [Batch 573/1173] [Iter 573] [G_Loss 6.002687] [D_Loss 0.010233]\n",
      "train [Epoch 0/10] [Batch 574/1173] [Iter 574] [G_Loss 5.016258] [D_Loss 0.023070]\n",
      "train [Epoch 0/10] [Batch 575/1173] [Iter 575] [G_Loss 5.233765] [D_Loss 0.014567]\n",
      "train [Epoch 0/10] [Batch 576/1173] [Iter 576] [G_Loss 6.494287] [D_Loss 0.021629]\n",
      "train [Epoch 0/10] [Batch 577/1173] [Iter 577] [G_Loss 6.709716] [D_Loss 0.037436]\n",
      "train [Epoch 0/10] [Batch 578/1173] [Iter 578] [G_Loss 6.334695] [D_Loss 0.009866]\n",
      "train [Epoch 0/10] [Batch 579/1173] [Iter 579] [G_Loss 6.421082] [D_Loss 0.009164]\n",
      "train [Epoch 0/10] [Batch 580/1173] [Iter 580] [G_Loss 6.208592] [D_Loss 0.005609]\n",
      "train [Epoch 0/10] [Batch 581/1173] [Iter 581] [G_Loss 6.151875] [D_Loss 0.014261]\n",
      "train [Epoch 0/10] [Batch 582/1173] [Iter 582] [G_Loss 5.682081] [D_Loss 0.011438]\n",
      "train [Epoch 0/10] [Batch 583/1173] [Iter 583] [G_Loss 6.319845] [D_Loss 0.012467]\n",
      "train [Epoch 0/10] [Batch 584/1173] [Iter 584] [G_Loss 6.589262] [D_Loss 0.010608]\n",
      "train [Epoch 0/10] [Batch 585/1173] [Iter 585] [G_Loss 5.963007] [D_Loss 0.004742]\n",
      "train [Epoch 0/10] [Batch 586/1173] [Iter 586] [G_Loss 6.443772] [D_Loss 0.014115]\n",
      "train [Epoch 0/10] [Batch 587/1173] [Iter 587] [G_Loss 6.480186] [D_Loss 0.017030]\n",
      "train [Epoch 0/10] [Batch 591/1173] [Iter 591] [G_Loss 5.662761] [D_Loss 0.014370]\n",
      "train [Epoch 0/10] [Batch 592/1173] [Iter 592] [G_Loss 6.045365] [D_Loss 0.007100]\n",
      "train [Epoch 0/10] [Batch 593/1173] [Iter 593] [G_Loss 6.216703] [D_Loss 0.115260]\n",
      "train [Epoch 0/10] [Batch 594/1173] [Iter 594] [G_Loss 4.207660] [D_Loss 0.047680]\n",
      "train [Epoch 0/10] [Batch 595/1173] [Iter 595] [G_Loss 6.246358] [D_Loss 0.007487]\n",
      "train [Epoch 0/10] [Batch 596/1173] [Iter 596] [G_Loss 7.710866] [D_Loss 0.091125]\n",
      "train [Epoch 0/10] [Batch 597/1173] [Iter 597] [G_Loss 6.639529] [D_Loss 0.118264]\n",
      "train [Epoch 0/10] [Batch 598/1173] [Iter 598] [G_Loss 3.570606] [D_Loss 0.118970]\n",
      "train [Epoch 0/10] [Batch 599/1173] [Iter 599] [G_Loss 11.277252] [D_Loss 0.050869]\n",
      "train [Epoch 0/10] [Batch 600/1173] [Iter 600] [G_Loss 12.478925] [D_Loss 0.181831]\n",
      "train [Epoch 0/10] [Batch 601/1173] [Iter 601] [G_Loss 9.295721] [D_Loss 0.054271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 602/1173] [Iter 602] [G_Loss 6.723105] [D_Loss 0.003369]\n",
      "train [Epoch 0/10] [Batch 603/1173] [Iter 603] [G_Loss 6.152917] [D_Loss 0.007940]\n",
      "train [Epoch 0/10] [Batch 604/1173] [Iter 604] [G_Loss 5.560339] [D_Loss 0.019515]\n",
      "train [Epoch 0/10] [Batch 605/1173] [Iter 605] [G_Loss 6.595509] [D_Loss 0.005873]\n",
      "train [Epoch 0/10] [Batch 606/1173] [Iter 606] [G_Loss 6.585356] [D_Loss 0.009348]\n",
      "train [Epoch 0/10] [Batch 607/1173] [Iter 607] [G_Loss 7.736100] [D_Loss 0.024549]\n",
      "train [Epoch 0/10] [Batch 608/1173] [Iter 608] [G_Loss 6.448489] [D_Loss 0.011093]\n",
      "train [Epoch 0/10] [Batch 609/1173] [Iter 609] [G_Loss 6.420691] [D_Loss 0.010438]\n",
      "train [Epoch 0/10] [Batch 610/1173] [Iter 610] [G_Loss 7.210567] [D_Loss 0.021283]\n",
      "train [Epoch 0/10] [Batch 611/1173] [Iter 611] [G_Loss 7.200096] [D_Loss 0.004602]\n",
      "train [Epoch 0/10] [Batch 612/1173] [Iter 612] [G_Loss 7.342530] [D_Loss 0.025826]\n",
      "train [Epoch 0/10] [Batch 613/1173] [Iter 613] [G_Loss 6.790118] [D_Loss 0.010499]\n",
      "train [Epoch 0/10] [Batch 614/1173] [Iter 614] [G_Loss 6.583258] [D_Loss 0.011926]\n",
      "train [Epoch 0/10] [Batch 615/1173] [Iter 615] [G_Loss 5.825882] [D_Loss 0.015479]\n",
      "train [Epoch 0/10] [Batch 616/1173] [Iter 616] [G_Loss 8.971534] [D_Loss 0.047629]\n",
      "train [Epoch 0/10] [Batch 617/1173] [Iter 617] [G_Loss 6.580445] [D_Loss 0.077671]\n",
      "train [Epoch 0/10] [Batch 618/1173] [Iter 618] [G_Loss 11.143734] [D_Loss 0.001522]\n",
      "train [Epoch 0/10] [Batch 619/1173] [Iter 619] [G_Loss 12.267022] [D_Loss 0.124097]\n",
      "train [Epoch 0/10] [Batch 620/1173] [Iter 620] [G_Loss 9.224005] [D_Loss 0.048142]\n",
      "train [Epoch 0/10] [Batch 621/1173] [Iter 621] [G_Loss 5.529297] [D_Loss 0.016512]\n",
      "train [Epoch 0/10] [Batch 622/1173] [Iter 622] [G_Loss 5.028962] [D_Loss 0.066432]\n",
      "train [Epoch 0/10] [Batch 623/1173] [Iter 623] [G_Loss 10.224832] [D_Loss 0.010195]\n",
      "train [Epoch 0/10] [Batch 624/1173] [Iter 624] [G_Loss 11.863361] [D_Loss 0.000808]\n",
      "train [Epoch 0/10] [Batch 625/1173] [Iter 625] [G_Loss 11.774693] [D_Loss 0.004428]\n",
      "train [Epoch 0/10] [Batch 626/1173] [Iter 626] [G_Loss 11.291373] [D_Loss 0.022912]\n",
      "train [Epoch 0/10] [Batch 627/1173] [Iter 627] [G_Loss 10.934055] [D_Loss 0.002456]\n",
      "train [Epoch 0/10] [Batch 628/1173] [Iter 628] [G_Loss 11.454714] [D_Loss 0.010706]\n",
      "train [Epoch 0/10] [Batch 629/1173] [Iter 629] [G_Loss 10.545306] [D_Loss 0.024565]\n",
      "train [Epoch 0/10] [Batch 630/1173] [Iter 630] [G_Loss 6.397577] [D_Loss 0.019281]\n",
      "train [Epoch 0/10] [Batch 631/1173] [Iter 631] [G_Loss 4.392896] [D_Loss 0.063165]\n",
      "train [Epoch 0/10] [Batch 632/1173] [Iter 632] [G_Loss 13.763321] [D_Loss 0.135007]\n",
      "train [Epoch 0/10] [Batch 633/1173] [Iter 633] [G_Loss 13.278984] [D_Loss 0.118884]\n",
      "train [Epoch 0/10] [Batch 634/1173] [Iter 634] [G_Loss 9.759994] [D_Loss 0.005642]\n",
      "train [Epoch 0/10] [Batch 635/1173] [Iter 635] [G_Loss 7.243546] [D_Loss 0.015447]\n",
      "train [Epoch 0/10] [Batch 636/1173] [Iter 636] [G_Loss 5.991502] [D_Loss 0.025562]\n",
      "train [Epoch 0/10] [Batch 637/1173] [Iter 637] [G_Loss 7.545091] [D_Loss 0.007051]\n",
      "train [Epoch 0/10] [Batch 638/1173] [Iter 638] [G_Loss 6.503456] [D_Loss 0.005396]\n",
      "train [Epoch 0/10] [Batch 639/1173] [Iter 639] [G_Loss 5.425934] [D_Loss 0.019055]\n",
      "train [Epoch 0/10] [Batch 640/1173] [Iter 640] [G_Loss 9.462745] [D_Loss 0.000603]\n",
      "train [Epoch 0/10] [Batch 641/1173] [Iter 641] [G_Loss 9.881295] [D_Loss 0.011128]\n",
      "train [Epoch 0/10] [Batch 642/1173] [Iter 642] [G_Loss 10.792216] [D_Loss 0.000876]\n",
      "train [Epoch 0/10] [Batch 643/1173] [Iter 643] [G_Loss 10.829635] [D_Loss 0.001637]\n",
      "train [Epoch 0/10] [Batch 644/1173] [Iter 644] [G_Loss 10.419617] [D_Loss 0.000412]\n",
      "train [Epoch 0/10] [Batch 645/1173] [Iter 645] [G_Loss 9.793847] [D_Loss 0.046699]\n",
      "train [Epoch 0/10] [Batch 646/1173] [Iter 646] [G_Loss 8.611902] [D_Loss 0.000890]\n",
      "train [Epoch 0/10] [Batch 647/1173] [Iter 647] [G_Loss 7.325404] [D_Loss 0.004108]\n",
      "train [Epoch 0/10] [Batch 648/1173] [Iter 648] [G_Loss 7.253625] [D_Loss 0.003449]\n",
      "train [Epoch 0/10] [Batch 649/1173] [Iter 649] [G_Loss 9.830339] [D_Loss 0.011526]\n",
      "train [Epoch 0/10] [Batch 650/1173] [Iter 650] [G_Loss 6.287912] [D_Loss 0.017106]\n",
      "train [Epoch 0/10] [Batch 651/1173] [Iter 651] [G_Loss 7.555943] [D_Loss 0.005559]\n",
      "train [Epoch 0/10] [Batch 652/1173] [Iter 652] [G_Loss 7.165847] [D_Loss 0.004935]\n",
      "train [Epoch 0/10] [Batch 653/1173] [Iter 653] [G_Loss 7.574601] [D_Loss 0.055402]\n",
      "train [Epoch 0/10] [Batch 654/1173] [Iter 654] [G_Loss 4.796290] [D_Loss 0.103223]\n",
      "train [Epoch 0/10] [Batch 655/1173] [Iter 655] [G_Loss 18.270498] [D_Loss 0.434548]\n",
      "train [Epoch 0/10] [Batch 656/1173] [Iter 656] [G_Loss 15.181562] [D_Loss 0.280327]\n",
      "train [Epoch 0/10] [Batch 657/1173] [Iter 657] [G_Loss 9.605698] [D_Loss 0.077419]\n",
      "train [Epoch 0/10] [Batch 658/1173] [Iter 658] [G_Loss 6.658019] [D_Loss 0.025911]\n",
      "train [Epoch 0/10] [Batch 659/1173] [Iter 659] [G_Loss 4.445661] [D_Loss 0.027011]\n",
      "train [Epoch 0/10] [Batch 660/1173] [Iter 660] [G_Loss 5.311755] [D_Loss 0.012134]\n",
      "train [Epoch 0/10] [Batch 661/1173] [Iter 661] [G_Loss 6.137887] [D_Loss 0.004849]\n",
      "train [Epoch 0/10] [Batch 662/1173] [Iter 662] [G_Loss 7.181037] [D_Loss 0.008391]\n",
      "train [Epoch 0/10] [Batch 663/1173] [Iter 663] [G_Loss 7.391731] [D_Loss 0.001780]\n",
      "train [Epoch 0/10] [Batch 664/1173] [Iter 664] [G_Loss 7.920122] [D_Loss 0.003038]\n",
      "train [Epoch 0/10] [Batch 665/1173] [Iter 665] [G_Loss 7.470789] [D_Loss 0.005468]\n",
      "train [Epoch 0/10] [Batch 666/1173] [Iter 666] [G_Loss 7.267076] [D_Loss 0.168889]\n",
      "train [Epoch 0/10] [Batch 667/1173] [Iter 667] [G_Loss 4.159819] [D_Loss 0.040994]\n",
      "train [Epoch 0/10] [Batch 668/1173] [Iter 668] [G_Loss 6.572489] [D_Loss 0.015875]\n",
      "train [Epoch 0/10] [Batch 669/1173] [Iter 669] [G_Loss 7.585090] [D_Loss 0.003565]\n",
      "train [Epoch 0/10] [Batch 670/1173] [Iter 670] [G_Loss 6.399024] [D_Loss 0.003512]\n",
      "train [Epoch 0/10] [Batch 671/1173] [Iter 671] [G_Loss 6.934864] [D_Loss 0.003779]\n",
      "train [Epoch 0/10] [Batch 672/1173] [Iter 672] [G_Loss 6.870616] [D_Loss 0.045462]\n",
      "train [Epoch 0/10] [Batch 673/1173] [Iter 673] [G_Loss 6.194282] [D_Loss 0.005847]\n",
      "train [Epoch 0/10] [Batch 674/1173] [Iter 674] [G_Loss 6.755943] [D_Loss 0.067842]\n",
      "train [Epoch 0/10] [Batch 675/1173] [Iter 675] [G_Loss 4.075414] [D_Loss 0.041286]\n",
      "train [Epoch 0/10] [Batch 676/1173] [Iter 676] [G_Loss 7.504794] [D_Loss 0.078441]\n",
      "train [Epoch 0/10] [Batch 677/1173] [Iter 677] [G_Loss 7.875986] [D_Loss 0.041508]\n",
      "train [Epoch 0/10] [Batch 678/1173] [Iter 678] [G_Loss 6.263638] [D_Loss 0.004776]\n",
      "train [Epoch 0/10] [Batch 679/1173] [Iter 679] [G_Loss 5.748793] [D_Loss 0.010894]\n",
      "train [Epoch 0/10] [Batch 680/1173] [Iter 680] [G_Loss 6.686441] [D_Loss 0.004620]\n",
      "train [Epoch 0/10] [Batch 681/1173] [Iter 681] [G_Loss 8.013762] [D_Loss 0.001418]\n",
      "train [Epoch 0/10] [Batch 682/1173] [Iter 682] [G_Loss 8.110904] [D_Loss 0.001664]\n",
      "train [Epoch 0/10] [Batch 683/1173] [Iter 683] [G_Loss 8.622484] [D_Loss 0.029589]\n",
      "train [Epoch 0/10] [Batch 684/1173] [Iter 684] [G_Loss 6.618734] [D_Loss 0.020933]\n",
      "train [Epoch 0/10] [Batch 685/1173] [Iter 685] [G_Loss 7.954324] [D_Loss 0.017036]\n",
      "train [Epoch 0/10] [Batch 686/1173] [Iter 686] [G_Loss 6.853783] [D_Loss 0.004159]\n",
      "train [Epoch 0/10] [Batch 687/1173] [Iter 687] [G_Loss 7.181062] [D_Loss 0.006869]\n",
      "train [Epoch 0/10] [Batch 688/1173] [Iter 688] [G_Loss 7.149271] [D_Loss 0.004811]\n",
      "train [Epoch 0/10] [Batch 689/1173] [Iter 689] [G_Loss 7.787025] [D_Loss 0.002916]\n",
      "train [Epoch 0/10] [Batch 690/1173] [Iter 690] [G_Loss 7.960167] [D_Loss 0.052406]\n",
      "train [Epoch 0/10] [Batch 691/1173] [Iter 691] [G_Loss 6.180161] [D_Loss 0.028364]\n",
      "train [Epoch 0/10] [Batch 692/1173] [Iter 692] [G_Loss 6.957699] [D_Loss 0.015813]\n",
      "train [Epoch 0/10] [Batch 693/1173] [Iter 693] [G_Loss 6.755096] [D_Loss 0.017790]\n",
      "train [Epoch 0/10] [Batch 694/1173] [Iter 694] [G_Loss 8.075586] [D_Loss 0.006417]\n",
      "train [Epoch 0/10] [Batch 695/1173] [Iter 695] [G_Loss 8.994300] [D_Loss 0.004798]\n",
      "train [Epoch 0/10] [Batch 696/1173] [Iter 696] [G_Loss 9.911924] [D_Loss 0.001834]\n",
      "train [Epoch 0/10] [Batch 697/1173] [Iter 697] [G_Loss 10.073022] [D_Loss 0.047444]\n",
      "train [Epoch 0/10] [Batch 698/1173] [Iter 698] [G_Loss 7.966167] [D_Loss 0.075393]\n",
      "train [Epoch 0/10] [Batch 699/1173] [Iter 699] [G_Loss 5.832085] [D_Loss 0.050284]\n",
      "train [Epoch 0/10] [Batch 700/1173] [Iter 700] [G_Loss 9.661720] [D_Loss 0.010552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 701/1173] [Iter 701] [G_Loss 10.404201] [D_Loss 0.002342]\n",
      "train [Epoch 0/10] [Batch 702/1173] [Iter 702] [G_Loss 10.977517] [D_Loss 0.000852]\n",
      "train [Epoch 0/10] [Batch 703/1173] [Iter 703] [G_Loss 10.586782] [D_Loss 0.017514]\n",
      "train [Epoch 0/10] [Batch 704/1173] [Iter 704] [G_Loss 9.473842] [D_Loss 0.022955]\n",
      "train [Epoch 0/10] [Batch 705/1173] [Iter 705] [G_Loss 7.459170] [D_Loss 0.019317]\n",
      "train [Epoch 0/10] [Batch 706/1173] [Iter 706] [G_Loss 8.168605] [D_Loss 0.029464]\n",
      "train [Epoch 0/10] [Batch 707/1173] [Iter 707] [G_Loss 8.290555] [D_Loss 0.006897]\n",
      "train [Epoch 0/10] [Batch 708/1173] [Iter 708] [G_Loss 7.173402] [D_Loss 0.030355]\n",
      "train [Epoch 0/10] [Batch 709/1173] [Iter 709] [G_Loss 11.552355] [D_Loss 0.010059]\n",
      "train [Epoch 0/10] [Batch 710/1173] [Iter 710] [G_Loss 12.137691] [D_Loss 0.039948]\n",
      "train [Epoch 0/10] [Batch 711/1173] [Iter 711] [G_Loss 11.023128] [D_Loss 0.008389]\n",
      "train [Epoch 0/10] [Batch 712/1173] [Iter 712] [G_Loss 9.765117] [D_Loss 0.003173]\n",
      "train [Epoch 0/10] [Batch 713/1173] [Iter 713] [G_Loss 9.401491] [D_Loss 0.017363]\n",
      "train [Epoch 0/10] [Batch 714/1173] [Iter 714] [G_Loss 6.122370] [D_Loss 0.061920]\n",
      "train [Epoch 0/10] [Batch 715/1173] [Iter 715] [G_Loss 15.648603] [D_Loss 0.084332]\n",
      "train [Epoch 0/10] [Batch 716/1173] [Iter 716] [G_Loss 15.493426] [D_Loss 0.056679]\n",
      "train [Epoch 0/10] [Batch 717/1173] [Iter 717] [G_Loss 13.633491] [D_Loss 0.044437]\n",
      "train [Epoch 0/10] [Batch 718/1173] [Iter 718] [G_Loss 11.903486] [D_Loss 0.001733]\n",
      "train [Epoch 0/10] [Batch 719/1173] [Iter 719] [G_Loss 10.696742] [D_Loss 0.002738]\n",
      "train [Epoch 0/10] [Batch 720/1173] [Iter 720] [G_Loss 8.133884] [D_Loss 0.007007]\n",
      "train [Epoch 0/10] [Batch 721/1173] [Iter 721] [G_Loss 6.805416] [D_Loss 0.032009]\n",
      "train [Epoch 0/10] [Batch 722/1173] [Iter 722] [G_Loss 13.630371] [D_Loss 0.165367]\n",
      "train [Epoch 0/10] [Batch 723/1173] [Iter 723] [G_Loss 13.676595] [D_Loss 0.092159]\n",
      "train [Epoch 0/10] [Batch 724/1173] [Iter 724] [G_Loss 8.749289] [D_Loss 0.141116]\n",
      "train [Epoch 0/10] [Batch 725/1173] [Iter 725] [G_Loss 4.754681] [D_Loss 0.133647]\n",
      "train [Epoch 0/10] [Batch 726/1173] [Iter 726] [G_Loss 12.460245] [D_Loss 0.022036]\n",
      "train [Epoch 0/10] [Batch 727/1173] [Iter 727] [G_Loss 12.873703] [D_Loss 0.000222]\n",
      "train [Epoch 0/10] [Batch 728/1173] [Iter 728] [G_Loss 11.142144] [D_Loss 0.000815]\n",
      "train [Epoch 0/10] [Batch 729/1173] [Iter 729] [G_Loss 10.562709] [D_Loss 0.000159]\n",
      "train [Epoch 0/10] [Batch 730/1173] [Iter 730] [G_Loss 8.151819] [D_Loss 0.010069]\n",
      "train [Epoch 0/10] [Batch 731/1173] [Iter 731] [G_Loss 8.089199] [D_Loss 0.063733]\n",
      "train [Epoch 0/10] [Batch 732/1173] [Iter 732] [G_Loss 8.803389] [D_Loss 0.071425]\n",
      "train [Epoch 0/10] [Batch 733/1173] [Iter 733] [G_Loss 8.452659] [D_Loss 0.044381]\n",
      "train [Epoch 0/10] [Batch 734/1173] [Iter 734] [G_Loss 7.247767] [D_Loss 0.014584]\n",
      "train [Epoch 0/10] [Batch 735/1173] [Iter 735] [G_Loss 8.699389] [D_Loss 0.001883]\n",
      "train [Epoch 0/10] [Batch 736/1173] [Iter 736] [G_Loss 9.121411] [D_Loss 0.001440]\n",
      "train [Epoch 0/10] [Batch 737/1173] [Iter 737] [G_Loss 9.704757] [D_Loss 0.004613]\n",
      "train [Epoch 0/10] [Batch 738/1173] [Iter 738] [G_Loss 8.958810] [D_Loss 0.020181]\n",
      "train [Epoch 0/10] [Batch 739/1173] [Iter 739] [G_Loss 8.648255] [D_Loss 0.016964]\n",
      "train [Epoch 0/10] [Batch 740/1173] [Iter 740] [G_Loss 7.660429] [D_Loss 0.053083]\n",
      "train [Epoch 0/10] [Batch 741/1173] [Iter 741] [G_Loss 6.568905] [D_Loss 0.019665]\n",
      "train [Epoch 0/10] [Batch 742/1173] [Iter 742] [G_Loss 8.402991] [D_Loss 0.001554]\n",
      "train [Epoch 0/10] [Batch 743/1173] [Iter 743] [G_Loss 9.175238] [D_Loss 0.000909]\n",
      "train [Epoch 0/10] [Batch 744/1173] [Iter 744] [G_Loss 9.000722] [D_Loss 0.001217]\n",
      "train [Epoch 0/10] [Batch 745/1173] [Iter 745] [G_Loss 9.908518] [D_Loss 0.053927]\n",
      "train [Epoch 0/10] [Batch 746/1173] [Iter 746] [G_Loss 7.725759] [D_Loss 0.002852]\n",
      "train [Epoch 0/10] [Batch 747/1173] [Iter 747] [G_Loss 7.966799] [D_Loss 0.001868]\n",
      "train [Epoch 0/10] [Batch 748/1173] [Iter 748] [G_Loss 7.594377] [D_Loss 0.003437]\n",
      "train [Epoch 0/10] [Batch 749/1173] [Iter 749] [G_Loss 8.242542] [D_Loss 0.020415]\n",
      "train [Epoch 0/10] [Batch 750/1173] [Iter 750] [G_Loss 7.144124] [D_Loss 0.003608]\n",
      "train [Epoch 0/10] [Batch 751/1173] [Iter 751] [G_Loss 7.430737] [D_Loss 0.011743]\n",
      "train [Epoch 0/10] [Batch 752/1173] [Iter 752] [G_Loss 7.351327] [D_Loss 0.002467]\n",
      "train [Epoch 0/10] [Batch 753/1173] [Iter 753] [G_Loss 7.292800] [D_Loss 0.005171]\n",
      "train [Epoch 0/10] [Batch 754/1173] [Iter 754] [G_Loss 8.574681] [D_Loss 0.002810]\n",
      "train [Epoch 0/10] [Batch 755/1173] [Iter 755] [G_Loss 8.751526] [D_Loss 0.015191]\n",
      "train [Epoch 0/10] [Batch 756/1173] [Iter 756] [G_Loss 9.559854] [D_Loss 0.003577]\n",
      "train [Epoch 0/10] [Batch 757/1173] [Iter 757] [G_Loss 8.511991] [D_Loss 0.002700]\n",
      "train [Epoch 0/10] [Batch 758/1173] [Iter 758] [G_Loss 8.584073] [D_Loss 0.063447]\n",
      "train [Epoch 0/10] [Batch 759/1173] [Iter 759] [G_Loss 7.051559] [D_Loss 0.131258]\n",
      "train [Epoch 0/10] [Batch 760/1173] [Iter 760] [G_Loss 5.090859] [D_Loss 0.043192]\n",
      "train [Epoch 0/10] [Batch 761/1173] [Iter 761] [G_Loss 9.146336] [D_Loss 0.001151]\n",
      "train [Epoch 0/10] [Batch 762/1173] [Iter 762] [G_Loss 10.461511] [D_Loss 0.010739]\n",
      "train [Epoch 0/10] [Batch 763/1173] [Iter 763] [G_Loss 10.622683] [D_Loss 0.002774]\n",
      "train [Epoch 0/10] [Batch 764/1173] [Iter 764] [G_Loss 9.523701] [D_Loss 0.005513]\n",
      "train [Epoch 0/10] [Batch 765/1173] [Iter 765] [G_Loss 9.269409] [D_Loss 0.048435]\n",
      "train [Epoch 0/10] [Batch 766/1173] [Iter 766] [G_Loss 7.579215] [D_Loss 0.006640]\n",
      "train [Epoch 0/10] [Batch 767/1173] [Iter 767] [G_Loss 6.636077] [D_Loss 0.006321]\n",
      "train [Epoch 0/10] [Batch 768/1173] [Iter 768] [G_Loss 6.609331] [D_Loss 0.011185]\n",
      "train [Epoch 0/10] [Batch 769/1173] [Iter 769] [G_Loss 7.313048] [D_Loss 0.003723]\n",
      "train [Epoch 0/10] [Batch 770/1173] [Iter 770] [G_Loss 7.482625] [D_Loss 0.014059]\n",
      "train [Epoch 0/10] [Batch 771/1173] [Iter 771] [G_Loss 6.675654] [D_Loss 0.004459]\n",
      "train [Epoch 0/10] [Batch 772/1173] [Iter 772] [G_Loss 7.342107] [D_Loss 0.003851]\n",
      "train [Epoch 0/10] [Batch 773/1173] [Iter 773] [G_Loss 7.725748] [D_Loss 0.008117]\n",
      "train [Epoch 0/10] [Batch 774/1173] [Iter 774] [G_Loss 7.458088] [D_Loss 0.040007]\n",
      "train [Epoch 0/10] [Batch 775/1173] [Iter 775] [G_Loss 6.081510] [D_Loss 0.008531]\n",
      "train [Epoch 0/10] [Batch 776/1173] [Iter 776] [G_Loss 6.590459] [D_Loss 0.065189]\n",
      "train [Epoch 0/10] [Batch 777/1173] [Iter 777] [G_Loss 8.533450] [D_Loss 0.002407]\n",
      "train [Epoch 0/10] [Batch 778/1173] [Iter 778] [G_Loss 10.102810] [D_Loss 0.000362]\n",
      "train [Epoch 0/10] [Batch 779/1173] [Iter 779] [G_Loss 10.171610] [D_Loss 0.101581]\n",
      "train [Epoch 0/10] [Batch 780/1173] [Iter 780] [G_Loss 7.775960] [D_Loss 0.015029]\n",
      "train [Epoch 0/10] [Batch 781/1173] [Iter 781] [G_Loss 6.411978] [D_Loss 0.011541]\n",
      "train [Epoch 0/10] [Batch 782/1173] [Iter 782] [G_Loss 6.196536] [D_Loss 0.011043]\n",
      "train [Epoch 0/10] [Batch 783/1173] [Iter 783] [G_Loss 7.637987] [D_Loss 0.003819]\n",
      "train [Epoch 0/10] [Batch 784/1173] [Iter 784] [G_Loss 8.573192] [D_Loss 0.012101]\n",
      "train [Epoch 0/10] [Batch 785/1173] [Iter 785] [G_Loss 8.836633] [D_Loss 0.002285]\n",
      "train [Epoch 0/10] [Batch 786/1173] [Iter 786] [G_Loss 8.932556] [D_Loss 0.000689]\n",
      "train [Epoch 0/10] [Batch 787/1173] [Iter 787] [G_Loss 8.826045] [D_Loss 0.000762]\n",
      "train [Epoch 0/10] [Batch 788/1173] [Iter 788] [G_Loss 8.972253] [D_Loss 0.008315]\n",
      "train [Epoch 0/10] [Batch 789/1173] [Iter 789] [G_Loss 8.534209] [D_Loss 0.000846]\n",
      "train [Epoch 0/10] [Batch 790/1173] [Iter 790] [G_Loss 8.427838] [D_Loss 0.000659]\n",
      "train [Epoch 0/10] [Batch 791/1173] [Iter 791] [G_Loss 7.996679] [D_Loss 0.002853]\n",
      "train [Epoch 0/10] [Batch 792/1173] [Iter 792] [G_Loss 8.312749] [D_Loss 0.001695]\n",
      "train [Epoch 0/10] [Batch 793/1173] [Iter 793] [G_Loss 8.660844] [D_Loss 0.003997]\n",
      "train [Epoch 0/10] [Batch 794/1173] [Iter 794] [G_Loss 8.692751] [D_Loss 0.071874]\n",
      "train [Epoch 0/10] [Batch 795/1173] [Iter 795] [G_Loss 6.843882] [D_Loss 0.005898]\n",
      "train [Epoch 0/10] [Batch 796/1173] [Iter 796] [G_Loss 6.195149] [D_Loss 0.007715]\n",
      "train [Epoch 0/10] [Batch 797/1173] [Iter 797] [G_Loss 7.304980] [D_Loss 0.006320]\n",
      "train [Epoch 0/10] [Batch 798/1173] [Iter 798] [G_Loss 7.025069] [D_Loss 0.012874]\n",
      "train [Epoch 0/10] [Batch 799/1173] [Iter 799] [G_Loss 7.656514] [D_Loss 0.018288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 800/1173] [Iter 800] [G_Loss 10.149006] [D_Loss 0.000928]\n",
      "train [Epoch 0/10] [Batch 801/1173] [Iter 801] [G_Loss 11.632154] [D_Loss 0.079637]\n",
      "train [Epoch 0/10] [Batch 802/1173] [Iter 802] [G_Loss 10.886811] [D_Loss 0.000275]\n",
      "train [Epoch 0/10] [Batch 803/1173] [Iter 803] [G_Loss 9.822719] [D_Loss 0.002087]\n",
      "train [Epoch 0/10] [Batch 804/1173] [Iter 804] [G_Loss 8.876369] [D_Loss 0.003053]\n",
      "train [Epoch 0/10] [Batch 805/1173] [Iter 805] [G_Loss 10.216339] [D_Loss 0.003661]\n",
      "train [Epoch 0/10] [Batch 806/1173] [Iter 806] [G_Loss 8.535340] [D_Loss 0.002091]\n",
      "train [Epoch 0/10] [Batch 807/1173] [Iter 807] [G_Loss 8.296196] [D_Loss 0.006000]\n",
      "train [Epoch 0/10] [Batch 808/1173] [Iter 808] [G_Loss 9.278138] [D_Loss 0.003064]\n",
      "train [Epoch 0/10] [Batch 809/1173] [Iter 809] [G_Loss 9.630798] [D_Loss 0.061664]\n",
      "train [Epoch 0/10] [Batch 810/1173] [Iter 810] [G_Loss 7.620356] [D_Loss 0.006051]\n",
      "train [Epoch 0/10] [Batch 811/1173] [Iter 811] [G_Loss 8.279477] [D_Loss 0.001650]\n",
      "train [Epoch 0/10] [Batch 812/1173] [Iter 812] [G_Loss 8.019169] [D_Loss 0.003620]\n",
      "train [Epoch 0/10] [Batch 813/1173] [Iter 813] [G_Loss 8.020020] [D_Loss 0.003704]\n",
      "train [Epoch 0/10] [Batch 814/1173] [Iter 814] [G_Loss 8.159608] [D_Loss 0.038188]\n",
      "train [Epoch 0/10] [Batch 815/1173] [Iter 815] [G_Loss 7.735179] [D_Loss 0.005887]\n",
      "train [Epoch 0/10] [Batch 816/1173] [Iter 816] [G_Loss 8.210601] [D_Loss 0.047001]\n",
      "train [Epoch 0/10] [Batch 817/1173] [Iter 817] [G_Loss 7.247197] [D_Loss 0.005390]\n",
      "train [Epoch 0/10] [Batch 818/1173] [Iter 818] [G_Loss 7.287090] [D_Loss 0.011991]\n",
      "train [Epoch 0/10] [Batch 819/1173] [Iter 819] [G_Loss 8.551712] [D_Loss 0.010174]\n",
      "train [Epoch 0/10] [Batch 820/1173] [Iter 820] [G_Loss 8.329367] [D_Loss 0.002413]\n",
      "train [Epoch 0/10] [Batch 821/1173] [Iter 821] [G_Loss 7.898989] [D_Loss 0.010098]\n",
      "train [Epoch 0/10] [Batch 822/1173] [Iter 822] [G_Loss 8.643183] [D_Loss 0.008517]\n",
      "train [Epoch 0/10] [Batch 823/1173] [Iter 823] [G_Loss 7.437948] [D_Loss 0.007636]\n",
      "train [Epoch 0/10] [Batch 824/1173] [Iter 824] [G_Loss 8.297736] [D_Loss 0.004361]\n",
      "train [Epoch 0/10] [Batch 825/1173] [Iter 825] [G_Loss 8.772199] [D_Loss 0.003626]\n",
      "train [Epoch 0/10] [Batch 826/1173] [Iter 826] [G_Loss 9.020040] [D_Loss 0.002235]\n",
      "train [Epoch 0/10] [Batch 827/1173] [Iter 827] [G_Loss 9.660316] [D_Loss 0.001109]\n",
      "train [Epoch 0/10] [Batch 828/1173] [Iter 828] [G_Loss 9.549024] [D_Loss 0.005226]\n",
      "train [Epoch 0/10] [Batch 829/1173] [Iter 829] [G_Loss 9.156674] [D_Loss 0.004654]\n",
      "train [Epoch 0/10] [Batch 830/1173] [Iter 830] [G_Loss 8.410849] [D_Loss 0.014072]\n",
      "train [Epoch 0/10] [Batch 831/1173] [Iter 831] [G_Loss 6.971238] [D_Loss 0.033895]\n",
      "train [Epoch 0/10] [Batch 832/1173] [Iter 832] [G_Loss 8.588752] [D_Loss 0.001352]\n",
      "train [Epoch 0/10] [Batch 833/1173] [Iter 833] [G_Loss 9.737043] [D_Loss 0.000418]\n",
      "train [Epoch 0/10] [Batch 834/1173] [Iter 834] [G_Loss 10.246500] [D_Loss 0.000490]\n",
      "train [Epoch 0/10] [Batch 835/1173] [Iter 835] [G_Loss 10.312348] [D_Loss 0.000264]\n",
      "train [Epoch 0/10] [Batch 836/1173] [Iter 836] [G_Loss 10.352612] [D_Loss 0.000375]\n",
      "train [Epoch 0/10] [Batch 837/1173] [Iter 837] [G_Loss 9.860435] [D_Loss 0.000659]\n",
      "train [Epoch 0/10] [Batch 838/1173] [Iter 838] [G_Loss 9.855697] [D_Loss 0.010081]\n",
      "train [Epoch 0/10] [Batch 839/1173] [Iter 839] [G_Loss 8.894206] [D_Loss 0.039686]\n",
      "train [Epoch 0/10] [Batch 840/1173] [Iter 840] [G_Loss 8.521546] [D_Loss 0.002300]\n",
      "train [Epoch 0/10] [Batch 841/1173] [Iter 841] [G_Loss 7.762372] [D_Loss 0.002682]\n",
      "train [Epoch 0/10] [Batch 842/1173] [Iter 842] [G_Loss 8.009221] [D_Loss 0.001673]\n",
      "train [Epoch 0/10] [Batch 843/1173] [Iter 843] [G_Loss 7.965526] [D_Loss 0.011919]\n",
      "train [Epoch 0/10] [Batch 844/1173] [Iter 844] [G_Loss 7.646221] [D_Loss 0.005906]\n",
      "train [Epoch 0/10] [Batch 845/1173] [Iter 845] [G_Loss 8.223460] [D_Loss 0.003505]\n",
      "train [Epoch 0/10] [Batch 846/1173] [Iter 846] [G_Loss 8.810043] [D_Loss 0.000881]\n",
      "train [Epoch 0/10] [Batch 847/1173] [Iter 847] [G_Loss 9.117581] [D_Loss 0.001266]\n",
      "train [Epoch 0/10] [Batch 848/1173] [Iter 848] [G_Loss 9.915133] [D_Loss 0.001182]\n",
      "train [Epoch 0/10] [Batch 849/1173] [Iter 849] [G_Loss 9.613615] [D_Loss 0.000763]\n",
      "train [Epoch 0/10] [Batch 850/1173] [Iter 850] [G_Loss 10.517090] [D_Loss 0.000886]\n",
      "train [Epoch 0/10] [Batch 851/1173] [Iter 851] [G_Loss 10.052466] [D_Loss 0.001069]\n",
      "train [Epoch 0/10] [Batch 852/1173] [Iter 852] [G_Loss 10.162480] [D_Loss 0.000247]\n",
      "train [Epoch 0/10] [Batch 853/1173] [Iter 853] [G_Loss 9.950175] [D_Loss 0.000301]\n",
      "train [Epoch 0/10] [Batch 854/1173] [Iter 854] [G_Loss 10.654446] [D_Loss 0.000232]\n",
      "train [Epoch 0/10] [Batch 855/1173] [Iter 855] [G_Loss 10.866489] [D_Loss 0.023518]\n",
      "train [Epoch 0/10] [Batch 856/1173] [Iter 856] [G_Loss 9.769862] [D_Loss 0.002632]\n",
      "train [Epoch 0/10] [Batch 857/1173] [Iter 857] [G_Loss 8.964376] [D_Loss 0.011925]\n",
      "train [Epoch 0/10] [Batch 858/1173] [Iter 858] [G_Loss 10.940670] [D_Loss 0.000785]\n",
      "train [Epoch 0/10] [Batch 859/1173] [Iter 859] [G_Loss 11.771931] [D_Loss 0.011835]\n",
      "train [Epoch 0/10] [Batch 860/1173] [Iter 860] [G_Loss 9.965711] [D_Loss 0.000498]\n",
      "train [Epoch 0/10] [Batch 861/1173] [Iter 861] [G_Loss 9.945336] [D_Loss 0.000611]\n",
      "train [Epoch 0/10] [Batch 862/1173] [Iter 862] [G_Loss 10.126493] [D_Loss 0.000261]\n",
      "train [Epoch 0/10] [Batch 863/1173] [Iter 863] [G_Loss 10.686437] [D_Loss 0.016203]\n",
      "train [Epoch 0/10] [Batch 864/1173] [Iter 864] [G_Loss 9.082399] [D_Loss 0.001535]\n",
      "train [Epoch 0/10] [Batch 865/1173] [Iter 865] [G_Loss 7.543553] [D_Loss 0.004357]\n",
      "train [Epoch 0/10] [Batch 866/1173] [Iter 866] [G_Loss 9.039532] [D_Loss 0.001420]\n",
      "train [Epoch 0/10] [Batch 867/1173] [Iter 867] [G_Loss 8.889716] [D_Loss 0.000783]\n",
      "train [Epoch 0/10] [Batch 868/1173] [Iter 868] [G_Loss 9.066962] [D_Loss 0.001210]\n",
      "train [Epoch 0/10] [Batch 869/1173] [Iter 869] [G_Loss 9.395209] [D_Loss 0.013482]\n",
      "train [Epoch 0/10] [Batch 870/1173] [Iter 870] [G_Loss 7.371604] [D_Loss 0.013248]\n",
      "train [Epoch 0/10] [Batch 871/1173] [Iter 871] [G_Loss 10.003856] [D_Loss 0.021480]\n",
      "train [Epoch 0/10] [Batch 872/1173] [Iter 872] [G_Loss 8.428865] [D_Loss 0.005709]\n",
      "train [Epoch 0/10] [Batch 873/1173] [Iter 873] [G_Loss 8.561670] [D_Loss 0.001134]\n",
      "train [Epoch 0/10] [Batch 874/1173] [Iter 874] [G_Loss 8.545002] [D_Loss 0.002775]\n",
      "train [Epoch 0/10] [Batch 875/1173] [Iter 875] [G_Loss 8.283495] [D_Loss 0.003064]\n",
      "train [Epoch 0/10] [Batch 876/1173] [Iter 876] [G_Loss 8.187613] [D_Loss 0.003875]\n",
      "train [Epoch 0/10] [Batch 877/1173] [Iter 877] [G_Loss 10.019587] [D_Loss 0.000260]\n",
      "train [Epoch 0/10] [Batch 878/1173] [Iter 878] [G_Loss 9.890783] [D_Loss 0.000204]\n",
      "train [Epoch 0/10] [Batch 879/1173] [Iter 879] [G_Loss 10.044519] [D_Loss 0.000335]\n",
      "train [Epoch 0/10] [Batch 880/1173] [Iter 880] [G_Loss 9.540573] [D_Loss 0.014516]\n",
      "train [Epoch 0/10] [Batch 881/1173] [Iter 881] [G_Loss 6.496417] [D_Loss 0.029519]\n",
      "train [Epoch 0/10] [Batch 882/1173] [Iter 882] [G_Loss 11.039978] [D_Loss 0.001175]\n",
      "train [Epoch 0/10] [Batch 883/1173] [Iter 883] [G_Loss 11.989289] [D_Loss 0.040355]\n",
      "train [Epoch 0/10] [Batch 884/1173] [Iter 884] [G_Loss 11.104604] [D_Loss 0.002889]\n",
      "train [Epoch 0/10] [Batch 885/1173] [Iter 885] [G_Loss 8.822353] [D_Loss 0.014068]\n",
      "train [Epoch 0/10] [Batch 886/1173] [Iter 886] [G_Loss 10.470830] [D_Loss 0.036190]\n",
      "train [Epoch 0/10] [Batch 887/1173] [Iter 887] [G_Loss 9.846968] [D_Loss 0.004793]\n",
      "train [Epoch 0/10] [Batch 888/1173] [Iter 888] [G_Loss 8.720338] [D_Loss 0.002255]\n",
      "train [Epoch 0/10] [Batch 889/1173] [Iter 889] [G_Loss 8.220256] [D_Loss 0.001371]\n",
      "train [Epoch 0/10] [Batch 890/1173] [Iter 890] [G_Loss 8.724242] [D_Loss 0.002163]\n",
      "train [Epoch 0/10] [Batch 891/1173] [Iter 891] [G_Loss 8.474983] [D_Loss 0.030385]\n",
      "train [Epoch 0/10] [Batch 892/1173] [Iter 892] [G_Loss 7.412282] [D_Loss 0.002306]\n",
      "train [Epoch 0/10] [Batch 893/1173] [Iter 893] [G_Loss 5.962523] [D_Loss 0.011444]\n",
      "train [Epoch 0/10] [Batch 894/1173] [Iter 894] [G_Loss 8.003500] [D_Loss 0.002211]\n",
      "train [Epoch 0/10] [Batch 895/1173] [Iter 895] [G_Loss 9.327819] [D_Loss 0.000466]\n",
      "train [Epoch 0/10] [Batch 896/1173] [Iter 896] [G_Loss 9.846034] [D_Loss 0.006887]\n",
      "train [Epoch 0/10] [Batch 897/1173] [Iter 897] [G_Loss 9.567217] [D_Loss 0.024467]\n",
      "train [Epoch 0/10] [Batch 898/1173] [Iter 898] [G_Loss 7.758968] [D_Loss 0.005183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 899/1173] [Iter 899] [G_Loss 8.257024] [D_Loss 0.001495]\n",
      "train [Epoch 0/10] [Batch 900/1173] [Iter 900] [G_Loss 7.673631] [D_Loss 0.002192]\n",
      "train [Epoch 0/10] [Batch 901/1173] [Iter 901] [G_Loss 8.162182] [D_Loss 0.001060]\n",
      "train [Epoch 0/10] [Batch 902/1173] [Iter 902] [G_Loss 8.833540] [D_Loss 0.019674]\n",
      "train [Epoch 0/10] [Batch 903/1173] [Iter 903] [G_Loss 7.710322] [D_Loss 0.002497]\n",
      "train [Epoch 0/10] [Batch 904/1173] [Iter 904] [G_Loss 7.620625] [D_Loss 0.004890]\n",
      "train [Epoch 0/10] [Batch 905/1173] [Iter 905] [G_Loss 7.244477] [D_Loss 0.001962]\n",
      "train [Epoch 0/10] [Batch 906/1173] [Iter 906] [G_Loss 7.255416] [D_Loss 0.006805]\n",
      "train [Epoch 0/10] [Batch 907/1173] [Iter 907] [G_Loss 8.597007] [D_Loss 0.004149]\n",
      "train [Epoch 0/10] [Batch 908/1173] [Iter 908] [G_Loss 9.206196] [D_Loss 0.000377]\n",
      "train [Epoch 0/10] [Batch 909/1173] [Iter 909] [G_Loss 9.507601] [D_Loss 0.005939]\n",
      "train [Epoch 0/10] [Batch 910/1173] [Iter 910] [G_Loss 8.178848] [D_Loss 0.001225]\n",
      "train [Epoch 0/10] [Batch 911/1173] [Iter 911] [G_Loss 8.501546] [D_Loss 0.005768]\n",
      "train [Epoch 0/10] [Batch 912/1173] [Iter 912] [G_Loss 8.150565] [D_Loss 0.001127]\n",
      "train [Epoch 0/10] [Batch 913/1173] [Iter 913] [G_Loss 8.156853] [D_Loss 0.001948]\n",
      "train [Epoch 0/10] [Batch 914/1173] [Iter 914] [G_Loss 8.387880] [D_Loss 0.010588]\n",
      "train [Epoch 0/10] [Batch 915/1173] [Iter 915] [G_Loss 7.982600] [D_Loss 0.002283]\n",
      "train [Epoch 0/10] [Batch 916/1173] [Iter 916] [G_Loss 6.970825] [D_Loss 0.004780]\n",
      "train [Epoch 0/10] [Batch 917/1173] [Iter 917] [G_Loss 8.539463] [D_Loss 0.000615]\n",
      "train [Epoch 0/10] [Batch 918/1173] [Iter 918] [G_Loss 9.413693] [D_Loss 0.000556]\n",
      "train [Epoch 0/10] [Batch 919/1173] [Iter 919] [G_Loss 9.511086] [D_Loss 0.001165]\n",
      "train [Epoch 0/10] [Batch 920/1173] [Iter 920] [G_Loss 9.722678] [D_Loss 0.000250]\n",
      "train [Epoch 0/10] [Batch 921/1173] [Iter 921] [G_Loss 11.051328] [D_Loss 0.000066]\n",
      "train [Epoch 0/10] [Batch 922/1173] [Iter 922] [G_Loss 10.770491] [D_Loss 0.018343]\n",
      "train [Epoch 0/10] [Batch 923/1173] [Iter 923] [G_Loss 9.060281] [D_Loss 0.000612]\n",
      "train [Epoch 0/10] [Batch 924/1173] [Iter 924] [G_Loss 8.766102] [D_Loss 0.015647]\n",
      "train [Epoch 0/10] [Batch 925/1173] [Iter 925] [G_Loss 7.271436] [D_Loss 0.001599]\n",
      "train [Epoch 0/10] [Batch 926/1173] [Iter 926] [G_Loss 7.510835] [D_Loss 0.003145]\n",
      "train [Epoch 0/10] [Batch 927/1173] [Iter 927] [G_Loss 8.129455] [D_Loss 0.001058]\n",
      "train [Epoch 0/10] [Batch 928/1173] [Iter 928] [G_Loss 7.826865] [D_Loss 0.029999]\n",
      "train [Epoch 0/10] [Batch 929/1173] [Iter 929] [G_Loss 7.295793] [D_Loss 0.003787]\n",
      "train [Epoch 0/10] [Batch 930/1173] [Iter 930] [G_Loss 7.705682] [D_Loss 0.001440]\n",
      "train [Epoch 0/10] [Batch 931/1173] [Iter 931] [G_Loss 7.935992] [D_Loss 0.002464]\n",
      "train [Epoch 0/10] [Batch 932/1173] [Iter 932] [G_Loss 8.362719] [D_Loss 0.001880]\n",
      "train [Epoch 0/10] [Batch 933/1173] [Iter 933] [G_Loss 10.173153] [D_Loss 0.009974]\n",
      "train [Epoch 0/10] [Batch 934/1173] [Iter 934] [G_Loss 9.523085] [D_Loss 0.010645]\n",
      "train [Epoch 0/10] [Batch 935/1173] [Iter 935] [G_Loss 7.751339] [D_Loss 0.001504]\n",
      "train [Epoch 0/10] [Batch 936/1173] [Iter 936] [G_Loss 7.282040] [D_Loss 0.002274]\n",
      "train [Epoch 0/10] [Batch 937/1173] [Iter 937] [G_Loss 8.121438] [D_Loss 0.001521]\n",
      "train [Epoch 0/10] [Batch 938/1173] [Iter 938] [G_Loss 8.359624] [D_Loss 0.001097]\n",
      "train [Epoch 0/10] [Batch 939/1173] [Iter 939] [G_Loss 8.817219] [D_Loss 0.001625]\n",
      "train [Epoch 0/10] [Batch 940/1173] [Iter 940] [G_Loss 9.622631] [D_Loss 0.002262]\n",
      "train [Epoch 0/10] [Batch 941/1173] [Iter 941] [G_Loss 9.978352] [D_Loss 0.000311]\n",
      "train [Epoch 0/10] [Batch 942/1173] [Iter 942] [G_Loss 10.505908] [D_Loss 0.002417]\n",
      "train [Epoch 0/10] [Batch 943/1173] [Iter 943] [G_Loss 9.967235] [D_Loss 0.000177]\n",
      "train [Epoch 0/10] [Batch 944/1173] [Iter 944] [G_Loss 9.918509] [D_Loss 0.000509]\n",
      "train [Epoch 0/10] [Batch 945/1173] [Iter 945] [G_Loss 10.480998] [D_Loss 0.000260]\n",
      "train [Epoch 0/10] [Batch 946/1173] [Iter 946] [G_Loss 11.054705] [D_Loss 0.000097]\n",
      "train [Epoch 0/10] [Batch 947/1173] [Iter 947] [G_Loss 9.588881] [D_Loss 0.000458]\n",
      "train [Epoch 0/10] [Batch 948/1173] [Iter 948] [G_Loss 10.188103] [D_Loss 0.001416]\n",
      "train [Epoch 0/10] [Batch 949/1173] [Iter 949] [G_Loss 9.703196] [D_Loss 0.000326]\n",
      "train [Epoch 0/10] [Batch 950/1173] [Iter 950] [G_Loss 9.825670] [D_Loss 0.000274]\n",
      "train [Epoch 0/10] [Batch 951/1173] [Iter 951] [G_Loss 9.702805] [D_Loss 0.000282]\n",
      "train [Epoch 0/10] [Batch 952/1173] [Iter 952] [G_Loss 10.132651] [D_Loss 0.002182]\n",
      "train [Epoch 0/10] [Batch 953/1173] [Iter 953] [G_Loss 9.151299] [D_Loss 0.000343]\n",
      "train [Epoch 0/10] [Batch 954/1173] [Iter 954] [G_Loss 9.536283] [D_Loss 0.041020]\n",
      "train [Epoch 0/10] [Batch 955/1173] [Iter 955] [G_Loss 3.212436] [D_Loss 0.218641]\n",
      "train [Epoch 0/10] [Batch 956/1173] [Iter 956] [G_Loss 41.928020] [D_Loss 0.059988]\n",
      "train [Epoch 0/10] [Batch 957/1173] [Iter 957] [G_Loss 49.731640] [D_Loss 0.882240]\n",
      "train [Epoch 0/10] [Batch 958/1173] [Iter 958] [G_Loss 30.226999] [D_Loss 0.173360]\n",
      "train [Epoch 0/10] [Batch 959/1173] [Iter 959] [G_Loss 23.094975] [D_Loss 0.124529]\n",
      "train [Epoch 0/10] [Batch 960/1173] [Iter 960] [G_Loss 13.835772] [D_Loss 0.068126]\n",
      "train [Epoch 0/10] [Batch 961/1173] [Iter 961] [G_Loss 1.872218] [D_Loss 0.867801]\n",
      "train [Epoch 0/10] [Batch 962/1173] [Iter 962] [G_Loss 22.482313] [D_Loss 0.104900]\n",
      "train [Epoch 0/10] [Batch 963/1173] [Iter 963] [G_Loss 25.512856] [D_Loss 0.112589]\n",
      "train [Epoch 0/10] [Batch 964/1173] [Iter 964] [G_Loss 24.338764] [D_Loss 0.096431]\n",
      "train [Epoch 0/10] [Batch 965/1173] [Iter 965] [G_Loss 22.050774] [D_Loss 0.276577]\n",
      "train [Epoch 0/10] [Batch 966/1173] [Iter 966] [G_Loss 18.888935] [D_Loss 0.287392]\n",
      "train [Epoch 0/10] [Batch 967/1173] [Iter 967] [G_Loss 16.344646] [D_Loss 0.000548]\n",
      "train [Epoch 0/10] [Batch 968/1173] [Iter 968] [G_Loss 15.010432] [D_Loss 0.017285]\n",
      "train [Epoch 0/10] [Batch 969/1173] [Iter 969] [G_Loss 14.851171] [D_Loss 0.024997]\n",
      "train [Epoch 0/10] [Batch 970/1173] [Iter 970] [G_Loss 13.381531] [D_Loss 0.000190]\n",
      "train [Epoch 0/10] [Batch 971/1173] [Iter 971] [G_Loss 12.859480] [D_Loss 0.042109]\n",
      "train [Epoch 0/10] [Batch 972/1173] [Iter 972] [G_Loss 10.844511] [D_Loss 0.000049]\n",
      "train [Epoch 0/10] [Batch 973/1173] [Iter 973] [G_Loss 10.669927] [D_Loss 0.095123]\n",
      "train [Epoch 0/10] [Batch 974/1173] [Iter 974] [G_Loss 9.175759] [D_Loss 0.163859]\n",
      "train [Epoch 0/10] [Batch 975/1173] [Iter 975] [G_Loss 4.612172] [D_Loss 0.048091]\n",
      "train [Epoch 0/10] [Batch 976/1173] [Iter 976] [G_Loss 4.571207] [D_Loss 0.016921]\n",
      "train [Epoch 0/10] [Batch 977/1173] [Iter 977] [G_Loss 8.267291] [D_Loss 0.000380]\n",
      "train [Epoch 0/10] [Batch 978/1173] [Iter 978] [G_Loss 9.517702] [D_Loss 0.001372]\n",
      "train [Epoch 0/10] [Batch 979/1173] [Iter 979] [G_Loss 9.953070] [D_Loss 0.010437]\n",
      "train [Epoch 0/10] [Batch 980/1173] [Iter 980] [G_Loss 9.837360] [D_Loss 0.001935]\n",
      "train [Epoch 0/10] [Batch 981/1173] [Iter 981] [G_Loss 9.625538] [D_Loss 0.000102]\n",
      "train [Epoch 0/10] [Batch 982/1173] [Iter 982] [G_Loss 9.141698] [D_Loss 0.000415]\n",
      "train [Epoch 0/10] [Batch 983/1173] [Iter 983] [G_Loss 8.930775] [D_Loss 0.000236]\n",
      "train [Epoch 0/10] [Batch 984/1173] [Iter 984] [G_Loss 9.096293] [D_Loss 0.001288]\n",
      "train [Epoch 0/10] [Batch 985/1173] [Iter 985] [G_Loss 8.328718] [D_Loss 0.000385]\n",
      "train [Epoch 0/10] [Batch 986/1173] [Iter 986] [G_Loss 8.244014] [D_Loss 0.000428]\n",
      "train [Epoch 0/10] [Batch 987/1173] [Iter 987] [G_Loss 8.095112] [D_Loss 0.000711]\n",
      "train [Epoch 0/10] [Batch 988/1173] [Iter 988] [G_Loss 8.389014] [D_Loss 0.000337]\n",
      "train [Epoch 0/10] [Batch 989/1173] [Iter 989] [G_Loss 7.922424] [D_Loss 0.001157]\n",
      "train [Epoch 0/10] [Batch 990/1173] [Iter 990] [G_Loss 8.125472] [D_Loss 0.002627]\n",
      "train [Epoch 0/10] [Batch 991/1173] [Iter 991] [G_Loss 7.746098] [D_Loss 0.001438]\n",
      "train [Epoch 0/10] [Batch 992/1173] [Iter 992] [G_Loss 7.608645] [D_Loss 0.001036]\n",
      "train [Epoch 0/10] [Batch 993/1173] [Iter 993] [G_Loss 7.620110] [D_Loss 0.001038]\n",
      "train [Epoch 0/10] [Batch 994/1173] [Iter 994] [G_Loss 7.649762] [D_Loss 0.000759]\n",
      "train [Epoch 0/10] [Batch 995/1173] [Iter 995] [G_Loss 7.553663] [D_Loss 0.000963]\n",
      "train [Epoch 0/10] [Batch 996/1173] [Iter 996] [G_Loss 7.786399] [D_Loss 0.042978]\n",
      "train [Epoch 0/10] [Batch 997/1173] [Iter 997] [G_Loss 7.134502] [D_Loss 0.021369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 998/1173] [Iter 998] [G_Loss 6.391749] [D_Loss 0.002607]\n",
      "train [Epoch 0/10] [Batch 999/1173] [Iter 999] [G_Loss 6.540923] [D_Loss 0.001799]\n",
      "train [Epoch 0/10] [Batch 1000/1173] [Iter 1000] [G_Loss 7.033642] [D_Loss 0.001097]\n",
      "train [Epoch 0/10] [Batch 1001/1173] [Iter 1001] [G_Loss 6.432572] [D_Loss 0.002268]\n",
      "train [Epoch 0/10] [Batch 1002/1173] [Iter 1002] [G_Loss 6.543198] [D_Loss 0.003486]\n",
      "train [Epoch 0/10] [Batch 1003/1173] [Iter 1003] [G_Loss 6.432829] [D_Loss 0.002576]\n",
      "train [Epoch 0/10] [Batch 1004/1173] [Iter 1004] [G_Loss 6.357534] [D_Loss 0.006029]\n",
      "train [Epoch 0/10] [Batch 1005/1173] [Iter 1005] [G_Loss 6.458215] [D_Loss 0.002830]\n",
      "train [Epoch 0/10] [Batch 1006/1173] [Iter 1006] [G_Loss 6.790162] [D_Loss 0.001817]\n",
      "train [Epoch 0/10] [Batch 1007/1173] [Iter 1007] [G_Loss 6.283223] [D_Loss 0.002807]\n",
      "train [Epoch 0/10] [Batch 1008/1173] [Iter 1008] [G_Loss 6.950981] [D_Loss 0.001999]\n",
      "train [Epoch 0/10] [Batch 1009/1173] [Iter 1009] [G_Loss 7.687881] [D_Loss 0.000982]\n",
      "train [Epoch 0/10] [Batch 1010/1173] [Iter 1010] [G_Loss 6.784961] [D_Loss 0.001484]\n",
      "train [Epoch 0/10] [Batch 1011/1173] [Iter 1011] [G_Loss 7.130394] [D_Loss 0.058053]\n",
      "train [Epoch 0/10] [Batch 1012/1173] [Iter 1012] [G_Loss 8.079775] [D_Loss 0.000424]\n",
      "train [Epoch 0/10] [Batch 1013/1173] [Iter 1013] [G_Loss 8.188634] [D_Loss 0.011323]\n",
      "train [Epoch 0/10] [Batch 1014/1173] [Iter 1014] [G_Loss 8.295249] [D_Loss 0.000379]\n",
      "train [Epoch 0/10] [Batch 1015/1173] [Iter 1015] [G_Loss 7.959992] [D_Loss 0.000776]\n",
      "train [Epoch 0/10] [Batch 1016/1173] [Iter 1016] [G_Loss 8.257826] [D_Loss 0.000357]\n",
      "train [Epoch 0/10] [Batch 1017/1173] [Iter 1017] [G_Loss 8.061454] [D_Loss 0.000479]\n",
      "train [Epoch 0/10] [Batch 1018/1173] [Iter 1018] [G_Loss 8.230506] [D_Loss 0.000464]\n",
      "train [Epoch 0/10] [Batch 1019/1173] [Iter 1019] [G_Loss 8.104471] [D_Loss 0.001703]\n",
      "train [Epoch 0/10] [Batch 1020/1173] [Iter 1020] [G_Loss 7.852897] [D_Loss 0.000615]\n",
      "train [Epoch 0/10] [Batch 1021/1173] [Iter 1021] [G_Loss 7.786973] [D_Loss 0.000589]\n",
      "train [Epoch 0/10] [Batch 1022/1173] [Iter 1022] [G_Loss 7.660285] [D_Loss 0.000838]\n",
      "train [Epoch 0/10] [Batch 1023/1173] [Iter 1023] [G_Loss 7.883232] [D_Loss 0.000555]\n",
      "train [Epoch 0/10] [Batch 1024/1173] [Iter 1024] [G_Loss 8.375089] [D_Loss 0.013257]\n",
      "train [Epoch 0/10] [Batch 1025/1173] [Iter 1025] [G_Loss 8.249263] [D_Loss 0.000567]\n",
      "train [Epoch 0/10] [Batch 1026/1173] [Iter 1026] [G_Loss 8.074910] [D_Loss 0.000515]\n",
      "train [Epoch 0/10] [Batch 1027/1173] [Iter 1027] [G_Loss 8.057968] [D_Loss 0.001522]\n",
      "train [Epoch 0/10] [Batch 1028/1173] [Iter 1028] [G_Loss 8.261880] [D_Loss 0.000425]\n",
      "train [Epoch 0/10] [Batch 1029/1173] [Iter 1029] [G_Loss 8.129566] [D_Loss 0.000667]\n",
      "train [Epoch 0/10] [Batch 1030/1173] [Iter 1030] [G_Loss 8.216342] [D_Loss 0.001123]\n",
      "train [Epoch 0/10] [Batch 1031/1173] [Iter 1031] [G_Loss 8.756135] [D_Loss 0.000276]\n",
      "train [Epoch 0/10] [Batch 1032/1173] [Iter 1032] [G_Loss 8.630373] [D_Loss 0.000240]\n",
      "train [Epoch 0/10] [Batch 1033/1173] [Iter 1033] [G_Loss 8.779669] [D_Loss 0.000335]\n",
      "train [Epoch 0/10] [Batch 1034/1173] [Iter 1034] [G_Loss 8.359262] [D_Loss 0.000501]\n",
      "train [Epoch 0/10] [Batch 1035/1173] [Iter 1035] [G_Loss 7.959538] [D_Loss 0.013622]\n",
      "train [Epoch 0/10] [Batch 1036/1173] [Iter 1036] [G_Loss 8.344239] [D_Loss 0.002852]\n",
      "train [Epoch 0/10] [Batch 1037/1173] [Iter 1037] [G_Loss 8.842081] [D_Loss 0.000158]\n",
      "train [Epoch 0/10] [Batch 1038/1173] [Iter 1038] [G_Loss 8.428555] [D_Loss 0.000303]\n",
      "train [Epoch 0/10] [Batch 1039/1173] [Iter 1039] [G_Loss 8.113241] [D_Loss 0.000528]\n",
      "train [Epoch 0/10] [Batch 1040/1173] [Iter 1040] [G_Loss 8.195118] [D_Loss 0.000438]\n",
      "train [Epoch 0/10] [Batch 1041/1173] [Iter 1041] [G_Loss 8.318699] [D_Loss 0.000278]\n",
      "train [Epoch 0/10] [Batch 1042/1173] [Iter 1042] [G_Loss 8.564277] [D_Loss 0.002060]\n",
      "train [Epoch 0/10] [Batch 1043/1173] [Iter 1043] [G_Loss 8.231499] [D_Loss 0.000353]\n",
      "train [Epoch 0/10] [Batch 1044/1173] [Iter 1044] [G_Loss 8.442795] [D_Loss 0.000320]\n",
      "train [Epoch 0/10] [Batch 1045/1173] [Iter 1045] [G_Loss 8.849805] [D_Loss 0.000221]\n",
      "train [Epoch 0/10] [Batch 1046/1173] [Iter 1046] [G_Loss 8.421336] [D_Loss 0.000260]\n",
      "train [Epoch 0/10] [Batch 1047/1173] [Iter 1047] [G_Loss 8.605161] [D_Loss 0.000205]\n",
      "train [Epoch 0/10] [Batch 1048/1173] [Iter 1048] [G_Loss 8.450862] [D_Loss 0.000330]\n",
      "train [Epoch 0/10] [Batch 1049/1173] [Iter 1049] [G_Loss 8.603175] [D_Loss 0.000252]\n",
      "train [Epoch 0/10] [Batch 1050/1173] [Iter 1050] [G_Loss 8.565665] [D_Loss 0.001459]\n",
      "train [Epoch 0/10] [Batch 1051/1173] [Iter 1051] [G_Loss 8.411252] [D_Loss 0.000306]\n",
      "train [Epoch 0/10] [Batch 1052/1173] [Iter 1052] [G_Loss 8.740178] [D_Loss 0.000305]\n",
      "train [Epoch 0/10] [Batch 1053/1173] [Iter 1053] [G_Loss 8.381743] [D_Loss 0.000369]\n",
      "train [Epoch 0/10] [Batch 1054/1173] [Iter 1054] [G_Loss 8.225734] [D_Loss 0.000627]\n",
      "train [Epoch 0/10] [Batch 1055/1173] [Iter 1055] [G_Loss 8.255650] [D_Loss 0.000361]\n",
      "train [Epoch 0/10] [Batch 1056/1173] [Iter 1056] [G_Loss 8.108477] [D_Loss 0.035770]\n",
      "train [Epoch 0/10] [Batch 1057/1173] [Iter 1057] [G_Loss 8.839593] [D_Loss 0.000380]\n",
      "train [Epoch 0/10] [Batch 1058/1173] [Iter 1058] [G_Loss 7.667134] [D_Loss 0.000529]\n",
      "train [Epoch 0/10] [Batch 1059/1173] [Iter 1059] [G_Loss 11.415227] [D_Loss 0.004972]\n",
      "train [Epoch 0/10] [Batch 1060/1173] [Iter 1060] [G_Loss 8.483212] [D_Loss 0.000351]\n",
      "train [Epoch 0/10] [Batch 1061/1173] [Iter 1061] [G_Loss 7.755158] [D_Loss 0.000647]\n",
      "train [Epoch 0/10] [Batch 1062/1173] [Iter 1062] [G_Loss 7.499078] [D_Loss 0.000731]\n",
      "train [Epoch 0/10] [Batch 1063/1173] [Iter 1063] [G_Loss 7.662137] [D_Loss 0.000611]\n",
      "train [Epoch 0/10] [Batch 1064/1173] [Iter 1064] [G_Loss 7.872721] [D_Loss 0.000515]\n",
      "train [Epoch 0/10] [Batch 1065/1173] [Iter 1065] [G_Loss 8.130506] [D_Loss 0.000524]\n",
      "train [Epoch 0/10] [Batch 1066/1173] [Iter 1066] [G_Loss 7.715864] [D_Loss 0.001336]\n",
      "train [Epoch 0/10] [Batch 1067/1173] [Iter 1067] [G_Loss 7.582420] [D_Loss 0.000619]\n",
      "train [Epoch 0/10] [Batch 1068/1173] [Iter 1068] [G_Loss 7.954870] [D_Loss 0.000533]\n",
      "train [Epoch 0/10] [Batch 1069/1173] [Iter 1069] [G_Loss 7.597987] [D_Loss 0.000676]\n",
      "train [Epoch 0/10] [Batch 1070/1173] [Iter 1070] [G_Loss 8.261389] [D_Loss 0.000422]\n",
      "train [Epoch 0/10] [Batch 1071/1173] [Iter 1071] [G_Loss 7.695601] [D_Loss 0.000719]\n",
      "train [Epoch 0/10] [Batch 1072/1173] [Iter 1072] [G_Loss 8.030249] [D_Loss 0.000476]\n",
      "train [Epoch 0/10] [Batch 1073/1173] [Iter 1073] [G_Loss 7.942594] [D_Loss 0.000468]\n",
      "train [Epoch 0/10] [Batch 1074/1173] [Iter 1074] [G_Loss 7.867870] [D_Loss 0.001166]\n",
      "train [Epoch 0/10] [Batch 1075/1173] [Iter 1075] [G_Loss 7.861739] [D_Loss 0.000455]\n",
      "train [Epoch 0/10] [Batch 1076/1173] [Iter 1076] [G_Loss 8.091049] [D_Loss 0.000444]\n",
      "train [Epoch 0/10] [Batch 1077/1173] [Iter 1077] [G_Loss 7.855820] [D_Loss 0.000628]\n",
      "train [Epoch 0/10] [Batch 1078/1173] [Iter 1078] [G_Loss 8.258933] [D_Loss 0.000504]\n",
      "train [Epoch 0/10] [Batch 1079/1173] [Iter 1079] [G_Loss 8.083639] [D_Loss 0.000481]\n",
      "train [Epoch 0/10] [Batch 1080/1173] [Iter 1080] [G_Loss 7.971806] [D_Loss 0.001246]\n",
      "train [Epoch 0/10] [Batch 1081/1173] [Iter 1081] [G_Loss 8.218391] [D_Loss 0.000496]\n",
      "train [Epoch 0/10] [Batch 1082/1173] [Iter 1082] [G_Loss 8.492677] [D_Loss 0.000304]\n",
      "train [Epoch 0/10] [Batch 1083/1173] [Iter 1083] [G_Loss 7.868087] [D_Loss 0.001985]\n",
      "train [Epoch 0/10] [Batch 1084/1173] [Iter 1084] [G_Loss 7.971289] [D_Loss 0.000489]\n",
      "train [Epoch 0/10] [Batch 1085/1173] [Iter 1085] [G_Loss 8.061052] [D_Loss 0.000610]\n",
      "train [Epoch 0/10] [Batch 1086/1173] [Iter 1086] [G_Loss 7.968860] [D_Loss 0.000427]\n",
      "train [Epoch 0/10] [Batch 1087/1173] [Iter 1087] [G_Loss 7.938269] [D_Loss 0.001199]\n",
      "train [Epoch 0/10] [Batch 1088/1173] [Iter 1088] [G_Loss 8.233068] [D_Loss 0.000381]\n",
      "train [Epoch 0/10] [Batch 1089/1173] [Iter 1089] [G_Loss 8.571446] [D_Loss 0.000241]\n",
      "train [Epoch 0/10] [Batch 1090/1173] [Iter 1090] [G_Loss 8.377792] [D_Loss 0.000300]\n",
      "train [Epoch 0/10] [Batch 1091/1173] [Iter 1091] [G_Loss 8.286414] [D_Loss 0.001056]\n",
      "train [Epoch 0/10] [Batch 1092/1173] [Iter 1092] [G_Loss 8.290221] [D_Loss 0.000961]\n",
      "train [Epoch 0/10] [Batch 1093/1173] [Iter 1093] [G_Loss 8.268753] [D_Loss 0.000339]\n",
      "train [Epoch 0/10] [Batch 1094/1173] [Iter 1094] [G_Loss 8.321158] [D_Loss 0.000467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 1095/1173] [Iter 1095] [G_Loss 8.421984] [D_Loss 0.000300]\n",
      "train [Epoch 0/10] [Batch 1096/1173] [Iter 1096] [G_Loss 8.430933] [D_Loss 0.002832]\n",
      "train [Epoch 0/10] [Batch 1097/1173] [Iter 1097] [G_Loss 8.247802] [D_Loss 0.001410]\n",
      "train [Epoch 0/10] [Batch 1098/1173] [Iter 1098] [G_Loss 8.104677] [D_Loss 0.000587]\n",
      "train [Epoch 0/10] [Batch 1099/1173] [Iter 1099] [G_Loss 8.371483] [D_Loss 0.000666]\n",
      "train [Epoch 0/10] [Batch 1100/1173] [Iter 1100] [G_Loss 8.310215] [D_Loss 0.000573]\n",
      "train [Epoch 0/10] [Batch 1101/1173] [Iter 1101] [G_Loss 8.115624] [D_Loss 0.000560]\n",
      "train [Epoch 0/10] [Batch 1102/1173] [Iter 1102] [G_Loss 9.286182] [D_Loss 0.000112]\n",
      "train [Epoch 0/10] [Batch 1103/1173] [Iter 1103] [G_Loss 8.260077] [D_Loss 0.000330]\n",
      "train [Epoch 0/10] [Batch 1104/1173] [Iter 1104] [G_Loss 8.564466] [D_Loss 0.000318]\n",
      "train [Epoch 0/10] [Batch 1105/1173] [Iter 1105] [G_Loss 9.527561] [D_Loss 0.000221]\n",
      "train [Epoch 0/10] [Batch 1106/1173] [Iter 1106] [G_Loss 8.701002] [D_Loss 0.000223]\n",
      "train [Epoch 0/10] [Batch 1107/1173] [Iter 1107] [G_Loss 8.598139] [D_Loss 0.000285]\n",
      "train [Epoch 0/10] [Batch 1108/1173] [Iter 1108] [G_Loss 8.881608] [D_Loss 0.001826]\n",
      "train [Epoch 0/10] [Batch 1109/1173] [Iter 1109] [G_Loss 8.691768] [D_Loss 0.000590]\n",
      "train [Epoch 0/10] [Batch 1110/1173] [Iter 1110] [G_Loss 8.555116] [D_Loss 0.004893]\n",
      "train [Epoch 0/10] [Batch 1111/1173] [Iter 1111] [G_Loss 9.167007] [D_Loss 0.000282]\n",
      "train [Epoch 0/10] [Batch 1112/1173] [Iter 1112] [G_Loss 8.350010] [D_Loss 0.000327]\n",
      "train [Epoch 0/10] [Batch 1113/1173] [Iter 1113] [G_Loss 8.279150] [D_Loss 0.000355]\n",
      "train [Epoch 0/10] [Batch 1114/1173] [Iter 1114] [G_Loss 8.260507] [D_Loss 0.001139]\n",
      "train [Epoch 0/10] [Batch 1115/1173] [Iter 1115] [G_Loss 7.982100] [D_Loss 0.001227]\n",
      "train [Epoch 0/10] [Batch 1116/1173] [Iter 1116] [G_Loss 8.525272] [D_Loss 0.000308]\n",
      "train [Epoch 0/10] [Batch 1117/1173] [Iter 1117] [G_Loss 8.134647] [D_Loss 0.000388]\n",
      "train [Epoch 0/10] [Batch 1118/1173] [Iter 1118] [G_Loss 8.523831] [D_Loss 0.000267]\n",
      "train [Epoch 0/10] [Batch 1119/1173] [Iter 1119] [G_Loss 8.630769] [D_Loss 0.000194]\n",
      "train [Epoch 0/10] [Batch 1120/1173] [Iter 1120] [G_Loss 8.096688] [D_Loss 0.000600]\n",
      "train [Epoch 0/10] [Batch 1121/1173] [Iter 1121] [G_Loss 8.307844] [D_Loss 0.000424]\n",
      "train [Epoch 0/10] [Batch 1122/1173] [Iter 1122] [G_Loss 8.274940] [D_Loss 0.000350]\n",
      "train [Epoch 0/10] [Batch 1123/1173] [Iter 1123] [G_Loss 8.763281] [D_Loss 0.000406]\n",
      "train [Epoch 0/10] [Batch 1124/1173] [Iter 1124] [G_Loss 8.413143] [D_Loss 0.001131]\n",
      "train [Epoch 0/10] [Batch 1125/1173] [Iter 1125] [G_Loss 8.068827] [D_Loss 0.000515]\n",
      "train [Epoch 0/10] [Batch 1126/1173] [Iter 1126] [G_Loss 8.249207] [D_Loss 0.000342]\n",
      "train [Epoch 0/10] [Batch 1127/1173] [Iter 1127] [G_Loss 8.452030] [D_Loss 0.000261]\n",
      "train [Epoch 0/10] [Batch 1128/1173] [Iter 1128] [G_Loss 8.224533] [D_Loss 0.000356]\n",
      "train [Epoch 0/10] [Batch 1129/1173] [Iter 1129] [G_Loss 9.763881] [D_Loss 0.000720]\n",
      "train [Epoch 0/10] [Batch 1130/1173] [Iter 1130] [G_Loss 8.547482] [D_Loss 0.000308]\n",
      "train [Epoch 0/10] [Batch 1131/1173] [Iter 1131] [G_Loss 8.358254] [D_Loss 0.000300]\n",
      "train [Epoch 0/10] [Batch 1132/1173] [Iter 1132] [G_Loss 8.515635] [D_Loss 0.000363]\n",
      "train [Epoch 0/10] [Batch 1133/1173] [Iter 1133] [G_Loss 8.931005] [D_Loss 0.022924]\n",
      "train [Epoch 0/10] [Batch 1134/1173] [Iter 1134] [G_Loss 8.737445] [D_Loss 0.000233]\n",
      "train [Epoch 0/10] [Batch 1135/1173] [Iter 1135] [G_Loss 9.396845] [D_Loss 0.000107]\n",
      "train [Epoch 0/10] [Batch 1136/1173] [Iter 1136] [G_Loss 9.284248] [D_Loss 0.000124]\n",
      "train [Epoch 0/10] [Batch 1137/1173] [Iter 1137] [G_Loss 9.456160] [D_Loss 0.000116]\n",
      "train [Epoch 0/10] [Batch 1138/1173] [Iter 1138] [G_Loss 9.376281] [D_Loss 0.000117]\n",
      "train [Epoch 0/10] [Batch 1139/1173] [Iter 1139] [G_Loss 9.227790] [D_Loss 0.000163]\n",
      "train [Epoch 0/10] [Batch 1140/1173] [Iter 1140] [G_Loss 10.222731] [D_Loss 0.000068]\n",
      "train [Epoch 0/10] [Batch 1141/1173] [Iter 1141] [G_Loss 8.896365] [D_Loss 0.000317]\n",
      "train [Epoch 0/10] [Batch 1142/1173] [Iter 1142] [G_Loss 8.827791] [D_Loss 0.000170]\n",
      "train [Epoch 0/10] [Batch 1143/1173] [Iter 1143] [G_Loss 8.769415] [D_Loss 0.000221]\n",
      "train [Epoch 0/10] [Batch 1144/1173] [Iter 1144] [G_Loss 9.138590] [D_Loss 0.000820]\n",
      "train [Epoch 0/10] [Batch 1145/1173] [Iter 1145] [G_Loss 9.603031] [D_Loss 0.000130]\n",
      "train [Epoch 0/10] [Batch 1146/1173] [Iter 1146] [G_Loss 9.632281] [D_Loss 0.000084]\n",
      "train [Epoch 0/10] [Batch 1147/1173] [Iter 1147] [G_Loss 9.435476] [D_Loss 0.000107]\n",
      "train [Epoch 0/10] [Batch 1148/1173] [Iter 1148] [G_Loss 9.009573] [D_Loss 0.018892]\n",
      "train [Epoch 0/10] [Batch 1149/1173] [Iter 1149] [G_Loss 8.429134] [D_Loss 0.000266]\n",
      "train [Epoch 0/10] [Batch 1150/1173] [Iter 1150] [G_Loss 8.256208] [D_Loss 0.000395]\n",
      "train [Epoch 0/10] [Batch 1151/1173] [Iter 1151] [G_Loss 8.309353] [D_Loss 0.000306]\n",
      "train [Epoch 0/10] [Batch 1152/1173] [Iter 1152] [G_Loss 8.306880] [D_Loss 0.000374]\n",
      "train [Epoch 0/10] [Batch 1153/1173] [Iter 1153] [G_Loss 8.495209] [D_Loss 0.000468]\n",
      "train [Epoch 0/10] [Batch 1154/1173] [Iter 1154] [G_Loss 8.760300] [D_Loss 0.000200]\n",
      "train [Epoch 0/10] [Batch 1155/1173] [Iter 1155] [G_Loss 8.632080] [D_Loss 0.000249]\n",
      "train [Epoch 0/10] [Batch 1156/1173] [Iter 1156] [G_Loss 8.289900] [D_Loss 0.000291]\n",
      "train [Epoch 0/10] [Batch 1157/1173] [Iter 1157] [G_Loss 8.444064] [D_Loss 0.000236]\n",
      "train [Epoch 0/10] [Batch 1158/1173] [Iter 1158] [G_Loss 8.374169] [D_Loss 0.000516]\n",
      "train [Epoch 0/10] [Batch 1159/1173] [Iter 1159] [G_Loss 9.730825] [D_Loss 0.000095]\n",
      "train [Epoch 0/10] [Batch 1160/1173] [Iter 1160] [G_Loss 8.793211] [D_Loss 0.003157]\n",
      "train [Epoch 0/10] [Batch 1161/1173] [Iter 1161] [G_Loss 9.164660] [D_Loss 0.000151]\n",
      "train [Epoch 0/10] [Batch 1162/1173] [Iter 1162] [G_Loss 8.901550] [D_Loss 0.032432]\n",
      "train [Epoch 0/10] [Batch 1163/1173] [Iter 1163] [G_Loss 8.389331] [D_Loss 0.000405]\n",
      "train [Epoch 0/10] [Batch 1164/1173] [Iter 1164] [G_Loss 8.344934] [D_Loss 0.000317]\n",
      "train [Epoch 0/10] [Batch 1165/1173] [Iter 1165] [G_Loss 8.120896] [D_Loss 0.000701]\n",
      "train [Epoch 0/10] [Batch 1166/1173] [Iter 1166] [G_Loss 8.081496] [D_Loss 0.000406]\n",
      "train [Epoch 0/10] [Batch 1167/1173] [Iter 1167] [G_Loss 8.142255] [D_Loss 0.000533]\n",
      "train [Epoch 0/10] [Batch 1168/1173] [Iter 1168] [G_Loss 8.601495] [D_Loss 0.000247]\n",
      "train [Epoch 0/10] [Batch 1169/1173] [Iter 1169] [G_Loss 8.416198] [D_Loss 0.000284]\n",
      "train [Epoch 0/10] [Batch 1170/1173] [Iter 1170] [G_Loss 8.481613] [D_Loss 0.000306]\n",
      "train [Epoch 0/10] [Batch 1171/1173] [Iter 1171] [G_Loss 8.358215] [D_Loss 0.000355]\n",
      "train [Epoch 0/10] [Batch 1172/1173] [Iter 1172] [G_Loss 8.841711] [D_Loss 0.000288]\n",
      "val [Epoch 0/10] [Batch 0/31] [Iter 0] [G_Loss 8.739025] [D_Loss 0.000207]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 15.90 GiB total capacity; 15.09 GiB already allocated; 11.88 MiB free; 148.91 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a152329c3349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_gan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/cs231n-proj/src/scripts/train_gan.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(generator, discriminator, save)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalidation_rate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mval_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0msave_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../../outputs/gan/val_losses.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs231n-proj/src/scripts/train_gan.py\u001b[0m in \u001b[0;36mtrain_batches\u001b[0;34m(epoch, generator, discriminator, dataloader, train, save)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Generate images using the generator, find loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs231n-proj/src/scripts/train_gan.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleakyRelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleakyRelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleakyRelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    144\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    145\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     return torch.max_pool2d(\n\u001b[0;32m--> 494\u001b[0;31m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m max_pool2d = torch._jit_internal.boolean_dispatch(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 15.90 GiB total capacity; 15.09 GiB already allocated; 11.88 MiB free; 148.91 MiB cached)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_gan import *\n",
    "\n",
    "train_model(generator, discriminator, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.87s)\n",
      "creating index...\n",
      "index created!\n",
      "train [Epoch 0/10] [Batch 0/2345] [Iter 0] [G_Loss 0.987184] [D_Loss 1.406835]\n",
      "train [Epoch 0/10] [Batch 1/2345] [Iter 1] [G_Loss 51.995876] [D_Loss 0.095660]\n",
      "train [Epoch 0/10] [Batch 2/2345] [Iter 2] [G_Loss 4.838243] [D_Loss 11.466956]\n",
      "train [Epoch 0/10] [Batch 3/2345] [Iter 3] [G_Loss 94.425568] [D_Loss 35.702202]\n",
      "train [Epoch 0/10] [Batch 4/2345] [Iter 4] [G_Loss 40.008671] [D_Loss 6.659234]\n",
      "train [Epoch 0/10] [Batch 5/2345] [Iter 5] [G_Loss 4.465932] [D_Loss 1.476046]\n",
      "train [Epoch 0/10] [Batch 6/2345] [Iter 6] [G_Loss 1.738161] [D_Loss 2.731522]\n",
      "train [Epoch 0/10] [Batch 7/2345] [Iter 7] [G_Loss 15.739103] [D_Loss 0.527812]\n",
      "train [Epoch 0/10] [Batch 8/2345] [Iter 8] [G_Loss 17.393213] [D_Loss 1.801924]\n",
      "train [Epoch 0/10] [Batch 9/2345] [Iter 9] [G_Loss 8.765978] [D_Loss 0.346226]\n",
      "train [Epoch 0/10] [Batch 10/2345] [Iter 10] [G_Loss 5.073224] [D_Loss 0.568537]\n",
      "train [Epoch 0/10] [Batch 11/2345] [Iter 11] [G_Loss 4.906363] [D_Loss 0.345075]\n",
      "train [Epoch 0/10] [Batch 12/2345] [Iter 12] [G_Loss 5.693705] [D_Loss 0.232083]\n",
      "train [Epoch 0/10] [Batch 13/2345] [Iter 13] [G_Loss 5.716680] [D_Loss 0.672821]\n",
      "train [Epoch 0/10] [Batch 14/2345] [Iter 14] [G_Loss 5.486662] [D_Loss 0.482325]\n",
      "train [Epoch 0/10] [Batch 15/2345] [Iter 15] [G_Loss 4.683285] [D_Loss 0.417847]\n",
      "train [Epoch 0/10] [Batch 16/2345] [Iter 16] [G_Loss 4.972945] [D_Loss 0.835158]\n",
      "train [Epoch 0/10] [Batch 17/2345] [Iter 17] [G_Loss 5.261238] [D_Loss 0.597865]\n",
      "train [Epoch 0/10] [Batch 18/2345] [Iter 18] [G_Loss 2.935289] [D_Loss 0.577656]\n",
      "train [Epoch 0/10] [Batch 19/2345] [Iter 19] [G_Loss 3.372373] [D_Loss 0.896508]\n",
      "train [Epoch 0/10] [Batch 20/2345] [Iter 20] [G_Loss 2.551097] [D_Loss 0.793033]\n",
      "train [Epoch 0/10] [Batch 21/2345] [Iter 21] [G_Loss 3.526149] [D_Loss 0.691335]\n",
      "train [Epoch 0/10] [Batch 22/2345] [Iter 22] [G_Loss 1.182305] [D_Loss 0.892308]\n",
      "train [Epoch 0/10] [Batch 23/2345] [Iter 23] [G_Loss 3.452971] [D_Loss 0.874142]\n",
      "train [Epoch 0/10] [Batch 24/2345] [Iter 24] [G_Loss 3.080774] [D_Loss 0.674671]\n",
      "train [Epoch 0/10] [Batch 25/2345] [Iter 25] [G_Loss 1.901156] [D_Loss 0.502427]\n",
      "train [Epoch 0/10] [Batch 26/2345] [Iter 26] [G_Loss 1.797920] [D_Loss 0.936053]\n",
      "train [Epoch 0/10] [Batch 27/2345] [Iter 27] [G_Loss 1.850723] [D_Loss 0.498313]\n",
      "train [Epoch 0/10] [Batch 28/2345] [Iter 28] [G_Loss 2.609921] [D_Loss 0.638792]\n",
      "train [Epoch 0/10] [Batch 29/2345] [Iter 29] [G_Loss 2.213735] [D_Loss 0.556907]\n",
      "train [Epoch 0/10] [Batch 30/2345] [Iter 30] [G_Loss 2.233943] [D_Loss 0.622694]\n",
      "train [Epoch 0/10] [Batch 31/2345] [Iter 31] [G_Loss 2.130943] [D_Loss 0.450353]\n",
      "train [Epoch 0/10] [Batch 32/2345] [Iter 32] [G_Loss 2.597273] [D_Loss 0.489441]\n",
      "train [Epoch 0/10] [Batch 33/2345] [Iter 33] [G_Loss 1.889793] [D_Loss 0.541685]\n",
      "train [Epoch 0/10] [Batch 34/2345] [Iter 34] [G_Loss 5.043309] [D_Loss 0.618077]\n",
      "train [Epoch 0/10] [Batch 35/2345] [Iter 35] [G_Loss 2.113059] [D_Loss 0.591803]\n",
      "train [Epoch 0/10] [Batch 36/2345] [Iter 36] [G_Loss 3.813595] [D_Loss 0.541357]\n",
      "train [Epoch 0/10] [Batch 37/2345] [Iter 37] [G_Loss 2.707597] [D_Loss 0.454042]\n",
      "train [Epoch 0/10] [Batch 38/2345] [Iter 38] [G_Loss 2.737788] [D_Loss 0.464258]\n",
      "train [Epoch 0/10] [Batch 39/2345] [Iter 39] [G_Loss 2.501041] [D_Loss 0.440273]\n",
      "train [Epoch 0/10] [Batch 40/2345] [Iter 40] [G_Loss 3.317083] [D_Loss 0.558665]\n",
      "train [Epoch 0/10] [Batch 41/2345] [Iter 41] [G_Loss 2.065743] [D_Loss 0.825592]\n",
      "train [Epoch 0/10] [Batch 42/2345] [Iter 42] [G_Loss 5.622511] [D_Loss 0.934115]\n",
      "train [Epoch 0/10] [Batch 43/2345] [Iter 43] [G_Loss 1.087542] [D_Loss 1.425926]\n",
      "train [Epoch 0/10] [Batch 44/2345] [Iter 44] [G_Loss 4.243993] [D_Loss 0.866193]\n",
      "train [Epoch 0/10] [Batch 45/2345] [Iter 45] [G_Loss 3.534935] [D_Loss 0.463438]\n",
      "train [Epoch 0/10] [Batch 46/2345] [Iter 46] [G_Loss 2.422054] [D_Loss 0.519082]\n",
      "train [Epoch 0/10] [Batch 47/2345] [Iter 47] [G_Loss 2.345169] [D_Loss 0.574148]\n",
      "train [Epoch 0/10] [Batch 48/2345] [Iter 48] [G_Loss 2.202339] [D_Loss 0.576162]\n",
      "train [Epoch 0/10] [Batch 49/2345] [Iter 49] [G_Loss 2.377196] [D_Loss 0.492217]\n",
      "train [Epoch 0/10] [Batch 50/2345] [Iter 50] [G_Loss 2.439236] [D_Loss 0.498123]\n",
      "train [Epoch 0/10] [Batch 51/2345] [Iter 51] [G_Loss 1.854260] [D_Loss 0.558199]\n",
      "train [Epoch 0/10] [Batch 52/2345] [Iter 52] [G_Loss 2.745821] [D_Loss 0.650232]\n",
      "train [Epoch 0/10] [Batch 53/2345] [Iter 53] [G_Loss 2.042698] [D_Loss 0.493379]\n",
      "train [Epoch 0/10] [Batch 54/2345] [Iter 54] [G_Loss 3.085011] [D_Loss 0.568630]\n",
      "train [Epoch 0/10] [Batch 55/2345] [Iter 55] [G_Loss 1.691916] [D_Loss 0.713941]\n",
      "train [Epoch 0/10] [Batch 56/2345] [Iter 56] [G_Loss 5.097907] [D_Loss 0.604905]\n",
      "train [Epoch 0/10] [Batch 57/2345] [Iter 57] [G_Loss 3.228074] [D_Loss 0.652940]\n",
      "train [Epoch 0/10] [Batch 58/2345] [Iter 58] [G_Loss 2.140631] [D_Loss 0.910674]\n",
      "train [Epoch 0/10] [Batch 59/2345] [Iter 59] [G_Loss 4.476811] [D_Loss 0.669371]\n",
      "train [Epoch 0/10] [Batch 60/2345] [Iter 60] [G_Loss 2.182896] [D_Loss 0.796596]\n",
      "train [Epoch 0/10] [Batch 61/2345] [Iter 61] [G_Loss 1.134285] [D_Loss 0.972028]\n",
      "train [Epoch 0/10] [Batch 62/2345] [Iter 62] [G_Loss 3.606194] [D_Loss 0.785496]\n",
      "train [Epoch 0/10] [Batch 63/2345] [Iter 63] [G_Loss 2.498786] [D_Loss 0.524642]\n",
      "train [Epoch 0/10] [Batch 64/2345] [Iter 64] [G_Loss 1.989403] [D_Loss 0.606981]\n",
      "train [Epoch 0/10] [Batch 65/2345] [Iter 65] [G_Loss 1.802288] [D_Loss 0.683964]\n",
      "train [Epoch 0/10] [Batch 66/2345] [Iter 66] [G_Loss 2.487312] [D_Loss 0.583634]\n",
      "train [Epoch 0/10] [Batch 67/2345] [Iter 67] [G_Loss 2.056360] [D_Loss 0.616164]\n",
      "train [Epoch 0/10] [Batch 68/2345] [Iter 68] [G_Loss 1.612853] [D_Loss 0.656469]\n",
      "train [Epoch 0/10] [Batch 69/2345] [Iter 69] [G_Loss 2.800396] [D_Loss 0.546345]\n",
      "train [Epoch 0/10] [Batch 70/2345] [Iter 70] [G_Loss 2.637573] [D_Loss 0.469750]\n",
      "train [Epoch 0/10] [Batch 71/2345] [Iter 71] [G_Loss 2.147703] [D_Loss 0.624322]\n",
      "train [Epoch 0/10] [Batch 72/2345] [Iter 72] [G_Loss 3.515370] [D_Loss 0.679588]\n",
      "train [Epoch 0/10] [Batch 73/2345] [Iter 73] [G_Loss 1.467956] [D_Loss 0.699022]\n",
      "train [Epoch 0/10] [Batch 74/2345] [Iter 74] [G_Loss 3.848436] [D_Loss 1.049337]\n",
      "train [Epoch 0/10] [Batch 75/2345] [Iter 75] [G_Loss 1.140463] [D_Loss 0.843240]\n",
      "train [Epoch 0/10] [Batch 76/2345] [Iter 76] [G_Loss 3.169274] [D_Loss 0.687468]\n",
      "train [Epoch 0/10] [Batch 77/2345] [Iter 77] [G_Loss 2.986452] [D_Loss 0.545941]\n",
      "train [Epoch 0/10] [Batch 78/2345] [Iter 78] [G_Loss 2.149983] [D_Loss 0.864112]\n",
      "train [Epoch 0/10] [Batch 79/2345] [Iter 79] [G_Loss 1.833740] [D_Loss 0.566902]\n",
      "train [Epoch 0/10] [Batch 80/2345] [Iter 80] [G_Loss 2.375446] [D_Loss 0.459857]\n",
      "train [Epoch 0/10] [Batch 81/2345] [Iter 81] [G_Loss 2.332821] [D_Loss 0.653681]\n",
      "train [Epoch 0/10] [Batch 82/2345] [Iter 82] [G_Loss 1.763703] [D_Loss 0.470323]\n",
      "train [Epoch 0/10] [Batch 83/2345] [Iter 83] [G_Loss 2.441684] [D_Loss 0.578872]\n",
      "train [Epoch 0/10] [Batch 84/2345] [Iter 84] [G_Loss 3.169505] [D_Loss 0.666283]\n",
      "train [Epoch 0/10] [Batch 85/2345] [Iter 85] [G_Loss 2.070823] [D_Loss 0.802934]\n",
      "train [Epoch 0/10] [Batch 86/2345] [Iter 86] [G_Loss 2.248907] [D_Loss 0.517138]\n",
      "train [Epoch 0/10] [Batch 87/2345] [Iter 87] [G_Loss 2.048362] [D_Loss 0.641801]\n",
      "train [Epoch 0/10] [Batch 88/2345] [Iter 88] [G_Loss 2.607187] [D_Loss 0.642771]\n",
      "train [Epoch 0/10] [Batch 89/2345] [Iter 89] [G_Loss 2.556970] [D_Loss 0.482903]\n",
      "train [Epoch 0/10] [Batch 90/2345] [Iter 90] [G_Loss 2.576738] [D_Loss 0.631720]\n",
      "train [Epoch 0/10] [Batch 91/2345] [Iter 91] [G_Loss 2.806764] [D_Loss 0.714092]\n",
      "train [Epoch 0/10] [Batch 92/2345] [Iter 92] [G_Loss 3.147630] [D_Loss 0.633668]\n",
      "train [Epoch 0/10] [Batch 93/2345] [Iter 93] [G_Loss 2.136791] [D_Loss 0.958353]\n",
      "train [Epoch 0/10] [Batch 94/2345] [Iter 94] [G_Loss 3.193679] [D_Loss 0.645481]\n",
      "train [Epoch 0/10] [Batch 95/2345] [Iter 95] [G_Loss 1.927089] [D_Loss 0.679974]\n",
      "train [Epoch 0/10] [Batch 96/2345] [Iter 96] [G_Loss 1.929779] [D_Loss 0.656738]\n",
      "train [Epoch 0/10] [Batch 97/2345] [Iter 97] [G_Loss 1.580783] [D_Loss 0.884436]\n",
      "train [Epoch 0/10] [Batch 98/2345] [Iter 98] [G_Loss 1.508271] [D_Loss 1.159232]\n",
      "train [Epoch 0/10] [Batch 99/2345] [Iter 99] [G_Loss 6.587679] [D_Loss 1.433269]\n",
      "train [Epoch 0/10] [Batch 100/2345] [Iter 100] [G_Loss 1.704192] [D_Loss 0.968593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 101/2345] [Iter 101] [G_Loss 1.085752] [D_Loss 0.844079]\n",
      "train [Epoch 0/10] [Batch 102/2345] [Iter 102] [G_Loss 1.307188] [D_Loss 1.164947]\n",
      "train [Epoch 0/10] [Batch 103/2345] [Iter 103] [G_Loss 0.636037] [D_Loss 1.652486]\n",
      "train [Epoch 0/10] [Batch 104/2345] [Iter 104] [G_Loss 2.849054] [D_Loss 0.761956]\n",
      "train [Epoch 0/10] [Batch 105/2345] [Iter 105] [G_Loss 2.611170] [D_Loss 0.766232]\n",
      "train [Epoch 0/10] [Batch 106/2345] [Iter 106] [G_Loss 1.534588] [D_Loss 0.949279]\n",
      "train [Epoch 0/10] [Batch 107/2345] [Iter 107] [G_Loss 2.070915] [D_Loss 0.679532]\n",
      "train [Epoch 0/10] [Batch 108/2345] [Iter 108] [G_Loss 2.255796] [D_Loss 0.768639]\n",
      "train [Epoch 0/10] [Batch 109/2345] [Iter 109] [G_Loss 1.656849] [D_Loss 0.679134]\n",
      "train [Epoch 0/10] [Batch 110/2345] [Iter 110] [G_Loss 1.659516] [D_Loss 0.669007]\n",
      "train [Epoch 0/10] [Batch 111/2345] [Iter 111] [G_Loss 1.560882] [D_Loss 0.743038]\n",
      "train [Epoch 0/10] [Batch 112/2345] [Iter 112] [G_Loss 1.563963] [D_Loss 0.647347]\n",
      "train [Epoch 0/10] [Batch 113/2345] [Iter 113] [G_Loss 1.273588] [D_Loss 0.797556]\n",
      "train [Epoch 0/10] [Batch 114/2345] [Iter 114] [G_Loss 2.823316] [D_Loss 0.760053]\n",
      "train [Epoch 0/10] [Batch 115/2345] [Iter 115] [G_Loss 1.896545] [D_Loss 0.808166]\n",
      "train [Epoch 0/10] [Batch 116/2345] [Iter 116] [G_Loss 2.464791] [D_Loss 0.561243]\n",
      "train [Epoch 0/10] [Batch 117/2345] [Iter 117] [G_Loss 3.032380] [D_Loss 0.576176]\n",
      "train [Epoch 0/10] [Batch 118/2345] [Iter 118] [G_Loss 2.425960] [D_Loss 0.778255]\n",
      "train [Epoch 0/10] [Batch 119/2345] [Iter 119] [G_Loss 2.970179] [D_Loss 0.633914]\n",
      "train [Epoch 0/10] [Batch 120/2345] [Iter 120] [G_Loss 1.535788] [D_Loss 1.063524]\n",
      "train [Epoch 0/10] [Batch 121/2345] [Iter 121] [G_Loss 7.122699] [D_Loss 1.227064]\n",
      "train [Epoch 0/10] [Batch 122/2345] [Iter 122] [G_Loss 2.701071] [D_Loss 0.600825]\n",
      "train [Epoch 0/10] [Batch 123/2345] [Iter 123] [G_Loss 1.878181] [D_Loss 0.660761]\n",
      "train [Epoch 0/10] [Batch 124/2345] [Iter 124] [G_Loss 2.059456] [D_Loss 0.510598]\n",
      "train [Epoch 0/10] [Batch 125/2345] [Iter 125] [G_Loss 2.102433] [D_Loss 0.665779]\n",
      "train [Epoch 0/10] [Batch 126/2345] [Iter 126] [G_Loss 2.156286] [D_Loss 0.663747]\n",
      "train [Epoch 0/10] [Batch 127/2345] [Iter 127] [G_Loss 2.275440] [D_Loss 0.647004]\n",
      "train [Epoch 0/10] [Batch 128/2345] [Iter 128] [G_Loss 1.737592] [D_Loss 0.567265]\n",
      "train [Epoch 0/10] [Batch 129/2345] [Iter 129] [G_Loss 1.565364] [D_Loss 0.593806]\n",
      "train [Epoch 0/10] [Batch 130/2345] [Iter 130] [G_Loss 1.990948] [D_Loss 0.473944]\n",
      "train [Epoch 0/10] [Batch 131/2345] [Iter 131] [G_Loss 1.757720] [D_Loss 0.613014]\n",
      "train [Epoch 0/10] [Batch 132/2345] [Iter 132] [G_Loss 1.889439] [D_Loss 0.506210]\n",
      "train [Epoch 0/10] [Batch 133/2345] [Iter 133] [G_Loss 3.553859] [D_Loss 0.639631]\n",
      "train [Epoch 0/10] [Batch 134/2345] [Iter 134] [G_Loss 2.559664] [D_Loss 0.624150]\n",
      "train [Epoch 0/10] [Batch 135/2345] [Iter 135] [G_Loss 3.421448] [D_Loss 0.909798]\n",
      "train [Epoch 0/10] [Batch 136/2345] [Iter 136] [G_Loss 2.209249] [D_Loss 0.985983]\n",
      "train [Epoch 0/10] [Batch 137/2345] [Iter 137] [G_Loss 2.410108] [D_Loss 0.645727]\n",
      "train [Epoch 0/10] [Batch 138/2345] [Iter 138] [G_Loss 2.307066] [D_Loss 0.691784]\n",
      "train [Epoch 0/10] [Batch 139/2345] [Iter 139] [G_Loss 1.699835] [D_Loss 0.516960]\n",
      "train [Epoch 0/10] [Batch 140/2345] [Iter 140] [G_Loss 2.444669] [D_Loss 0.788572]\n",
      "train [Epoch 0/10] [Batch 141/2345] [Iter 141] [G_Loss 2.617069] [D_Loss 0.632051]\n",
      "train [Epoch 0/10] [Batch 142/2345] [Iter 142] [G_Loss 2.233972] [D_Loss 1.003550]\n",
      "train [Epoch 0/10] [Batch 143/2345] [Iter 143] [G_Loss 2.364070] [D_Loss 0.846324]\n",
      "train [Epoch 0/10] [Batch 144/2345] [Iter 144] [G_Loss 1.203174] [D_Loss 0.955132]\n",
      "train [Epoch 0/10] [Batch 145/2345] [Iter 145] [G_Loss 1.875437] [D_Loss 0.869134]\n",
      "train [Epoch 0/10] [Batch 146/2345] [Iter 146] [G_Loss 1.204003] [D_Loss 0.827465]\n",
      "train [Epoch 0/10] [Batch 147/2345] [Iter 147] [G_Loss 1.658459] [D_Loss 0.826377]\n",
      "train [Epoch 0/10] [Batch 148/2345] [Iter 148] [G_Loss 1.690584] [D_Loss 0.744017]\n",
      "train [Epoch 0/10] [Batch 149/2345] [Iter 149] [G_Loss 1.512250] [D_Loss 0.700400]\n",
      "train [Epoch 0/10] [Batch 150/2345] [Iter 150] [G_Loss 2.447492] [D_Loss 0.810104]\n",
      "train [Epoch 0/10] [Batch 151/2345] [Iter 151] [G_Loss 1.257944] [D_Loss 0.900997]\n",
      "train [Epoch 0/10] [Batch 152/2345] [Iter 152] [G_Loss 1.731853] [D_Loss 0.608351]\n",
      "train [Epoch 0/10] [Batch 153/2345] [Iter 153] [G_Loss 1.917740] [D_Loss 0.600584]\n",
      "train [Epoch 0/10] [Batch 154/2345] [Iter 154] [G_Loss 1.628592] [D_Loss 0.658932]\n",
      "train [Epoch 0/10] [Batch 155/2345] [Iter 155] [G_Loss 2.706036] [D_Loss 0.808407]\n",
      "train [Epoch 0/10] [Batch 156/2345] [Iter 156] [G_Loss 2.245545] [D_Loss 0.552873]\n",
      "train [Epoch 0/10] [Batch 157/2345] [Iter 157] [G_Loss 2.598806] [D_Loss 0.609666]\n",
      "train [Epoch 0/10] [Batch 158/2345] [Iter 158] [G_Loss 2.069127] [D_Loss 0.610584]\n",
      "train [Epoch 0/10] [Batch 159/2345] [Iter 159] [G_Loss 3.770587] [D_Loss 0.659785]\n",
      "train [Epoch 0/10] [Batch 160/2345] [Iter 160] [G_Loss 2.152580] [D_Loss 0.450719]\n",
      "train [Epoch 0/10] [Batch 161/2345] [Iter 161] [G_Loss 2.236203] [D_Loss 0.540823]\n",
      "train [Epoch 0/10] [Batch 162/2345] [Iter 162] [G_Loss 2.657971] [D_Loss 0.493948]\n",
      "train [Epoch 0/10] [Batch 163/2345] [Iter 163] [G_Loss 2.959877] [D_Loss 0.457126]\n",
      "train [Epoch 0/10] [Batch 164/2345] [Iter 164] [G_Loss 2.913591] [D_Loss 0.601204]\n",
      "train [Epoch 0/10] [Batch 166/2345] [Iter 166] [G_Loss 1.695883] [D_Loss 1.281701]\n",
      "train [Epoch 0/10] [Batch 167/2345] [Iter 167] [G_Loss 1.371941] [D_Loss 0.770693]\n",
      "train [Epoch 0/10] [Batch 168/2345] [Iter 168] [G_Loss 2.363436] [D_Loss 0.834454]\n",
      "train [Epoch 0/10] [Batch 169/2345] [Iter 169] [G_Loss 2.395161] [D_Loss 0.774960]\n",
      "train [Epoch 0/10] [Batch 170/2345] [Iter 170] [G_Loss 1.847029] [D_Loss 0.735437]\n",
      "train [Epoch 0/10] [Batch 171/2345] [Iter 171] [G_Loss 1.780461] [D_Loss 0.665279]\n",
      "train [Epoch 0/10] [Batch 172/2345] [Iter 172] [G_Loss 1.521037] [D_Loss 0.964627]\n",
      "train [Epoch 0/10] [Batch 173/2345] [Iter 173] [G_Loss 1.537826] [D_Loss 0.802894]\n",
      "train [Epoch 0/10] [Batch 174/2345] [Iter 174] [G_Loss 1.506348] [D_Loss 0.721032]\n",
      "train [Epoch 0/10] [Batch 175/2345] [Iter 175] [G_Loss 1.597777] [D_Loss 0.653601]\n",
      "train [Epoch 0/10] [Batch 176/2345] [Iter 176] [G_Loss 1.345202] [D_Loss 0.893704]\n",
      "train [Epoch 0/10] [Batch 177/2345] [Iter 177] [G_Loss 1.936923] [D_Loss 0.758321]\n",
      "train [Epoch 0/10] [Batch 178/2345] [Iter 178] [G_Loss 0.914706] [D_Loss 0.941160]\n",
      "train [Epoch 0/10] [Batch 179/2345] [Iter 179] [G_Loss 2.621919] [D_Loss 0.715793]\n",
      "train [Epoch 0/10] [Batch 180/2345] [Iter 180] [G_Loss 2.150381] [D_Loss 0.792955]\n",
      "train [Epoch 0/10] [Batch 181/2345] [Iter 181] [G_Loss 2.768841] [D_Loss 0.781747]\n",
      "train [Epoch 0/10] [Batch 182/2345] [Iter 182] [G_Loss 1.636154] [D_Loss 0.905120]\n",
      "train [Epoch 0/10] [Batch 183/2345] [Iter 183] [G_Loss 1.968100] [D_Loss 0.943241]\n",
      "train [Epoch 0/10] [Batch 184/2345] [Iter 184] [G_Loss 0.934749] [D_Loss 0.955933]\n",
      "train [Epoch 0/10] [Batch 185/2345] [Iter 185] [G_Loss 2.522652] [D_Loss 1.247755]\n",
      "train [Epoch 0/10] [Batch 186/2345] [Iter 186] [G_Loss 1.215426] [D_Loss 0.903524]\n",
      "train [Epoch 0/10] [Batch 187/2345] [Iter 187] [G_Loss 1.171132] [D_Loss 0.857845]\n",
      "train [Epoch 0/10] [Batch 188/2345] [Iter 188] [G_Loss 1.646012] [D_Loss 0.693351]\n",
      "train [Epoch 0/10] [Batch 189/2345] [Iter 189] [G_Loss 1.687255] [D_Loss 0.798733]\n",
      "train [Epoch 0/10] [Batch 190/2345] [Iter 190] [G_Loss 1.622331] [D_Loss 0.557090]\n",
      "train [Epoch 0/10] [Batch 191/2345] [Iter 191] [G_Loss 2.538739] [D_Loss 0.473958]\n",
      "train [Epoch 0/10] [Batch 192/2345] [Iter 192] [G_Loss 1.947882] [D_Loss 0.642831]\n",
      "train [Epoch 0/10] [Batch 193/2345] [Iter 193] [G_Loss 1.843472] [D_Loss 0.702015]\n",
      "train [Epoch 0/10] [Batch 194/2345] [Iter 194] [G_Loss 3.372481] [D_Loss 0.805723]\n",
      "train [Epoch 0/10] [Batch 195/2345] [Iter 195] [G_Loss 1.276217] [D_Loss 1.581724]\n",
      "train [Epoch 0/10] [Batch 196/2345] [Iter 196] [G_Loss 4.445293] [D_Loss 0.550985]\n",
      "train [Epoch 0/10] [Batch 197/2345] [Iter 197] [G_Loss 3.138423] [D_Loss 0.901342]\n",
      "train [Epoch 0/10] [Batch 198/2345] [Iter 198] [G_Loss 1.562253] [D_Loss 0.848036]\n",
      "train [Epoch 0/10] [Batch 199/2345] [Iter 199] [G_Loss 1.597110] [D_Loss 1.074443]\n",
      "train [Epoch 0/10] [Batch 200/2345] [Iter 200] [G_Loss 1.402521] [D_Loss 0.830477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 201/2345] [Iter 201] [G_Loss 1.832077] [D_Loss 0.775806]\n",
      "train [Epoch 0/10] [Batch 202/2345] [Iter 202] [G_Loss 1.515338] [D_Loss 0.609049]\n",
      "train [Epoch 0/10] [Batch 203/2345] [Iter 203] [G_Loss 1.308689] [D_Loss 0.636974]\n",
      "train [Epoch 0/10] [Batch 204/2345] [Iter 204] [G_Loss 1.595215] [D_Loss 0.611809]\n",
      "train [Epoch 0/10] [Batch 205/2345] [Iter 205] [G_Loss 1.436140] [D_Loss 0.734127]\n",
      "train [Epoch 0/10] [Batch 206/2345] [Iter 206] [G_Loss 2.330035] [D_Loss 0.568254]\n",
      "train [Epoch 0/10] [Batch 207/2345] [Iter 207] [G_Loss 2.135788] [D_Loss 0.486647]\n",
      "train [Epoch 0/10] [Batch 208/2345] [Iter 208] [G_Loss 3.574006] [D_Loss 0.549781]\n",
      "train [Epoch 0/10] [Batch 209/2345] [Iter 209] [G_Loss 2.412510] [D_Loss 0.451221]\n",
      "train [Epoch 0/10] [Batch 210/2345] [Iter 210] [G_Loss 2.377070] [D_Loss 0.809464]\n",
      "train [Epoch 0/10] [Batch 211/2345] [Iter 211] [G_Loss 3.303960] [D_Loss 0.622221]\n",
      "train [Epoch 0/10] [Batch 212/2345] [Iter 212] [G_Loss 1.250336] [D_Loss 0.743313]\n",
      "train [Epoch 0/10] [Batch 213/2345] [Iter 213] [G_Loss 3.812486] [D_Loss 0.683608]\n",
      "train [Epoch 0/10] [Batch 214/2345] [Iter 214] [G_Loss 1.373163] [D_Loss 0.801828]\n",
      "train [Epoch 0/10] [Batch 215/2345] [Iter 215] [G_Loss 1.315560] [D_Loss 0.752724]\n",
      "train [Epoch 0/10] [Batch 216/2345] [Iter 216] [G_Loss 1.574929] [D_Loss 0.675704]\n",
      "train [Epoch 0/10] [Batch 217/2345] [Iter 217] [G_Loss 2.492531] [D_Loss 1.022558]\n",
      "train [Epoch 0/10] [Batch 218/2345] [Iter 218] [G_Loss 2.488683] [D_Loss 1.014963]\n",
      "train [Epoch 0/10] [Batch 219/2345] [Iter 219] [G_Loss 1.649625] [D_Loss 0.872239]\n",
      "train [Epoch 0/10] [Batch 220/2345] [Iter 220] [G_Loss 1.478092] [D_Loss 0.842073]\n",
      "train [Epoch 0/10] [Batch 221/2345] [Iter 221] [G_Loss 1.370414] [D_Loss 0.684873]\n",
      "train [Epoch 0/10] [Batch 222/2345] [Iter 222] [G_Loss 1.643306] [D_Loss 0.634246]\n",
      "train [Epoch 0/10] [Batch 223/2345] [Iter 223] [G_Loss 2.437113] [D_Loss 0.671988]\n",
      "train [Epoch 0/10] [Batch 224/2345] [Iter 224] [G_Loss 2.489337] [D_Loss 0.695348]\n",
      "train [Epoch 0/10] [Batch 225/2345] [Iter 225] [G_Loss 1.967813] [D_Loss 0.664807]\n",
      "train [Epoch 0/10] [Batch 226/2345] [Iter 226] [G_Loss 2.446139] [D_Loss 0.548347]\n",
      "train [Epoch 0/10] [Batch 227/2345] [Iter 227] [G_Loss 1.492437] [D_Loss 0.704930]\n",
      "train [Epoch 0/10] [Batch 228/2345] [Iter 228] [G_Loss 2.015138] [D_Loss 0.676189]\n",
      "train [Epoch 0/10] [Batch 229/2345] [Iter 229] [G_Loss 1.655938] [D_Loss 0.482010]\n",
      "train [Epoch 0/10] [Batch 230/2345] [Iter 230] [G_Loss 1.377312] [D_Loss 0.575129]\n",
      "train [Epoch 0/10] [Batch 231/2345] [Iter 231] [G_Loss 1.713536] [D_Loss 0.607645]\n",
      "train [Epoch 0/10] [Batch 232/2345] [Iter 232] [G_Loss 1.935130] [D_Loss 0.564168]\n",
      "train [Epoch 0/10] [Batch 233/2345] [Iter 233] [G_Loss 1.803655] [D_Loss 1.222155]\n",
      "train [Epoch 0/10] [Batch 234/2345] [Iter 234] [G_Loss 3.530670] [D_Loss 0.943553]\n",
      "train [Epoch 0/10] [Batch 235/2345] [Iter 235] [G_Loss 0.892580] [D_Loss 0.912526]\n",
      "train [Epoch 0/10] [Batch 236/2345] [Iter 236] [G_Loss 3.208797] [D_Loss 1.459996]\n",
      "train [Epoch 0/10] [Batch 237/2345] [Iter 237] [G_Loss 1.051967] [D_Loss 1.099982]\n",
      "train [Epoch 0/10] [Batch 238/2345] [Iter 238] [G_Loss 0.888178] [D_Loss 1.004719]\n",
      "train [Epoch 0/10] [Batch 239/2345] [Iter 239] [G_Loss 2.046325] [D_Loss 1.168730]\n",
      "train [Epoch 0/10] [Batch 240/2345] [Iter 240] [G_Loss 0.916218] [D_Loss 0.892427]\n",
      "train [Epoch 0/10] [Batch 241/2345] [Iter 241] [G_Loss 1.059620] [D_Loss 1.419974]\n",
      "train [Epoch 0/10] [Batch 242/2345] [Iter 242] [G_Loss 2.097242] [D_Loss 0.914346]\n",
      "train [Epoch 0/10] [Batch 243/2345] [Iter 243] [G_Loss 1.791301] [D_Loss 0.599649]\n",
      "train [Epoch 0/10] [Batch 244/2345] [Iter 244] [G_Loss 2.220646] [D_Loss 0.429959]\n",
      "train [Epoch 0/10] [Batch 245/2345] [Iter 245] [G_Loss 2.554283] [D_Loss 0.491091]\n",
      "train [Epoch 0/10] [Batch 246/2345] [Iter 246] [G_Loss 1.612908] [D_Loss 0.995827]\n",
      "train [Epoch 0/10] [Batch 247/2345] [Iter 247] [G_Loss 2.541911] [D_Loss 0.752809]\n",
      "train [Epoch 0/10] [Batch 248/2345] [Iter 248] [G_Loss 1.967966] [D_Loss 0.720654]\n",
      "train [Epoch 0/10] [Batch 249/2345] [Iter 249] [G_Loss 0.743182] [D_Loss 1.001577]\n",
      "train [Epoch 0/10] [Batch 250/2345] [Iter 250] [G_Loss 1.744954] [D_Loss 0.783986]\n",
      "train [Epoch 0/10] [Batch 251/2345] [Iter 251] [G_Loss 1.281708] [D_Loss 0.870507]\n",
      "train [Epoch 0/10] [Batch 252/2345] [Iter 252] [G_Loss 1.215206] [D_Loss 0.953014]\n",
      "train [Epoch 0/10] [Batch 253/2345] [Iter 253] [G_Loss 1.537964] [D_Loss 0.990132]\n",
      "train [Epoch 0/10] [Batch 254/2345] [Iter 254] [G_Loss 1.332525] [D_Loss 0.822878]\n",
      "train [Epoch 0/10] [Batch 255/2345] [Iter 255] [G_Loss 1.543962] [D_Loss 0.780928]\n",
      "train [Epoch 0/10] [Batch 256/2345] [Iter 256] [G_Loss 1.782634] [D_Loss 0.763402]\n",
      "train [Epoch 0/10] [Batch 257/2345] [Iter 257] [G_Loss 1.908043] [D_Loss 0.565171]\n",
      "train [Epoch 0/10] [Batch 258/2345] [Iter 258] [G_Loss 1.789881] [D_Loss 0.946962]\n",
      "train [Epoch 0/10] [Batch 259/2345] [Iter 259] [G_Loss 2.762130] [D_Loss 1.550906]\n",
      "train [Epoch 0/10] [Batch 260/2345] [Iter 260] [G_Loss 0.272890] [D_Loss 2.045277]\n",
      "train [Epoch 0/10] [Batch 261/2345] [Iter 261] [G_Loss 1.635063] [D_Loss 0.725793]\n",
      "train [Epoch 0/10] [Batch 262/2345] [Iter 262] [G_Loss 0.871614] [D_Loss 0.958666]\n",
      "train [Epoch 0/10] [Batch 263/2345] [Iter 263] [G_Loss 1.814228] [D_Loss 1.034986]\n",
      "train [Epoch 0/10] [Batch 264/2345] [Iter 264] [G_Loss 1.168424] [D_Loss 0.749964]\n",
      "train [Epoch 0/10] [Batch 265/2345] [Iter 265] [G_Loss 1.164029] [D_Loss 0.762447]\n",
      "train [Epoch 0/10] [Batch 266/2345] [Iter 266] [G_Loss 1.478861] [D_Loss 0.634262]\n",
      "train [Epoch 0/10] [Batch 267/2345] [Iter 267] [G_Loss 1.769819] [D_Loss 0.679840]\n",
      "train [Epoch 0/10] [Batch 268/2345] [Iter 268] [G_Loss 1.760012] [D_Loss 0.712595]\n",
      "train [Epoch 0/10] [Batch 269/2345] [Iter 269] [G_Loss 1.681704] [D_Loss 0.691745]\n",
      "train [Epoch 0/10] [Batch 270/2345] [Iter 270] [G_Loss 1.651674] [D_Loss 0.690285]\n",
      "train [Epoch 0/10] [Batch 271/2345] [Iter 271] [G_Loss 1.190895] [D_Loss 0.739416]\n",
      "train [Epoch 0/10] [Batch 272/2345] [Iter 272] [G_Loss 2.886365] [D_Loss 1.049472]\n",
      "train [Epoch 0/10] [Batch 273/2345] [Iter 273] [G_Loss 5.247231] [D_Loss 1.081395]\n",
      "train [Epoch 0/10] [Batch 274/2345] [Iter 274] [G_Loss 3.215505] [D_Loss 0.561287]\n",
      "train [Epoch 0/10] [Batch 275/2345] [Iter 275] [G_Loss 24.548595] [D_Loss 3.656608]\n",
      "train [Epoch 0/10] [Batch 276/2345] [Iter 276] [G_Loss 0.489703] [D_Loss 2.370447]\n",
      "train [Epoch 0/10] [Batch 277/2345] [Iter 277] [G_Loss 4.651306] [D_Loss 0.761864]\n",
      "train [Epoch 0/10] [Batch 278/2345] [Iter 278] [G_Loss 3.691336] [D_Loss 0.462526]\n",
      "train [Epoch 0/10] [Batch 279/2345] [Iter 279] [G_Loss 1.775521] [D_Loss 0.576067]\n",
      "train [Epoch 0/10] [Batch 280/2345] [Iter 280] [G_Loss 2.594200] [D_Loss 0.564689]\n",
      "train [Epoch 0/10] [Batch 281/2345] [Iter 281] [G_Loss 3.642196] [D_Loss 0.541092]\n",
      "train [Epoch 0/10] [Batch 282/2345] [Iter 282] [G_Loss 3.237149] [D_Loss 0.334444]\n",
      "train [Epoch 0/10] [Batch 283/2345] [Iter 283] [G_Loss 2.629919] [D_Loss 0.370175]\n",
      "train [Epoch 0/10] [Batch 284/2345] [Iter 284] [G_Loss 3.250173] [D_Loss 0.482962]\n",
      "train [Epoch 0/10] [Batch 285/2345] [Iter 285] [G_Loss 1.313670] [D_Loss 0.791494]\n",
      "train [Epoch 0/10] [Batch 286/2345] [Iter 286] [G_Loss 5.405901] [D_Loss 2.736325]\n",
      "train [Epoch 0/10] [Batch 287/2345] [Iter 287] [G_Loss 0.099690] [D_Loss 3.039041]\n",
      "train [Epoch 0/10] [Batch 288/2345] [Iter 288] [G_Loss 2.524122] [D_Loss 0.859690]\n",
      "train [Epoch 0/10] [Batch 289/2345] [Iter 289] [G_Loss 2.480544] [D_Loss 0.766666]\n",
      "train [Epoch 0/10] [Batch 290/2345] [Iter 290] [G_Loss 1.755378] [D_Loss 0.603512]\n",
      "train [Epoch 0/10] [Batch 291/2345] [Iter 291] [G_Loss 1.950904] [D_Loss 0.547177]\n",
      "train [Epoch 0/10] [Batch 292/2345] [Iter 292] [G_Loss 4.282646] [D_Loss 0.958552]\n",
      "train [Epoch 0/10] [Batch 293/2345] [Iter 293] [G_Loss 0.319931] [D_Loss 2.538620]\n",
      "train [Epoch 0/10] [Batch 294/2345] [Iter 294] [G_Loss 1.683988] [D_Loss 0.513704]\n",
      "train [Epoch 0/10] [Batch 295/2345] [Iter 295] [G_Loss 2.799392] [D_Loss 0.465056]\n",
      "train [Epoch 0/10] [Batch 296/2345] [Iter 296] [G_Loss 2.386303] [D_Loss 0.449873]\n",
      "train [Epoch 0/10] [Batch 297/2345] [Iter 297] [G_Loss 0.779719] [D_Loss 0.965755]\n",
      "train [Epoch 0/10] [Batch 298/2345] [Iter 298] [G_Loss 7.450002] [D_Loss 5.145976]\n",
      "train [Epoch 0/10] [Batch 299/2345] [Iter 299] [G_Loss 1.287130] [D_Loss 1.187548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 300/2345] [Iter 300] [G_Loss 0.874904] [D_Loss 1.025306]\n",
      "train [Epoch 0/10] [Batch 301/2345] [Iter 301] [G_Loss 0.791601] [D_Loss 1.050784]\n",
      "train [Epoch 0/10] [Batch 302/2345] [Iter 302] [G_Loss 0.913387] [D_Loss 1.007380]\n",
      "train [Epoch 0/10] [Batch 303/2345] [Iter 303] [G_Loss 1.136974] [D_Loss 0.887584]\n",
      "train [Epoch 0/10] [Batch 304/2345] [Iter 304] [G_Loss 1.366886] [D_Loss 0.830682]\n",
      "train [Epoch 0/10] [Batch 305/2345] [Iter 305] [G_Loss 1.174173] [D_Loss 0.785846]\n",
      "train [Epoch 0/10] [Batch 306/2345] [Iter 306] [G_Loss 4.978845] [D_Loss 2.066573]\n",
      "train [Epoch 0/10] [Batch 307/2345] [Iter 307] [G_Loss 0.056075] [D_Loss 4.346786]\n",
      "train [Epoch 0/10] [Batch 308/2345] [Iter 308] [G_Loss 1.220800] [D_Loss 0.856248]\n",
      "train [Epoch 0/10] [Batch 309/2345] [Iter 309] [G_Loss 7.666074] [D_Loss 1.217994]\n",
      "train [Epoch 0/10] [Batch 310/2345] [Iter 310] [G_Loss 1.437885] [D_Loss 1.336709]\n",
      "train [Epoch 0/10] [Batch 311/2345] [Iter 311] [G_Loss 0.932631] [D_Loss 0.924045]\n",
      "train [Epoch 0/10] [Batch 312/2345] [Iter 312] [G_Loss 1.844826] [D_Loss 0.800109]\n",
      "train [Epoch 0/10] [Batch 313/2345] [Iter 313] [G_Loss 2.586605] [D_Loss 0.606445]\n",
      "train [Epoch 0/10] [Batch 314/2345] [Iter 314] [G_Loss 3.112116] [D_Loss 0.666257]\n",
      "train [Epoch 0/10] [Batch 315/2345] [Iter 315] [G_Loss 2.318418] [D_Loss 0.546688]\n",
      "train [Epoch 0/10] [Batch 316/2345] [Iter 316] [G_Loss 2.485403] [D_Loss 0.473849]\n",
      "train [Epoch 0/10] [Batch 317/2345] [Iter 317] [G_Loss 2.658813] [D_Loss 0.647633]\n",
      "train [Epoch 0/10] [Batch 318/2345] [Iter 318] [G_Loss 2.263792] [D_Loss 0.834844]\n",
      "train [Epoch 0/10] [Batch 319/2345] [Iter 319] [G_Loss 1.172568] [D_Loss 0.669138]\n",
      "train [Epoch 0/10] [Batch 320/2345] [Iter 320] [G_Loss 1.030811] [D_Loss 0.852718]\n",
      "train [Epoch 0/10] [Batch 321/2345] [Iter 321] [G_Loss 1.807496] [D_Loss 0.820004]\n",
      "train [Epoch 0/10] [Batch 322/2345] [Iter 322] [G_Loss 1.544052] [D_Loss 0.767045]\n",
      "train [Epoch 0/10] [Batch 323/2345] [Iter 323] [G_Loss 1.763172] [D_Loss 0.726313]\n",
      "train [Epoch 0/10] [Batch 324/2345] [Iter 324] [G_Loss 2.536195] [D_Loss 0.607005]\n",
      "train [Epoch 0/10] [Batch 325/2345] [Iter 325] [G_Loss 2.524660] [D_Loss 0.772626]\n",
      "train [Epoch 0/10] [Batch 326/2345] [Iter 326] [G_Loss 3.146937] [D_Loss 0.532896]\n",
      "train [Epoch 0/10] [Batch 327/2345] [Iter 327] [G_Loss 1.441110] [D_Loss 0.896015]\n",
      "train [Epoch 0/10] [Batch 334/2345] [Iter 334] [G_Loss 1.780254] [D_Loss 0.764422]\n",
      "train [Epoch 0/10] [Batch 335/2345] [Iter 335] [G_Loss 2.126687] [D_Loss 0.625208]\n",
      "train [Epoch 0/10] [Batch 336/2345] [Iter 336] [G_Loss 1.290596] [D_Loss 0.833459]\n",
      "train [Epoch 0/10] [Batch 337/2345] [Iter 337] [G_Loss 1.856951] [D_Loss 0.887040]\n",
      "train [Epoch 0/10] [Batch 338/2345] [Iter 338] [G_Loss 0.743344] [D_Loss 1.269170]\n",
      "train [Epoch 0/10] [Batch 339/2345] [Iter 339] [G_Loss 1.558662] [D_Loss 0.890511]\n",
      "train [Epoch 0/10] [Batch 340/2345] [Iter 340] [G_Loss 1.486118] [D_Loss 1.085266]\n",
      "train [Epoch 0/10] [Batch 341/2345] [Iter 341] [G_Loss 0.930981] [D_Loss 0.898519]\n",
      "train [Epoch 0/10] [Batch 342/2345] [Iter 342] [G_Loss 1.692239] [D_Loss 0.798004]\n",
      "train [Epoch 0/10] [Batch 343/2345] [Iter 343] [G_Loss 0.927244] [D_Loss 0.906792]\n",
      "train [Epoch 0/10] [Batch 344/2345] [Iter 344] [G_Loss 1.400845] [D_Loss 0.717028]\n",
      "train [Epoch 0/10] [Batch 345/2345] [Iter 345] [G_Loss 1.241145] [D_Loss 0.779921]\n",
      "train [Epoch 0/10] [Batch 346/2345] [Iter 346] [G_Loss 1.161549] [D_Loss 0.973172]\n",
      "train [Epoch 0/10] [Batch 347/2345] [Iter 347] [G_Loss 1.179121] [D_Loss 0.867815]\n",
      "train [Epoch 0/10] [Batch 348/2345] [Iter 348] [G_Loss 1.748584] [D_Loss 0.900470]\n",
      "train [Epoch 0/10] [Batch 349/2345] [Iter 349] [G_Loss 1.183018] [D_Loss 0.811648]\n",
      "train [Epoch 0/10] [Batch 350/2345] [Iter 350] [G_Loss 1.069196] [D_Loss 1.042390]\n",
      "train [Epoch 0/10] [Batch 351/2345] [Iter 351] [G_Loss 1.447289] [D_Loss 1.256446]\n",
      "train [Epoch 0/10] [Batch 352/2345] [Iter 352] [G_Loss 1.348367] [D_Loss 0.902499]\n",
      "train [Epoch 0/10] [Batch 353/2345] [Iter 353] [G_Loss 0.979141] [D_Loss 0.951547]\n",
      "train [Epoch 0/10] [Batch 354/2345] [Iter 354] [G_Loss 1.186624] [D_Loss 0.987127]\n",
      "train [Epoch 0/10] [Batch 355/2345] [Iter 355] [G_Loss 1.187482] [D_Loss 0.919001]\n",
      "train [Epoch 0/10] [Batch 356/2345] [Iter 356] [G_Loss 1.116014] [D_Loss 0.844255]\n",
      "train [Epoch 0/10] [Batch 357/2345] [Iter 357] [G_Loss 1.661668] [D_Loss 0.793680]\n",
      "train [Epoch 0/10] [Batch 358/2345] [Iter 358] [G_Loss 1.081479] [D_Loss 1.030947]\n",
      "train [Epoch 0/10] [Batch 359/2345] [Iter 359] [G_Loss 1.176401] [D_Loss 0.881433]\n",
      "train [Epoch 0/10] [Batch 360/2345] [Iter 360] [G_Loss 1.864744] [D_Loss 1.030170]\n",
      "train [Epoch 0/10] [Batch 361/2345] [Iter 361] [G_Loss 0.574514] [D_Loss 1.415129]\n",
      "train [Epoch 0/10] [Batch 362/2345] [Iter 362] [G_Loss 1.994204] [D_Loss 1.046329]\n",
      "train [Epoch 0/10] [Batch 363/2345] [Iter 363] [G_Loss 1.132195] [D_Loss 0.805886]\n",
      "train [Epoch 0/10] [Batch 364/2345] [Iter 364] [G_Loss 0.822255] [D_Loss 1.035587]\n",
      "train [Epoch 0/10] [Batch 365/2345] [Iter 365] [G_Loss 1.444813] [D_Loss 0.995066]\n",
      "train [Epoch 0/10] [Batch 366/2345] [Iter 366] [G_Loss 1.736347] [D_Loss 0.925051]\n",
      "train [Epoch 0/10] [Batch 367/2345] [Iter 367] [G_Loss 1.443404] [D_Loss 0.757894]\n",
      "train [Epoch 0/10] [Batch 368/2345] [Iter 368] [G_Loss 1.793659] [D_Loss 1.009717]\n",
      "train [Epoch 0/10] [Batch 369/2345] [Iter 369] [G_Loss 1.418585] [D_Loss 0.880086]\n",
      "train [Epoch 0/10] [Batch 370/2345] [Iter 370] [G_Loss 1.077316] [D_Loss 1.234686]\n",
      "train [Epoch 0/10] [Batch 371/2345] [Iter 371] [G_Loss 1.054710] [D_Loss 1.032487]\n",
      "train [Epoch 0/10] [Batch 372/2345] [Iter 372] [G_Loss 1.212597] [D_Loss 0.980976]\n",
      "train [Epoch 0/10] [Batch 373/2345] [Iter 373] [G_Loss 0.959168] [D_Loss 0.926542]\n",
      "train [Epoch 0/10] [Batch 374/2345] [Iter 374] [G_Loss 1.210691] [D_Loss 0.631251]\n",
      "train [Epoch 0/10] [Batch 375/2345] [Iter 375] [G_Loss 1.718534] [D_Loss 0.502679]\n",
      "train [Epoch 0/10] [Batch 376/2345] [Iter 376] [G_Loss 1.219311] [D_Loss 0.815205]\n",
      "train [Epoch 0/10] [Batch 377/2345] [Iter 377] [G_Loss 1.345477] [D_Loss 0.720571]\n",
      "train [Epoch 0/10] [Batch 378/2345] [Iter 378] [G_Loss 1.154657] [D_Loss 1.080780]\n",
      "train [Epoch 0/10] [Batch 379/2345] [Iter 379] [G_Loss 1.445591] [D_Loss 1.242322]\n",
      "train [Epoch 0/10] [Batch 380/2345] [Iter 380] [G_Loss 1.521371] [D_Loss 0.728466]\n",
      "train [Epoch 0/10] [Batch 381/2345] [Iter 381] [G_Loss 1.474465] [D_Loss 0.826405]\n",
      "train [Epoch 0/10] [Batch 382/2345] [Iter 382] [G_Loss 1.566265] [D_Loss 0.759785]\n",
      "train [Epoch 0/10] [Batch 383/2345] [Iter 383] [G_Loss 1.710674] [D_Loss 1.190198]\n",
      "train [Epoch 0/10] [Batch 384/2345] [Iter 384] [G_Loss 1.717524] [D_Loss 1.031780]\n",
      "train [Epoch 0/10] [Batch 385/2345] [Iter 385] [G_Loss 1.224135] [D_Loss 0.797468]\n",
      "train [Epoch 0/10] [Batch 386/2345] [Iter 386] [G_Loss 0.998831] [D_Loss 0.986218]\n",
      "train [Epoch 0/10] [Batch 387/2345] [Iter 387] [G_Loss 1.433134] [D_Loss 0.749506]\n",
      "train [Epoch 0/10] [Batch 388/2345] [Iter 388] [G_Loss 1.313233] [D_Loss 0.592938]\n",
      "train [Epoch 0/10] [Batch 389/2345] [Iter 389] [G_Loss 1.770249] [D_Loss 0.728373]\n",
      "train [Epoch 0/10] [Batch 390/2345] [Iter 390] [G_Loss 1.346837] [D_Loss 0.751708]\n",
      "train [Epoch 0/10] [Batch 391/2345] [Iter 391] [G_Loss 1.405894] [D_Loss 0.774774]\n",
      "train [Epoch 0/10] [Batch 392/2345] [Iter 392] [G_Loss 1.697839] [D_Loss 1.108829]\n",
      "train [Epoch 0/10] [Batch 393/2345] [Iter 393] [G_Loss 1.261487] [D_Loss 1.095601]\n",
      "train [Epoch 0/10] [Batch 394/2345] [Iter 394] [G_Loss 1.109231] [D_Loss 1.166996]\n",
      "train [Epoch 0/10] [Batch 395/2345] [Iter 395] [G_Loss 0.854130] [D_Loss 1.584220]\n",
      "train [Epoch 0/10] [Batch 396/2345] [Iter 396] [G_Loss 1.700697] [D_Loss 1.347403]\n",
      "train [Epoch 0/10] [Batch 397/2345] [Iter 397] [G_Loss 2.390512] [D_Loss 0.804920]\n",
      "train [Epoch 0/10] [Batch 398/2345] [Iter 398] [G_Loss 1.562582] [D_Loss 0.942699]\n",
      "train [Epoch 0/10] [Batch 399/2345] [Iter 399] [G_Loss 1.147895] [D_Loss 1.126963]\n",
      "train [Epoch 0/10] [Batch 400/2345] [Iter 400] [G_Loss 1.452350] [D_Loss 1.222908]\n",
      "train [Epoch 0/10] [Batch 401/2345] [Iter 401] [G_Loss 1.444009] [D_Loss 1.370629]\n",
      "train [Epoch 0/10] [Batch 406/2345] [Iter 406] [G_Loss 0.773040] [D_Loss 1.183448]\n",
      "train [Epoch 0/10] [Batch 407/2345] [Iter 407] [G_Loss 0.932556] [D_Loss 1.031188]\n",
      "train [Epoch 0/10] [Batch 408/2345] [Iter 408] [G_Loss 1.086646] [D_Loss 1.115217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 409/2345] [Iter 409] [G_Loss 0.715435] [D_Loss 1.096937]\n",
      "train [Epoch 0/10] [Batch 410/2345] [Iter 410] [G_Loss 1.278187] [D_Loss 0.878397]\n",
      "train [Epoch 0/10] [Batch 411/2345] [Iter 411] [G_Loss 1.017664] [D_Loss 0.830646]\n",
      "train [Epoch 0/10] [Batch 412/2345] [Iter 412] [G_Loss 1.047711] [D_Loss 0.576602]\n",
      "train [Epoch 0/10] [Batch 413/2345] [Iter 413] [G_Loss 2.365165] [D_Loss 0.812524]\n",
      "train [Epoch 0/10] [Batch 414/2345] [Iter 414] [G_Loss 0.825357] [D_Loss 0.842292]\n",
      "train [Epoch 0/10] [Batch 415/2345] [Iter 415] [G_Loss 2.100225] [D_Loss 0.770430]\n",
      "train [Epoch 0/10] [Batch 416/2345] [Iter 416] [G_Loss 1.279527] [D_Loss 0.761932]\n",
      "train [Epoch 0/10] [Batch 417/2345] [Iter 417] [G_Loss 0.978602] [D_Loss 0.890756]\n",
      "train [Epoch 0/10] [Batch 418/2345] [Iter 418] [G_Loss 1.185473] [D_Loss 1.087427]\n",
      "train [Epoch 0/10] [Batch 419/2345] [Iter 419] [G_Loss 2.018323] [D_Loss 0.930967]\n",
      "train [Epoch 0/10] [Batch 420/2345] [Iter 420] [G_Loss 2.046839] [D_Loss 0.994781]\n",
      "train [Epoch 0/10] [Batch 421/2345] [Iter 421] [G_Loss 0.940960] [D_Loss 1.238821]\n",
      "train [Epoch 0/10] [Batch 422/2345] [Iter 422] [G_Loss 1.814764] [D_Loss 1.363828]\n",
      "train [Epoch 0/10] [Batch 423/2345] [Iter 423] [G_Loss 0.863349] [D_Loss 1.116492]\n",
      "train [Epoch 0/10] [Batch 424/2345] [Iter 424] [G_Loss 0.777737] [D_Loss 1.274461]\n",
      "train [Epoch 0/10] [Batch 425/2345] [Iter 425] [G_Loss 0.936857] [D_Loss 1.184431]\n",
      "train [Epoch 0/10] [Batch 426/2345] [Iter 426] [G_Loss 1.119632] [D_Loss 1.063399]\n",
      "train [Epoch 0/10] [Batch 427/2345] [Iter 427] [G_Loss 1.033938] [D_Loss 1.050004]\n",
      "train [Epoch 0/10] [Batch 428/2345] [Iter 428] [G_Loss 0.816569] [D_Loss 1.249202]\n",
      "train [Epoch 0/10] [Batch 429/2345] [Iter 429] [G_Loss 1.347224] [D_Loss 1.061077]\n",
      "train [Epoch 0/10] [Batch 430/2345] [Iter 430] [G_Loss 1.023553] [D_Loss 1.202486]\n",
      "train [Epoch 0/10] [Batch 431/2345] [Iter 431] [G_Loss 1.304393] [D_Loss 1.244063]\n",
      "train [Epoch 0/10] [Batch 432/2345] [Iter 432] [G_Loss 1.019248] [D_Loss 1.288918]\n",
      "train [Epoch 0/10] [Batch 433/2345] [Iter 433] [G_Loss 0.936245] [D_Loss 1.328909]\n",
      "train [Epoch 0/10] [Batch 434/2345] [Iter 434] [G_Loss 1.162746] [D_Loss 1.281430]\n",
      "train [Epoch 0/10] [Batch 435/2345] [Iter 435] [G_Loss 0.813155] [D_Loss 1.093027]\n",
      "train [Epoch 0/10] [Batch 436/2345] [Iter 436] [G_Loss 0.972200] [D_Loss 1.058314]\n",
      "train [Epoch 0/10] [Batch 437/2345] [Iter 437] [G_Loss 0.963354] [D_Loss 1.038118]\n",
      "train [Epoch 0/10] [Batch 438/2345] [Iter 438] [G_Loss 1.036738] [D_Loss 1.031008]\n",
      "train [Epoch 0/10] [Batch 439/2345] [Iter 439] [G_Loss 1.182886] [D_Loss 0.882765]\n",
      "train [Epoch 0/10] [Batch 440/2345] [Iter 440] [G_Loss 1.309498] [D_Loss 0.658636]\n",
      "train [Epoch 0/10] [Batch 441/2345] [Iter 441] [G_Loss 0.902622] [D_Loss 1.013941]\n",
      "train [Epoch 0/10] [Batch 448/2345] [Iter 448] [G_Loss 1.673228] [D_Loss 1.343114]\n",
      "train [Epoch 0/10] [Batch 449/2345] [Iter 449] [G_Loss 1.021019] [D_Loss 1.379820]\n",
      "train [Epoch 0/10] [Batch 450/2345] [Iter 450] [G_Loss 1.110520] [D_Loss 1.368112]\n",
      "train [Epoch 0/10] [Batch 451/2345] [Iter 451] [G_Loss 1.065683] [D_Loss 1.169572]\n",
      "train [Epoch 0/10] [Batch 452/2345] [Iter 452] [G_Loss 1.372911] [D_Loss 1.113098]\n",
      "train [Epoch 0/10] [Batch 453/2345] [Iter 453] [G_Loss 0.877773] [D_Loss 1.000946]\n",
      "train [Epoch 0/10] [Batch 454/2345] [Iter 454] [G_Loss 1.287931] [D_Loss 0.898075]\n",
      "train [Epoch 0/10] [Batch 455/2345] [Iter 455] [G_Loss 0.846768] [D_Loss 0.961942]\n",
      "train [Epoch 0/10] [Batch 456/2345] [Iter 456] [G_Loss 1.752653] [D_Loss 0.980212]\n",
      "train [Epoch 0/10] [Batch 457/2345] [Iter 457] [G_Loss 1.035798] [D_Loss 1.091966]\n",
      "train [Epoch 0/10] [Batch 458/2345] [Iter 458] [G_Loss 0.917051] [D_Loss 0.921587]\n",
      "train [Epoch 0/10] [Batch 459/2345] [Iter 459] [G_Loss 1.130031] [D_Loss 0.911317]\n",
      "train [Epoch 0/10] [Batch 460/2345] [Iter 460] [G_Loss 1.254546] [D_Loss 0.758751]\n",
      "train [Epoch 0/10] [Batch 461/2345] [Iter 461] [G_Loss 1.112226] [D_Loss 0.945678]\n",
      "train [Epoch 0/10] [Batch 462/2345] [Iter 462] [G_Loss 1.198642] [D_Loss 1.014572]\n",
      "train [Epoch 0/10] [Batch 463/2345] [Iter 463] [G_Loss 1.537911] [D_Loss 0.874769]\n",
      "train [Epoch 0/10] [Batch 464/2345] [Iter 464] [G_Loss 1.751812] [D_Loss 0.709622]\n",
      "train [Epoch 0/10] [Batch 465/2345] [Iter 465] [G_Loss 1.353878] [D_Loss 1.041961]\n",
      "train [Epoch 0/10] [Batch 466/2345] [Iter 466] [G_Loss 1.581354] [D_Loss 0.966379]\n",
      "train [Epoch 0/10] [Batch 467/2345] [Iter 467] [G_Loss 2.278995] [D_Loss 1.519062]\n",
      "train [Epoch 0/10] [Batch 468/2345] [Iter 468] [G_Loss 0.537146] [D_Loss 1.802330]\n",
      "train [Epoch 0/10] [Batch 469/2345] [Iter 469] [G_Loss 1.048577] [D_Loss 1.280306]\n",
      "train [Epoch 0/10] [Batch 470/2345] [Iter 470] [G_Loss 1.440778] [D_Loss 1.246694]\n",
      "train [Epoch 0/10] [Batch 471/2345] [Iter 471] [G_Loss 0.637115] [D_Loss 1.247318]\n",
      "train [Epoch 0/10] [Batch 472/2345] [Iter 472] [G_Loss 0.962177] [D_Loss 1.044007]\n",
      "train [Epoch 0/10] [Batch 473/2345] [Iter 473] [G_Loss 1.428749] [D_Loss 1.003637]\n",
      "train [Epoch 0/10] [Batch 474/2345] [Iter 474] [G_Loss 1.055992] [D_Loss 0.861409]\n",
      "train [Epoch 0/10] [Batch 475/2345] [Iter 475] [G_Loss 1.087072] [D_Loss 0.961123]\n",
      "train [Epoch 0/10] [Batch 476/2345] [Iter 476] [G_Loss 1.250815] [D_Loss 0.681031]\n",
      "train [Epoch 0/10] [Batch 477/2345] [Iter 477] [G_Loss 1.569215] [D_Loss 0.754126]\n",
      "train [Epoch 0/10] [Batch 478/2345] [Iter 478] [G_Loss 1.423826] [D_Loss 0.592884]\n",
      "train [Epoch 0/10] [Batch 479/2345] [Iter 479] [G_Loss 1.507463] [D_Loss 0.734683]\n",
      "train [Epoch 0/10] [Batch 480/2345] [Iter 480] [G_Loss 1.258250] [D_Loss 0.828976]\n",
      "train [Epoch 0/10] [Batch 481/2345] [Iter 481] [G_Loss 2.957034] [D_Loss 2.104439]\n",
      "train [Epoch 0/10] [Batch 482/2345] [Iter 482] [G_Loss 0.641939] [D_Loss 1.134515]\n",
      "train [Epoch 0/10] [Batch 483/2345] [Iter 483] [G_Loss 3.808224] [D_Loss 0.491720]\n",
      "train [Epoch 0/10] [Batch 484/2345] [Iter 484] [G_Loss 6.752700] [D_Loss 0.548588]\n",
      "train [Epoch 0/10] [Batch 485/2345] [Iter 485] [G_Loss 6.122915] [D_Loss 0.884344]\n",
      "train [Epoch 0/10] [Batch 486/2345] [Iter 486] [G_Loss 3.081504] [D_Loss 0.420084]\n",
      "train [Epoch 0/10] [Batch 487/2345] [Iter 487] [G_Loss 2.799988] [D_Loss 0.521471]\n",
      "train [Epoch 0/10] [Batch 488/2345] [Iter 488] [G_Loss 1.796098] [D_Loss 0.642247]\n",
      "train [Epoch 0/10] [Batch 489/2345] [Iter 489] [G_Loss 2.538821] [D_Loss 1.203046]\n",
      "train [Epoch 0/10] [Batch 490/2345] [Iter 490] [G_Loss 1.723503] [D_Loss 1.033884]\n",
      "train [Epoch 0/10] [Batch 491/2345] [Iter 491] [G_Loss 0.880884] [D_Loss 1.154078]\n",
      "train [Epoch 0/10] [Batch 492/2345] [Iter 492] [G_Loss 1.529330] [D_Loss 0.948976]\n",
      "train [Epoch 0/10] [Batch 493/2345] [Iter 493] [G_Loss 0.989878] [D_Loss 1.535292]\n",
      "train [Epoch 0/10] [Batch 494/2345] [Iter 494] [G_Loss 1.493078] [D_Loss 0.793316]\n",
      "train [Epoch 0/10] [Batch 495/2345] [Iter 495] [G_Loss 0.859563] [D_Loss 1.024598]\n",
      "train [Epoch 0/10] [Batch 496/2345] [Iter 496] [G_Loss 1.244026] [D_Loss 1.114038]\n",
      "train [Epoch 0/10] [Batch 497/2345] [Iter 497] [G_Loss 1.514320] [D_Loss 1.018885]\n",
      "train [Epoch 0/10] [Batch 498/2345] [Iter 498] [G_Loss 1.425233] [D_Loss 0.900787]\n",
      "train [Epoch 0/10] [Batch 499/2345] [Iter 499] [G_Loss 0.758365] [D_Loss 1.203335]\n",
      "train [Epoch 0/10] [Batch 500/2345] [Iter 500] [G_Loss 1.522267] [D_Loss 1.037014]\n",
      "train [Epoch 0/10] [Batch 501/2345] [Iter 501] [G_Loss 0.934539] [D_Loss 1.047403]\n",
      "train [Epoch 0/10] [Batch 502/2345] [Iter 502] [G_Loss 0.849097] [D_Loss 1.247910]\n",
      "train [Epoch 0/10] [Batch 503/2345] [Iter 503] [G_Loss 1.287364] [D_Loss 1.554868]\n",
      "train [Epoch 0/10] [Batch 504/2345] [Iter 504] [G_Loss 0.192149] [D_Loss 2.416483]\n",
      "train [Epoch 0/10] [Batch 505/2345] [Iter 505] [G_Loss 1.447345] [D_Loss 1.677626]\n",
      "train [Epoch 0/10] [Batch 506/2345] [Iter 506] [G_Loss 0.849824] [D_Loss 1.294204]\n",
      "train [Epoch 0/10] [Batch 507/2345] [Iter 507] [G_Loss 0.694508] [D_Loss 1.259487]\n",
      "train [Epoch 0/10] [Batch 508/2345] [Iter 508] [G_Loss 0.797738] [D_Loss 1.132962]\n",
      "train [Epoch 0/10] [Batch 509/2345] [Iter 509] [G_Loss 0.763222] [D_Loss 1.288253]\n",
      "train [Epoch 0/10] [Batch 510/2345] [Iter 510] [G_Loss 0.885462] [D_Loss 1.395617]\n",
      "train [Epoch 0/10] [Batch 511/2345] [Iter 511] [G_Loss 1.028267] [D_Loss 1.458323]\n",
      "train [Epoch 0/10] [Batch 512/2345] [Iter 512] [G_Loss 0.853875] [D_Loss 1.307662]\n",
      "train [Epoch 0/10] [Batch 513/2345] [Iter 513] [G_Loss 0.815809] [D_Loss 1.265077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 514/2345] [Iter 514] [G_Loss 0.751123] [D_Loss 1.230248]\n",
      "train [Epoch 0/10] [Batch 515/2345] [Iter 515] [G_Loss 0.814231] [D_Loss 1.296842]\n",
      "train [Epoch 0/10] [Batch 516/2345] [Iter 516] [G_Loss 0.764218] [D_Loss 1.186557]\n",
      "train [Epoch 0/10] [Batch 517/2345] [Iter 517] [G_Loss 0.991719] [D_Loss 1.213651]\n",
      "train [Epoch 0/10] [Batch 518/2345] [Iter 518] [G_Loss 1.420190] [D_Loss 1.041630]\n",
      "train [Epoch 0/10] [Batch 519/2345] [Iter 519] [G_Loss 0.511149] [D_Loss 1.465096]\n",
      "train [Epoch 0/10] [Batch 520/2345] [Iter 520] [G_Loss 1.061451] [D_Loss 1.603263]\n",
      "train [Epoch 0/10] [Batch 521/2345] [Iter 521] [G_Loss 0.983141] [D_Loss 1.286313]\n",
      "train [Epoch 0/10] [Batch 522/2345] [Iter 522] [G_Loss 0.823443] [D_Loss 1.202710]\n",
      "train [Epoch 0/10] [Batch 523/2345] [Iter 523] [G_Loss 0.914082] [D_Loss 1.168693]\n",
      "train [Epoch 0/10] [Batch 524/2345] [Iter 524] [G_Loss 0.892516] [D_Loss 1.122005]\n",
      "train [Epoch 0/10] [Batch 525/2345] [Iter 525] [G_Loss 1.001151] [D_Loss 1.094724]\n",
      "train [Epoch 0/10] [Batch 526/2345] [Iter 526] [G_Loss 0.947977] [D_Loss 1.063141]\n",
      "train [Epoch 0/10] [Batch 527/2345] [Iter 527] [G_Loss 1.085463] [D_Loss 1.027105]\n",
      "train [Epoch 0/10] [Batch 528/2345] [Iter 528] [G_Loss 1.259629] [D_Loss 0.824796]\n",
      "train [Epoch 0/10] [Batch 529/2345] [Iter 529] [G_Loss 1.175952] [D_Loss 0.956114]\n",
      "train [Epoch 0/10] [Batch 530/2345] [Iter 530] [G_Loss 1.002601] [D_Loss 0.916878]\n",
      "train [Epoch 0/10] [Batch 531/2345] [Iter 531] [G_Loss 1.411946] [D_Loss 0.853590]\n",
      "train [Epoch 0/10] [Batch 532/2345] [Iter 532] [G_Loss 1.069659] [D_Loss 0.813340]\n",
      "train [Epoch 0/10] [Batch 533/2345] [Iter 533] [G_Loss 1.498124] [D_Loss 1.027402]\n",
      "train [Epoch 0/10] [Batch 534/2345] [Iter 534] [G_Loss 0.669473] [D_Loss 1.401112]\n",
      "train [Epoch 0/10] [Batch 535/2345] [Iter 535] [G_Loss 1.673956] [D_Loss 1.481185]\n",
      "train [Epoch 0/10] [Batch 536/2345] [Iter 536] [G_Loss 0.849591] [D_Loss 1.051478]\n",
      "train [Epoch 0/10] [Batch 537/2345] [Iter 537] [G_Loss 5.510334] [D_Loss 0.366923]\n",
      "train [Epoch 0/10] [Batch 538/2345] [Iter 538] [G_Loss 1.554580] [D_Loss 1.114392]\n",
      "train [Epoch 0/10] [Batch 539/2345] [Iter 539] [G_Loss 2.704192] [D_Loss 1.615719]\n",
      "train [Epoch 0/10] [Batch 540/2345] [Iter 540] [G_Loss 0.505640] [D_Loss 2.084200]\n",
      "train [Epoch 0/10] [Batch 541/2345] [Iter 541] [G_Loss 1.074394] [D_Loss 1.325062]\n",
      "train [Epoch 0/10] [Batch 542/2345] [Iter 542] [G_Loss 1.585035] [D_Loss 1.250915]\n",
      "train [Epoch 0/10] [Batch 543/2345] [Iter 543] [G_Loss 1.076760] [D_Loss 1.160841]\n",
      "train [Epoch 0/10] [Batch 544/2345] [Iter 544] [G_Loss 0.839194] [D_Loss 1.057781]\n",
      "train [Epoch 0/10] [Batch 545/2345] [Iter 545] [G_Loss 0.981075] [D_Loss 0.992840]\n",
      "train [Epoch 0/10] [Batch 546/2345] [Iter 546] [G_Loss 1.117477] [D_Loss 0.985239]\n",
      "train [Epoch 0/10] [Batch 547/2345] [Iter 547] [G_Loss 1.116894] [D_Loss 0.890368]\n",
      "train [Epoch 0/10] [Batch 548/2345] [Iter 548] [G_Loss 1.295384] [D_Loss 0.815091]\n",
      "train [Epoch 0/10] [Batch 549/2345] [Iter 549] [G_Loss 1.259069] [D_Loss 0.814683]\n",
      "train [Epoch 0/10] [Batch 550/2345] [Iter 550] [G_Loss 1.387189] [D_Loss 0.755134]\n",
      "train [Epoch 0/10] [Batch 551/2345] [Iter 551] [G_Loss 1.372314] [D_Loss 0.730129]\n",
      "train [Epoch 0/10] [Batch 552/2345] [Iter 552] [G_Loss 1.316358] [D_Loss 0.563411]\n",
      "train [Epoch 0/10] [Batch 553/2345] [Iter 553] [G_Loss 1.811327] [D_Loss 0.634404]\n",
      "train [Epoch 0/10] [Batch 554/2345] [Iter 554] [G_Loss 2.100603] [D_Loss 0.518520]\n",
      "train [Epoch 0/10] [Batch 555/2345] [Iter 555] [G_Loss 0.816775] [D_Loss 0.814098]\n",
      "train [Epoch 0/10] [Batch 556/2345] [Iter 556] [G_Loss 3.320460] [D_Loss 1.132409]\n",
      "train [Epoch 0/10] [Batch 557/2345] [Iter 557] [G_Loss 2.457296] [D_Loss 0.961279]\n",
      "train [Epoch 0/10] [Batch 558/2345] [Iter 558] [G_Loss 1.452599] [D_Loss 0.681543]\n",
      "train [Epoch 0/10] [Batch 559/2345] [Iter 559] [G_Loss 1.199563] [D_Loss 0.761734]\n",
      "train [Epoch 0/10] [Batch 560/2345] [Iter 560] [G_Loss 1.115992] [D_Loss 0.707651]\n",
      "train [Epoch 0/10] [Batch 561/2345] [Iter 561] [G_Loss 1.337080] [D_Loss 0.522731]\n",
      "train [Epoch 0/10] [Batch 562/2345] [Iter 562] [G_Loss 1.947648] [D_Loss 0.934803]\n",
      "train [Epoch 0/10] [Batch 563/2345] [Iter 563] [G_Loss 1.585410] [D_Loss 0.879233]\n",
      "train [Epoch 0/10] [Batch 564/2345] [Iter 564] [G_Loss 1.131341] [D_Loss 1.224645]\n",
      "train [Epoch 0/10] [Batch 565/2345] [Iter 565] [G_Loss 1.200471] [D_Loss 1.392544]\n",
      "train [Epoch 0/10] [Batch 566/2345] [Iter 566] [G_Loss 3.708329] [D_Loss 1.951151]\n",
      "train [Epoch 0/10] [Batch 567/2345] [Iter 567] [G_Loss 1.500662] [D_Loss 0.753528]\n",
      "train [Epoch 0/10] [Batch 568/2345] [Iter 568] [G_Loss 1.209339] [D_Loss 0.893656]\n",
      "train [Epoch 0/10] [Batch 569/2345] [Iter 569] [G_Loss 1.291426] [D_Loss 0.967607]\n",
      "train [Epoch 0/10] [Batch 570/2345] [Iter 570] [G_Loss 1.575265] [D_Loss 0.831023]\n",
      "train [Epoch 0/10] [Batch 571/2345] [Iter 571] [G_Loss 1.278112] [D_Loss 1.040293]\n",
      "train [Epoch 0/10] [Batch 572/2345] [Iter 572] [G_Loss 0.909191] [D_Loss 1.250626]\n",
      "train [Epoch 0/10] [Batch 573/2345] [Iter 573] [G_Loss 1.392483] [D_Loss 1.291277]\n",
      "train [Epoch 0/10] [Batch 574/2345] [Iter 574] [G_Loss 0.530224] [D_Loss 1.220747]\n",
      "train [Epoch 0/10] [Batch 575/2345] [Iter 575] [G_Loss 1.355957] [D_Loss 0.757790]\n",
      "train [Epoch 0/10] [Batch 576/2345] [Iter 576] [G_Loss 1.711716] [D_Loss 0.629816]\n",
      "train [Epoch 0/10] [Batch 577/2345] [Iter 577] [G_Loss 1.589442] [D_Loss 0.651693]\n",
      "train [Epoch 0/10] [Batch 578/2345] [Iter 578] [G_Loss 1.402871] [D_Loss 0.551285]\n",
      "train [Epoch 0/10] [Batch 579/2345] [Iter 579] [G_Loss 1.378057] [D_Loss 0.531193]\n",
      "train [Epoch 0/10] [Batch 580/2345] [Iter 580] [G_Loss 0.998306] [D_Loss 1.318752]\n",
      "train [Epoch 0/10] [Batch 581/2345] [Iter 581] [G_Loss 1.459730] [D_Loss 1.735626]\n",
      "train [Epoch 0/10] [Batch 582/2345] [Iter 582] [G_Loss 1.510174] [D_Loss 1.291736]\n",
      "train [Epoch 0/10] [Batch 583/2345] [Iter 583] [G_Loss 5.477306] [D_Loss 0.536543]\n",
      "train [Epoch 0/10] [Batch 584/2345] [Iter 584] [G_Loss 0.418557] [D_Loss 2.837588]\n",
      "train [Epoch 0/10] [Batch 585/2345] [Iter 585] [G_Loss 1.473907] [D_Loss 1.327209]\n",
      "train [Epoch 0/10] [Batch 586/2345] [Iter 586] [G_Loss 1.520212] [D_Loss 1.193080]\n",
      "train [Epoch 0/10] [Batch 587/2345] [Iter 587] [G_Loss 1.598723] [D_Loss 0.884001]\n",
      "train [Epoch 0/10] [Batch 588/2345] [Iter 588] [G_Loss 1.448154] [D_Loss 0.776999]\n",
      "train [Epoch 0/10] [Batch 589/2345] [Iter 589] [G_Loss 1.557052] [D_Loss 0.628526]\n",
      "train [Epoch 0/10] [Batch 590/2345] [Iter 590] [G_Loss 1.716930] [D_Loss 0.638510]\n",
      "train [Epoch 0/10] [Batch 591/2345] [Iter 591] [G_Loss 1.656722] [D_Loss 0.580125]\n",
      "train [Epoch 0/10] [Batch 592/2345] [Iter 592] [G_Loss 1.396109] [D_Loss 0.716938]\n",
      "train [Epoch 0/10] [Batch 593/2345] [Iter 593] [G_Loss 1.729268] [D_Loss 0.618231]\n",
      "train [Epoch 0/10] [Batch 594/2345] [Iter 594] [G_Loss 1.334752] [D_Loss 0.689731]\n",
      "train [Epoch 0/10] [Batch 595/2345] [Iter 595] [G_Loss 1.164413] [D_Loss 0.958447]\n",
      "train [Epoch 0/10] [Batch 596/2345] [Iter 596] [G_Loss 2.353202] [D_Loss 1.255722]\n",
      "train [Epoch 0/10] [Batch 597/2345] [Iter 597] [G_Loss 0.737327] [D_Loss 1.764558]\n",
      "train [Epoch 0/10] [Batch 598/2345] [Iter 598] [G_Loss 6.274525] [D_Loss 1.475722]\n",
      "train [Epoch 0/10] [Batch 599/2345] [Iter 599] [G_Loss 1.242087] [D_Loss 0.956629]\n",
      "train [Epoch 0/10] [Batch 600/2345] [Iter 600] [G_Loss 0.866440] [D_Loss 1.036320]\n",
      "train [Epoch 0/10] [Batch 601/2345] [Iter 601] [G_Loss 1.027557] [D_Loss 0.974937]\n",
      "train [Epoch 0/10] [Batch 602/2345] [Iter 602] [G_Loss 1.182039] [D_Loss 0.880672]\n",
      "train [Epoch 0/10] [Batch 603/2345] [Iter 603] [G_Loss 1.446644] [D_Loss 0.784994]\n",
      "train [Epoch 0/10] [Batch 604/2345] [Iter 604] [G_Loss 1.271726] [D_Loss 0.744791]\n",
      "train [Epoch 0/10] [Batch 605/2345] [Iter 605] [G_Loss 1.022043] [D_Loss 0.990510]\n",
      "train [Epoch 0/10] [Batch 606/2345] [Iter 606] [G_Loss 0.867174] [D_Loss 1.315075]\n",
      "train [Epoch 0/10] [Batch 607/2345] [Iter 607] [G_Loss 1.010579] [D_Loss 1.293155]\n",
      "train [Epoch 0/10] [Batch 608/2345] [Iter 608] [G_Loss 1.825864] [D_Loss 1.056462]\n",
      "train [Epoch 0/10] [Batch 609/2345] [Iter 609] [G_Loss 1.157292] [D_Loss 1.237339]\n",
      "train [Epoch 0/10] [Batch 610/2345] [Iter 610] [G_Loss 1.075543] [D_Loss 1.202684]\n",
      "train [Epoch 0/10] [Batch 611/2345] [Iter 611] [G_Loss 0.902605] [D_Loss 1.384771]\n",
      "train [Epoch 0/10] [Batch 612/2345] [Iter 612] [G_Loss 1.022691] [D_Loss 1.351011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 613/2345] [Iter 613] [G_Loss 1.401470] [D_Loss 1.073365]\n",
      "train [Epoch 0/10] [Batch 614/2345] [Iter 614] [G_Loss 0.808973] [D_Loss 1.210744]\n",
      "train [Epoch 0/10] [Batch 615/2345] [Iter 615] [G_Loss 1.162168] [D_Loss 1.330091]\n",
      "train [Epoch 0/10] [Batch 616/2345] [Iter 616] [G_Loss 1.146418] [D_Loss 1.163998]\n",
      "train [Epoch 0/10] [Batch 617/2345] [Iter 617] [G_Loss 1.179029] [D_Loss 1.158708]\n",
      "train [Epoch 0/10] [Batch 618/2345] [Iter 618] [G_Loss 0.730460] [D_Loss 1.178618]\n",
      "train [Epoch 0/10] [Batch 619/2345] [Iter 619] [G_Loss 0.903497] [D_Loss 1.036279]\n",
      "train [Epoch 0/10] [Batch 620/2345] [Iter 620] [G_Loss 1.212599] [D_Loss 1.016866]\n",
      "train [Epoch 0/10] [Batch 621/2345] [Iter 621] [G_Loss 0.990633] [D_Loss 0.980590]\n",
      "train [Epoch 0/10] [Batch 622/2345] [Iter 622] [G_Loss 0.939247] [D_Loss 0.936558]\n",
      "train [Epoch 0/10] [Batch 623/2345] [Iter 623] [G_Loss 1.203771] [D_Loss 0.735656]\n",
      "train [Epoch 0/10] [Batch 624/2345] [Iter 624] [G_Loss 1.704222] [D_Loss 0.965812]\n",
      "train [Epoch 0/10] [Batch 625/2345] [Iter 625] [G_Loss 0.964813] [D_Loss 0.785180]\n",
      "train [Epoch 0/10] [Batch 626/2345] [Iter 626] [G_Loss 1.406361] [D_Loss 0.779055]\n",
      "train [Epoch 0/10] [Batch 627/2345] [Iter 627] [G_Loss 1.595283] [D_Loss 0.655214]\n",
      "train [Epoch 0/10] [Batch 628/2345] [Iter 628] [G_Loss 1.329296] [D_Loss 0.802392]\n",
      "train [Epoch 0/10] [Batch 629/2345] [Iter 629] [G_Loss 1.230928] [D_Loss 1.003556]\n",
      "train [Epoch 0/10] [Batch 630/2345] [Iter 630] [G_Loss 0.388710] [D_Loss 2.531367]\n",
      "train [Epoch 0/10] [Batch 631/2345] [Iter 631] [G_Loss 3.746964] [D_Loss 2.428474]\n",
      "train [Epoch 0/10] [Batch 632/2345] [Iter 632] [G_Loss 0.990252] [D_Loss 1.438456]\n",
      "train [Epoch 0/10] [Batch 633/2345] [Iter 633] [G_Loss 0.559231] [D_Loss 1.481782]\n",
      "train [Epoch 0/10] [Batch 634/2345] [Iter 634] [G_Loss 0.576413] [D_Loss 1.364907]\n",
      "train [Epoch 0/10] [Batch 635/2345] [Iter 635] [G_Loss 0.746478] [D_Loss 1.263431]\n",
      "train [Epoch 0/10] [Batch 636/2345] [Iter 636] [G_Loss 0.901950] [D_Loss 1.250158]\n",
      "train [Epoch 0/10] [Batch 637/2345] [Iter 637] [G_Loss 0.964772] [D_Loss 1.142184]\n",
      "train [Epoch 0/10] [Batch 638/2345] [Iter 638] [G_Loss 0.932127] [D_Loss 1.077969]\n",
      "train [Epoch 0/10] [Batch 639/2345] [Iter 639] [G_Loss 1.220818] [D_Loss 0.948869]\n",
      "train [Epoch 0/10] [Batch 640/2345] [Iter 640] [G_Loss 1.153460] [D_Loss 0.913428]\n",
      "train [Epoch 0/10] [Batch 641/2345] [Iter 641] [G_Loss 0.999661] [D_Loss 0.893217]\n",
      "train [Epoch 0/10] [Batch 642/2345] [Iter 642] [G_Loss 1.504845] [D_Loss 1.009205]\n",
      "train [Epoch 0/10] [Batch 643/2345] [Iter 643] [G_Loss 0.761745] [D_Loss 1.207201]\n",
      "train [Epoch 0/10] [Batch 644/2345] [Iter 644] [G_Loss 1.307708] [D_Loss 1.207166]\n",
      "train [Epoch 0/10] [Batch 645/2345] [Iter 645] [G_Loss 0.811330] [D_Loss 1.105046]\n",
      "train [Epoch 0/10] [Batch 646/2345] [Iter 646] [G_Loss 0.897996] [D_Loss 1.222773]\n",
      "train [Epoch 0/10] [Batch 647/2345] [Iter 647] [G_Loss 0.933728] [D_Loss 1.319719]\n",
      "train [Epoch 0/10] [Batch 648/2345] [Iter 648] [G_Loss 1.168050] [D_Loss 1.205847]\n",
      "train [Epoch 0/10] [Batch 649/2345] [Iter 649] [G_Loss 0.756032] [D_Loss 1.038513]\n",
      "train [Epoch 0/10] [Batch 650/2345] [Iter 650] [G_Loss 1.060332] [D_Loss 1.004876]\n",
      "train [Epoch 0/10] [Batch 651/2345] [Iter 651] [G_Loss 1.346473] [D_Loss 1.088070]\n",
      "train [Epoch 0/10] [Batch 652/2345] [Iter 652] [G_Loss 1.052295] [D_Loss 0.902114]\n",
      "train [Epoch 0/10] [Batch 653/2345] [Iter 653] [G_Loss 1.074214] [D_Loss 0.862493]\n",
      "train [Epoch 0/10] [Batch 654/2345] [Iter 654] [G_Loss 1.333765] [D_Loss 0.948947]\n",
      "train [Epoch 0/10] [Batch 655/2345] [Iter 655] [G_Loss 1.110954] [D_Loss 0.865789]\n",
      "train [Epoch 0/10] [Batch 656/2345] [Iter 656] [G_Loss 1.247862] [D_Loss 0.717756]\n",
      "train [Epoch 0/10] [Batch 657/2345] [Iter 657] [G_Loss 1.540463] [D_Loss 0.802556]\n",
      "train [Epoch 0/10] [Batch 658/2345] [Iter 658] [G_Loss 1.890242] [D_Loss 0.803102]\n",
      "train [Epoch 0/10] [Batch 659/2345] [Iter 659] [G_Loss 0.925362] [D_Loss 1.007692]\n",
      "train [Epoch 0/10] [Batch 660/2345] [Iter 660] [G_Loss 1.078136] [D_Loss 1.186888]\n",
      "train [Epoch 0/10] [Batch 661/2345] [Iter 661] [G_Loss 1.355560] [D_Loss 1.445004]\n",
      "train [Epoch 0/10] [Batch 662/2345] [Iter 662] [G_Loss 1.121786] [D_Loss 1.154974]\n",
      "train [Epoch 0/10] [Batch 663/2345] [Iter 663] [G_Loss 1.317147] [D_Loss 1.357395]\n",
      "train [Epoch 0/10] [Batch 664/2345] [Iter 664] [G_Loss 1.288297] [D_Loss 1.612051]\n",
      "train [Epoch 0/10] [Batch 665/2345] [Iter 665] [G_Loss 0.815855] [D_Loss 1.572325]\n",
      "train [Epoch 0/10] [Batch 666/2345] [Iter 666] [G_Loss 1.172222] [D_Loss 1.317377]\n",
      "train [Epoch 0/10] [Batch 667/2345] [Iter 667] [G_Loss 0.841016] [D_Loss 1.420336]\n",
      "train [Epoch 0/10] [Batch 668/2345] [Iter 668] [G_Loss 0.840870] [D_Loss 1.426348]\n",
      "train [Epoch 0/10] [Batch 669/2345] [Iter 669] [G_Loss 0.896085] [D_Loss 1.450609]\n",
      "train [Epoch 0/10] [Batch 670/2345] [Iter 670] [G_Loss 0.733028] [D_Loss 1.798264]\n",
      "train [Epoch 0/10] [Batch 671/2345] [Iter 671] [G_Loss 0.872228] [D_Loss 1.465675]\n",
      "train [Epoch 0/10] [Batch 672/2345] [Iter 672] [G_Loss 1.077120] [D_Loss 1.151974]\n",
      "train [Epoch 0/10] [Batch 673/2345] [Iter 673] [G_Loss 0.826274] [D_Loss 1.260390]\n",
      "train [Epoch 0/10] [Batch 674/2345] [Iter 674] [G_Loss 1.068638] [D_Loss 1.014816]\n",
      "train [Epoch 0/10] [Batch 675/2345] [Iter 675] [G_Loss 1.094771] [D_Loss 1.068245]\n",
      "train [Epoch 0/10] [Batch 676/2345] [Iter 676] [G_Loss 1.137228] [D_Loss 0.979563]\n",
      "train [Epoch 0/10] [Batch 677/2345] [Iter 677] [G_Loss 1.540956] [D_Loss 1.113730]\n",
      "train [Epoch 0/10] [Batch 678/2345] [Iter 678] [G_Loss 0.806121] [D_Loss 1.142807]\n",
      "train [Epoch 0/10] [Batch 679/2345] [Iter 679] [G_Loss 1.072258] [D_Loss 1.247645]\n",
      "train [Epoch 0/10] [Batch 680/2345] [Iter 680] [G_Loss 0.963797] [D_Loss 1.350613]\n",
      "train [Epoch 0/10] [Batch 681/2345] [Iter 681] [G_Loss 0.893499] [D_Loss 1.223483]\n",
      "train [Epoch 0/10] [Batch 682/2345] [Iter 682] [G_Loss 0.934312] [D_Loss 1.296648]\n",
      "train [Epoch 0/10] [Batch 683/2345] [Iter 683] [G_Loss 0.678980] [D_Loss 1.175463]\n",
      "train [Epoch 0/10] [Batch 684/2345] [Iter 684] [G_Loss 1.159006] [D_Loss 1.060008]\n",
      "train [Epoch 0/10] [Batch 685/2345] [Iter 685] [G_Loss 1.136667] [D_Loss 0.888171]\n",
      "train [Epoch 0/10] [Batch 686/2345] [Iter 686] [G_Loss 0.818793] [D_Loss 0.859017]\n",
      "train [Epoch 0/10] [Batch 687/2345] [Iter 687] [G_Loss 1.854117] [D_Loss 0.934738]\n",
      "train [Epoch 0/10] [Batch 688/2345] [Iter 688] [G_Loss 0.837900] [D_Loss 0.864743]\n",
      "train [Epoch 0/10] [Batch 689/2345] [Iter 689] [G_Loss 1.820772] [D_Loss 0.856891]\n",
      "train [Epoch 0/10] [Batch 690/2345] [Iter 690] [G_Loss 1.373890] [D_Loss 0.596354]\n",
      "train [Epoch 0/10] [Batch 691/2345] [Iter 691] [G_Loss 1.074259] [D_Loss 0.734470]\n",
      "train [Epoch 0/10] [Batch 692/2345] [Iter 692] [G_Loss 1.978527] [D_Loss 0.744934]\n",
      "train [Epoch 0/10] [Batch 693/2345] [Iter 693] [G_Loss 1.280826] [D_Loss 0.996390]\n",
      "train [Epoch 0/10] [Batch 694/2345] [Iter 694] [G_Loss 1.460704] [D_Loss 0.837709]\n",
      "train [Epoch 0/10] [Batch 695/2345] [Iter 695] [G_Loss 1.415542] [D_Loss 1.037551]\n",
      "train [Epoch 0/10] [Batch 696/2345] [Iter 696] [G_Loss 1.760014] [D_Loss 0.673720]\n",
      "train [Epoch 0/10] [Batch 697/2345] [Iter 697] [G_Loss 1.262881] [D_Loss 1.333978]\n",
      "train [Epoch 0/10] [Batch 698/2345] [Iter 698] [G_Loss 2.079703] [D_Loss 1.453628]\n",
      "train [Epoch 0/10] [Batch 699/2345] [Iter 699] [G_Loss 0.590762] [D_Loss 1.587303]\n",
      "train [Epoch 0/10] [Batch 700/2345] [Iter 700] [G_Loss 0.957754] [D_Loss 1.395908]\n",
      "train [Epoch 0/10] [Batch 701/2345] [Iter 701] [G_Loss 0.994089] [D_Loss 1.446177]\n",
      "train [Epoch 0/10] [Batch 702/2345] [Iter 702] [G_Loss 1.144772] [D_Loss 1.117791]\n",
      "train [Epoch 0/10] [Batch 703/2345] [Iter 703] [G_Loss 1.086925] [D_Loss 1.105931]\n",
      "train [Epoch 0/10] [Batch 704/2345] [Iter 704] [G_Loss 0.793894] [D_Loss 1.148423]\n",
      "train [Epoch 0/10] [Batch 705/2345] [Iter 705] [G_Loss 0.821450] [D_Loss 1.203145]\n",
      "train [Epoch 0/10] [Batch 706/2345] [Iter 706] [G_Loss 0.852067] [D_Loss 1.365591]\n",
      "train [Epoch 0/10] [Batch 707/2345] [Iter 707] [G_Loss 1.101395] [D_Loss 1.188861]\n",
      "train [Epoch 0/10] [Batch 708/2345] [Iter 708] [G_Loss 0.842882] [D_Loss 1.143211]\n",
      "train [Epoch 0/10] [Batch 709/2345] [Iter 709] [G_Loss 0.967730] [D_Loss 0.997126]\n",
      "train [Epoch 0/10] [Batch 710/2345] [Iter 710] [G_Loss 1.459225] [D_Loss 0.772759]\n",
      "train [Epoch 0/10] [Batch 711/2345] [Iter 711] [G_Loss 1.529341] [D_Loss 0.858934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 712/2345] [Iter 712] [G_Loss 1.800008] [D_Loss 0.839441]\n",
      "train [Epoch 0/10] [Batch 713/2345] [Iter 713] [G_Loss 2.132183] [D_Loss 0.953459]\n",
      "train [Epoch 0/10] [Batch 714/2345] [Iter 714] [G_Loss 0.705946] [D_Loss 1.376787]\n",
      "train [Epoch 0/10] [Batch 715/2345] [Iter 715] [G_Loss 2.562547] [D_Loss 1.895815]\n",
      "train [Epoch 0/10] [Batch 716/2345] [Iter 716] [G_Loss 0.233521] [D_Loss 1.819247]\n",
      "train [Epoch 0/10] [Batch 717/2345] [Iter 717] [G_Loss 0.922546] [D_Loss 1.146595]\n",
      "train [Epoch 0/10] [Batch 718/2345] [Iter 718] [G_Loss 1.551358] [D_Loss 1.150194]\n",
      "train [Epoch 0/10] [Batch 719/2345] [Iter 719] [G_Loss 1.216470] [D_Loss 1.068645]\n",
      "train [Epoch 0/10] [Batch 720/2345] [Iter 720] [G_Loss 0.706039] [D_Loss 1.049095]\n",
      "train [Epoch 0/10] [Batch 721/2345] [Iter 721] [G_Loss 1.035904] [D_Loss 1.102333]\n",
      "train [Epoch 0/10] [Batch 722/2345] [Iter 722] [G_Loss 1.203747] [D_Loss 0.896204]\n",
      "train [Epoch 0/10] [Batch 723/2345] [Iter 723] [G_Loss 1.191809] [D_Loss 0.888555]\n",
      "train [Epoch 0/10] [Batch 724/2345] [Iter 724] [G_Loss 1.065803] [D_Loss 0.887224]\n",
      "train [Epoch 0/10] [Batch 725/2345] [Iter 725] [G_Loss 1.217071] [D_Loss 0.812565]\n",
      "train [Epoch 0/10] [Batch 726/2345] [Iter 726] [G_Loss 1.347669] [D_Loss 0.592431]\n",
      "train [Epoch 0/10] [Batch 727/2345] [Iter 727] [G_Loss 1.907598] [D_Loss 0.860875]\n",
      "train [Epoch 0/10] [Batch 728/2345] [Iter 728] [G_Loss 1.265052] [D_Loss 0.631067]\n",
      "train [Epoch 0/10] [Batch 729/2345] [Iter 729] [G_Loss 1.659341] [D_Loss 0.563593]\n",
      "train [Epoch 0/10] [Batch 730/2345] [Iter 730] [G_Loss 1.818184] [D_Loss 0.483676]\n",
      "train [Epoch 0/10] [Batch 731/2345] [Iter 731] [G_Loss 1.857285] [D_Loss 0.626090]\n",
      "train [Epoch 0/10] [Batch 732/2345] [Iter 732] [G_Loss 0.598399] [D_Loss 2.009577]\n",
      "train [Epoch 0/10] [Batch 733/2345] [Iter 733] [G_Loss 2.099783] [D_Loss 2.053036]\n",
      "train [Epoch 0/10] [Batch 734/2345] [Iter 734] [G_Loss 1.297201] [D_Loss 1.238121]\n",
      "train [Epoch 0/10] [Batch 735/2345] [Iter 735] [G_Loss 0.789216] [D_Loss 1.357589]\n",
      "train [Epoch 0/10] [Batch 736/2345] [Iter 736] [G_Loss 0.917142] [D_Loss 1.151808]\n",
      "train [Epoch 0/10] [Batch 737/2345] [Iter 737] [G_Loss 1.161413] [D_Loss 1.129322]\n",
      "train [Epoch 0/10] [Batch 738/2345] [Iter 738] [G_Loss 0.749100] [D_Loss 1.120238]\n",
      "train [Epoch 0/10] [Batch 739/2345] [Iter 739] [G_Loss 0.798202] [D_Loss 1.086466]\n",
      "train [Epoch 0/10] [Batch 740/2345] [Iter 740] [G_Loss 0.978177] [D_Loss 1.272065]\n",
      "train [Epoch 0/10] [Batch 741/2345] [Iter 741] [G_Loss 0.529746] [D_Loss 1.615553]\n",
      "train [Epoch 0/10] [Batch 742/2345] [Iter 742] [G_Loss 1.281846] [D_Loss 1.109807]\n",
      "train [Epoch 0/10] [Batch 743/2345] [Iter 743] [G_Loss 0.905523] [D_Loss 1.396250]\n",
      "train [Epoch 0/10] [Batch 744/2345] [Iter 744] [G_Loss 1.150207] [D_Loss 1.321072]\n",
      "train [Epoch 0/10] [Batch 745/2345] [Iter 745] [G_Loss 0.873649] [D_Loss 1.312899]\n",
      "train [Epoch 0/10] [Batch 746/2345] [Iter 746] [G_Loss 0.952256] [D_Loss 1.289888]\n",
      "train [Epoch 0/10] [Batch 747/2345] [Iter 747] [G_Loss 1.094422] [D_Loss 1.185071]\n",
      "train [Epoch 0/10] [Batch 748/2345] [Iter 748] [G_Loss 1.210848] [D_Loss 1.117224]\n",
      "train [Epoch 0/10] [Batch 749/2345] [Iter 749] [G_Loss 1.010655] [D_Loss 1.018389]\n",
      "train [Epoch 0/10] [Batch 750/2345] [Iter 750] [G_Loss 1.011009] [D_Loss 1.120764]\n",
      "train [Epoch 0/10] [Batch 751/2345] [Iter 751] [G_Loss 1.236092] [D_Loss 0.948711]\n",
      "train [Epoch 0/10] [Batch 752/2345] [Iter 752] [G_Loss 0.917413] [D_Loss 0.929979]\n",
      "train [Epoch 0/10] [Batch 753/2345] [Iter 753] [G_Loss 1.258044] [D_Loss 0.836977]\n",
      "train [Epoch 0/10] [Batch 754/2345] [Iter 754] [G_Loss 1.529685] [D_Loss 0.881103]\n",
      "train [Epoch 0/10] [Batch 755/2345] [Iter 755] [G_Loss 1.009992] [D_Loss 0.809390]\n",
      "train [Epoch 0/10] [Batch 756/2345] [Iter 756] [G_Loss 1.491641] [D_Loss 0.625236]\n",
      "train [Epoch 0/10] [Batch 757/2345] [Iter 757] [G_Loss 1.547752] [D_Loss 0.593608]\n",
      "train [Epoch 0/10] [Batch 758/2345] [Iter 758] [G_Loss 1.114792] [D_Loss 1.011852]\n",
      "train [Epoch 0/10] [Batch 759/2345] [Iter 759] [G_Loss 0.800768] [D_Loss 1.605801]\n",
      "train [Epoch 0/10] [Batch 760/2345] [Iter 760] [G_Loss 0.568870] [D_Loss 2.141204]\n",
      "train [Epoch 0/10] [Batch 761/2345] [Iter 761] [G_Loss 1.798556] [D_Loss 1.169150]\n",
      "train [Epoch 0/10] [Batch 762/2345] [Iter 762] [G_Loss 1.871684] [D_Loss 0.693124]\n",
      "train [Epoch 0/10] [Batch 763/2345] [Iter 763] [G_Loss 1.129686] [D_Loss 1.415199]\n",
      "train [Epoch 0/10] [Batch 764/2345] [Iter 764] [G_Loss 2.244728] [D_Loss 1.494493]\n",
      "train [Epoch 0/10] [Batch 765/2345] [Iter 765] [G_Loss 0.959512] [D_Loss 1.484806]\n",
      "train [Epoch 0/10] [Batch 766/2345] [Iter 766] [G_Loss 0.818753] [D_Loss 1.224635]\n",
      "train [Epoch 0/10] [Batch 767/2345] [Iter 767] [G_Loss 1.032013] [D_Loss 1.144190]\n",
      "train [Epoch 0/10] [Batch 768/2345] [Iter 768] [G_Loss 0.915196] [D_Loss 1.165715]\n",
      "train [Epoch 0/10] [Batch 769/2345] [Iter 769] [G_Loss 1.120983] [D_Loss 1.067216]\n",
      "train [Epoch 0/10] [Batch 770/2345] [Iter 770] [G_Loss 0.985262] [D_Loss 1.107029]\n",
      "train [Epoch 0/10] [Batch 771/2345] [Iter 771] [G_Loss 1.170325] [D_Loss 0.904047]\n",
      "train [Epoch 0/10] [Batch 772/2345] [Iter 772] [G_Loss 1.169259] [D_Loss 0.831567]\n",
      "train [Epoch 0/10] [Batch 773/2345] [Iter 773] [G_Loss 1.659977] [D_Loss 0.741451]\n",
      "train [Epoch 0/10] [Batch 774/2345] [Iter 774] [G_Loss 1.109591] [D_Loss 0.929581]\n",
      "train [Epoch 0/10] [Batch 775/2345] [Iter 775] [G_Loss 1.590805] [D_Loss 0.826632]\n",
      "train [Epoch 0/10] [Batch 776/2345] [Iter 776] [G_Loss 0.871612] [D_Loss 1.186488]\n",
      "train [Epoch 0/10] [Batch 777/2345] [Iter 777] [G_Loss 2.309814] [D_Loss 1.149020]\n",
      "train [Epoch 0/10] [Batch 778/2345] [Iter 778] [G_Loss 1.046054] [D_Loss 0.803004]\n",
      "train [Epoch 0/10] [Batch 779/2345] [Iter 779] [G_Loss 1.070920] [D_Loss 0.694238]\n",
      "train [Epoch 0/10] [Batch 780/2345] [Iter 780] [G_Loss 1.797104] [D_Loss 0.643270]\n",
      "train [Epoch 0/10] [Batch 781/2345] [Iter 781] [G_Loss 2.150519] [D_Loss 0.551471]\n",
      "train [Epoch 0/10] [Batch 782/2345] [Iter 782] [G_Loss 1.211563] [D_Loss 0.575637]\n",
      "train [Epoch 0/10] [Batch 783/2345] [Iter 783] [G_Loss 1.913996] [D_Loss 0.609113]\n",
      "train [Epoch 0/10] [Batch 784/2345] [Iter 784] [G_Loss 1.652922] [D_Loss 0.559348]\n",
      "train [Epoch 0/10] [Batch 785/2345] [Iter 785] [G_Loss 2.684563] [D_Loss 1.022704]\n",
      "train [Epoch 0/10] [Batch 786/2345] [Iter 786] [G_Loss 1.378532] [D_Loss 0.668909]\n",
      "train [Epoch 0/10] [Batch 787/2345] [Iter 787] [G_Loss 0.830477] [D_Loss 1.273725]\n",
      "train [Epoch 0/10] [Batch 788/2345] [Iter 788] [G_Loss 1.438719] [D_Loss 2.625670]\n",
      "train [Epoch 0/10] [Batch 789/2345] [Iter 789] [G_Loss 1.182615] [D_Loss 2.139296]\n",
      "train [Epoch 0/10] [Batch 790/2345] [Iter 790] [G_Loss 5.895960] [D_Loss 0.701280]\n",
      "train [Epoch 0/10] [Batch 791/2345] [Iter 791] [G_Loss 4.769296] [D_Loss 1.253746]\n",
      "train [Epoch 0/10] [Batch 792/2345] [Iter 792] [G_Loss 1.049400] [D_Loss 0.861034]\n",
      "train [Epoch 0/10] [Batch 793/2345] [Iter 793] [G_Loss 4.209469] [D_Loss 0.709782]\n",
      "train [Epoch 0/10] [Batch 794/2345] [Iter 794] [G_Loss 3.066565] [D_Loss 0.566146]\n",
      "train [Epoch 0/10] [Batch 795/2345] [Iter 795] [G_Loss 4.322678] [D_Loss 0.499564]\n",
      "train [Epoch 0/10] [Batch 796/2345] [Iter 796] [G_Loss 0.676347] [D_Loss 1.584982]\n",
      "train [Epoch 0/10] [Batch 797/2345] [Iter 797] [G_Loss 2.371021] [D_Loss 1.176019]\n",
      "train [Epoch 0/10] [Batch 798/2345] [Iter 798] [G_Loss 0.959396] [D_Loss 1.397071]\n",
      "train [Epoch 0/10] [Batch 799/2345] [Iter 799] [G_Loss 0.782857] [D_Loss 1.306817]\n",
      "train [Epoch 0/10] [Batch 800/2345] [Iter 800] [G_Loss 1.015041] [D_Loss 1.156197]\n",
      "train [Epoch 0/10] [Batch 801/2345] [Iter 801] [G_Loss 1.199674] [D_Loss 0.910901]\n",
      "train [Epoch 0/10] [Batch 802/2345] [Iter 802] [G_Loss 1.265459] [D_Loss 0.884370]\n",
      "train [Epoch 0/10] [Batch 803/2345] [Iter 803] [G_Loss 1.146793] [D_Loss 0.939503]\n",
      "train [Epoch 0/10] [Batch 804/2345] [Iter 804] [G_Loss 1.355189] [D_Loss 0.778945]\n",
      "train [Epoch 0/10] [Batch 805/2345] [Iter 805] [G_Loss 1.087492] [D_Loss 0.998901]\n",
      "train [Epoch 0/10] [Batch 806/2345] [Iter 806] [G_Loss 1.348393] [D_Loss 0.927174]\n",
      "train [Epoch 0/10] [Batch 807/2345] [Iter 807] [G_Loss 1.252089] [D_Loss 1.006471]\n",
      "train [Epoch 0/10] [Batch 808/2345] [Iter 808] [G_Loss 0.958213] [D_Loss 1.020370]\n",
      "train [Epoch 0/10] [Batch 809/2345] [Iter 809] [G_Loss 1.090837] [D_Loss 0.881123]\n",
      "train [Epoch 0/10] [Batch 810/2345] [Iter 810] [G_Loss 1.146709] [D_Loss 0.773480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 811/2345] [Iter 811] [G_Loss 1.367402] [D_Loss 0.825623]\n",
      "train [Epoch 0/10] [Batch 812/2345] [Iter 812] [G_Loss 1.375891] [D_Loss 0.809408]\n",
      "train [Epoch 0/10] [Batch 813/2345] [Iter 813] [G_Loss 1.192918] [D_Loss 0.868315]\n",
      "train [Epoch 0/10] [Batch 814/2345] [Iter 814] [G_Loss 0.869865] [D_Loss 1.235805]\n",
      "train [Epoch 0/10] [Batch 815/2345] [Iter 815] [G_Loss 1.970086] [D_Loss 1.246959]\n",
      "train [Epoch 0/10] [Batch 816/2345] [Iter 816] [G_Loss 0.779859] [D_Loss 1.399786]\n",
      "train [Epoch 0/10] [Batch 817/2345] [Iter 817] [G_Loss 1.993181] [D_Loss 0.756950]\n",
      "train [Epoch 0/10] [Batch 818/2345] [Iter 818] [G_Loss 1.020989] [D_Loss 1.300241]\n",
      "train [Epoch 0/10] [Batch 819/2345] [Iter 819] [G_Loss 1.192425] [D_Loss 1.321638]\n",
      "train [Epoch 0/10] [Batch 820/2345] [Iter 820] [G_Loss 3.210922] [D_Loss 0.787305]\n",
      "train [Epoch 0/10] [Batch 821/2345] [Iter 821] [G_Loss 1.105762] [D_Loss 0.931853]\n",
      "train [Epoch 0/10] [Batch 822/2345] [Iter 822] [G_Loss 1.660867] [D_Loss 1.360338]\n",
      "train [Epoch 0/10] [Batch 823/2345] [Iter 823] [G_Loss 1.333785] [D_Loss 1.051158]\n",
      "train [Epoch 0/10] [Batch 824/2345] [Iter 824] [G_Loss 1.114412] [D_Loss 0.874234]\n",
      "train [Epoch 0/10] [Batch 825/2345] [Iter 825] [G_Loss 1.882697] [D_Loss 0.691138]\n",
      "train [Epoch 0/10] [Batch 826/2345] [Iter 826] [G_Loss 1.888150] [D_Loss 0.885477]\n",
      "train [Epoch 0/10] [Batch 827/2345] [Iter 827] [G_Loss 1.281109] [D_Loss 0.679646]\n",
      "train [Epoch 0/10] [Batch 828/2345] [Iter 828] [G_Loss 1.122394] [D_Loss 0.829640]\n",
      "train [Epoch 0/10] [Batch 829/2345] [Iter 829] [G_Loss 1.701171] [D_Loss 0.684124]\n",
      "train [Epoch 0/10] [Batch 830/2345] [Iter 830] [G_Loss 1.930964] [D_Loss 0.797175]\n",
      "train [Epoch 0/10] [Batch 831/2345] [Iter 831] [G_Loss 1.685911] [D_Loss 0.694918]\n",
      "train [Epoch 0/10] [Batch 832/2345] [Iter 832] [G_Loss 2.131015] [D_Loss 0.521881]\n",
      "train [Epoch 0/10] [Batch 833/2345] [Iter 833] [G_Loss 3.068691] [D_Loss 1.205737]\n",
      "train [Epoch 0/10] [Batch 834/2345] [Iter 834] [G_Loss 0.603914] [D_Loss 1.869295]\n",
      "train [Epoch 0/10] [Batch 835/2345] [Iter 835] [G_Loss 16.943287] [D_Loss 2.876875]\n",
      "train [Epoch 0/10] [Batch 836/2345] [Iter 836] [G_Loss 6.037400] [D_Loss 0.821271]\n",
      "train [Epoch 0/10] [Batch 837/2345] [Iter 837] [G_Loss 1.256292] [D_Loss 1.164251]\n",
      "train [Epoch 0/10] [Batch 838/2345] [Iter 838] [G_Loss 2.055266] [D_Loss 0.894582]\n",
      "train [Epoch 0/10] [Batch 839/2345] [Iter 839] [G_Loss 1.647221] [D_Loss 0.677802]\n",
      "train [Epoch 0/10] [Batch 840/2345] [Iter 840] [G_Loss 1.576972] [D_Loss 0.683521]\n",
      "train [Epoch 0/10] [Batch 841/2345] [Iter 841] [G_Loss 1.809603] [D_Loss 0.746918]\n",
      "train [Epoch 0/10] [Batch 842/2345] [Iter 842] [G_Loss 1.854666] [D_Loss 0.641811]\n",
      "train [Epoch 0/10] [Batch 843/2345] [Iter 843] [G_Loss 1.610299] [D_Loss 0.631294]\n",
      "train [Epoch 0/10] [Batch 844/2345] [Iter 844] [G_Loss 2.867574] [D_Loss 0.780998]\n",
      "train [Epoch 0/10] [Batch 845/2345] [Iter 845] [G_Loss 1.921845] [D_Loss 0.549561]\n",
      "train [Epoch 0/10] [Batch 846/2345] [Iter 846] [G_Loss 2.396407] [D_Loss 0.558376]\n",
      "train [Epoch 0/10] [Batch 847/2345] [Iter 847] [G_Loss 1.911940] [D_Loss 0.550813]\n",
      "train [Epoch 0/10] [Batch 848/2345] [Iter 848] [G_Loss 1.964160] [D_Loss 0.442021]\n",
      "train [Epoch 0/10] [Batch 849/2345] [Iter 849] [G_Loss 2.076198] [D_Loss 0.361064]\n",
      "train [Epoch 0/10] [Batch 850/2345] [Iter 850] [G_Loss 2.019336] [D_Loss 0.423955]\n",
      "train [Epoch 0/10] [Batch 851/2345] [Iter 851] [G_Loss 1.738705] [D_Loss 0.554443]\n",
      "train [Epoch 0/10] [Batch 852/2345] [Iter 852] [G_Loss 2.595571] [D_Loss 0.549575]\n",
      "train [Epoch 0/10] [Batch 853/2345] [Iter 853] [G_Loss 1.044436] [D_Loss 1.136372]\n",
      "train [Epoch 0/10] [Batch 854/2345] [Iter 854] [G_Loss 4.828278] [D_Loss 1.327918]\n",
      "train [Epoch 0/10] [Batch 855/2345] [Iter 855] [G_Loss 2.250753] [D_Loss 0.759394]\n",
      "train [Epoch 0/10] [Batch 856/2345] [Iter 856] [G_Loss 0.777496] [D_Loss 1.038433]\n",
      "train [Epoch 0/10] [Batch 857/2345] [Iter 857] [G_Loss 1.656987] [D_Loss 0.785335]\n",
      "train [Epoch 0/10] [Batch 858/2345] [Iter 858] [G_Loss 2.174447] [D_Loss 0.914715]\n",
      "train [Epoch 0/10] [Batch 859/2345] [Iter 859] [G_Loss 1.112595] [D_Loss 0.854188]\n",
      "train [Epoch 0/10] [Batch 860/2345] [Iter 860] [G_Loss 0.934795] [D_Loss 1.781097]\n",
      "train [Epoch 0/10] [Batch 861/2345] [Iter 861] [G_Loss 3.392343] [D_Loss 1.213117]\n",
      "train [Epoch 0/10] [Batch 862/2345] [Iter 862] [G_Loss 1.034241] [D_Loss 0.960999]\n",
      "train [Epoch 0/10] [Batch 863/2345] [Iter 863] [G_Loss 0.807381] [D_Loss 1.159611]\n",
      "train [Epoch 0/10] [Batch 864/2345] [Iter 864] [G_Loss 2.271705] [D_Loss 1.495104]\n",
      "train [Epoch 0/10] [Batch 865/2345] [Iter 865] [G_Loss 1.364592] [D_Loss 0.897625]\n",
      "train [Epoch 0/10] [Batch 866/2345] [Iter 866] [G_Loss 0.929516] [D_Loss 0.961980]\n",
      "train [Epoch 0/10] [Batch 867/2345] [Iter 867] [G_Loss 0.935016] [D_Loss 0.976757]\n",
      "train [Epoch 0/10] [Batch 868/2345] [Iter 868] [G_Loss 0.824938] [D_Loss 1.340929]\n",
      "train [Epoch 0/10] [Batch 869/2345] [Iter 869] [G_Loss 0.904487] [D_Loss 1.235205]\n",
      "train [Epoch 0/10] [Batch 870/2345] [Iter 870] [G_Loss 0.788888] [D_Loss 1.271780]\n",
      "train [Epoch 0/10] [Batch 871/2345] [Iter 871] [G_Loss 1.264605] [D_Loss 0.987086]\n",
      "train [Epoch 0/10] [Batch 872/2345] [Iter 872] [G_Loss 1.071506] [D_Loss 1.176613]\n",
      "train [Epoch 0/10] [Batch 873/2345] [Iter 873] [G_Loss 1.936405] [D_Loss 0.727999]\n",
      "train [Epoch 0/10] [Batch 874/2345] [Iter 874] [G_Loss 0.658537] [D_Loss 1.378154]\n",
      "train [Epoch 0/10] [Batch 875/2345] [Iter 875] [G_Loss 1.163586] [D_Loss 0.931278]\n",
      "train [Epoch 0/10] [Batch 876/2345] [Iter 876] [G_Loss 1.391764] [D_Loss 1.091918]\n",
      "train [Epoch 0/10] [Batch 877/2345] [Iter 877] [G_Loss 1.224492] [D_Loss 0.842201]\n",
      "train [Epoch 0/10] [Batch 878/2345] [Iter 878] [G_Loss 1.538306] [D_Loss 0.638875]\n",
      "train [Epoch 0/10] [Batch 879/2345] [Iter 879] [G_Loss 0.778179] [D_Loss 1.280332]\n",
      "train [Epoch 0/10] [Batch 880/2345] [Iter 880] [G_Loss 1.930440] [D_Loss 1.061467]\n",
      "train [Epoch 0/10] [Batch 881/2345] [Iter 881] [G_Loss 1.500787] [D_Loss 0.866651]\n",
      "train [Epoch 0/10] [Batch 882/2345] [Iter 882] [G_Loss 1.237471] [D_Loss 1.030652]\n",
      "train [Epoch 0/10] [Batch 883/2345] [Iter 883] [G_Loss 1.534599] [D_Loss 0.734240]\n",
      "train [Epoch 0/10] [Batch 884/2345] [Iter 884] [G_Loss 1.790073] [D_Loss 0.783021]\n",
      "train [Epoch 0/10] [Batch 885/2345] [Iter 885] [G_Loss 1.449825] [D_Loss 0.749831]\n",
      "train [Epoch 0/10] [Batch 886/2345] [Iter 886] [G_Loss 1.297629] [D_Loss 0.695416]\n",
      "train [Epoch 0/10] [Batch 887/2345] [Iter 887] [G_Loss 1.575095] [D_Loss 0.843115]\n",
      "train [Epoch 0/10] [Batch 888/2345] [Iter 888] [G_Loss 1.221814] [D_Loss 0.711793]\n",
      "train [Epoch 0/10] [Batch 889/2345] [Iter 889] [G_Loss 1.650874] [D_Loss 0.476426]\n",
      "train [Epoch 0/10] [Batch 890/2345] [Iter 890] [G_Loss 2.100559] [D_Loss 0.327468]\n",
      "train [Epoch 0/10] [Batch 891/2345] [Iter 891] [G_Loss 1.927594] [D_Loss 0.353428]\n",
      "train [Epoch 0/10] [Batch 892/2345] [Iter 892] [G_Loss 3.723007] [D_Loss 1.907981]\n",
      "train [Epoch 0/10] [Batch 893/2345] [Iter 893] [G_Loss 1.546384] [D_Loss 0.536427]\n",
      "train [Epoch 0/10] [Batch 894/2345] [Iter 894] [G_Loss 1.154282] [D_Loss 0.737360]\n",
      "train [Epoch 0/10] [Batch 895/2345] [Iter 895] [G_Loss 1.815631] [D_Loss 0.384667]\n",
      "train [Epoch 0/10] [Batch 896/2345] [Iter 896] [G_Loss 2.943212] [D_Loss 0.377115]\n",
      "train [Epoch 0/10] [Batch 897/2345] [Iter 897] [G_Loss 2.663885] [D_Loss 0.235645]\n",
      "train [Epoch 0/10] [Batch 898/2345] [Iter 898] [G_Loss 1.374335] [D_Loss 0.568321]\n",
      "train [Epoch 0/10] [Batch 899/2345] [Iter 899] [G_Loss 0.352187] [D_Loss 2.482037]\n",
      "train [Epoch 0/10] [Batch 900/2345] [Iter 900] [G_Loss 5.447507] [D_Loss 4.271319]\n",
      "train [Epoch 0/10] [Batch 901/2345] [Iter 901] [G_Loss 2.334546] [D_Loss 1.484017]\n",
      "train [Epoch 0/10] [Batch 902/2345] [Iter 902] [G_Loss 1.005968] [D_Loss 1.064717]\n",
      "train [Epoch 0/10] [Batch 903/2345] [Iter 903] [G_Loss 0.575430] [D_Loss 1.284237]\n",
      "train [Epoch 0/10] [Batch 904/2345] [Iter 904] [G_Loss 0.916273] [D_Loss 0.930021]\n",
      "train [Epoch 0/10] [Batch 905/2345] [Iter 905] [G_Loss 1.262654] [D_Loss 1.044029]\n",
      "train [Epoch 0/10] [Batch 906/2345] [Iter 906] [G_Loss 1.110027] [D_Loss 0.914672]\n",
      "train [Epoch 0/10] [Batch 907/2345] [Iter 907] [G_Loss 1.076002] [D_Loss 0.918643]\n",
      "train [Epoch 0/10] [Batch 908/2345] [Iter 908] [G_Loss 1.352942] [D_Loss 0.956120]\n",
      "train [Epoch 0/10] [Batch 909/2345] [Iter 909] [G_Loss 1.109543] [D_Loss 0.931910]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 910/2345] [Iter 910] [G_Loss 1.225683] [D_Loss 0.907227]\n",
      "train [Epoch 0/10] [Batch 911/2345] [Iter 911] [G_Loss 1.088495] [D_Loss 0.769139]\n",
      "train [Epoch 0/10] [Batch 912/2345] [Iter 912] [G_Loss 1.395156] [D_Loss 0.761896]\n",
      "train [Epoch 0/10] [Batch 913/2345] [Iter 913] [G_Loss 1.581485] [D_Loss 0.844873]\n",
      "train [Epoch 0/10] [Batch 914/2345] [Iter 914] [G_Loss 0.988288] [D_Loss 0.919518]\n",
      "train [Epoch 0/10] [Batch 915/2345] [Iter 915] [G_Loss 0.643476] [D_Loss 1.608690]\n",
      "train [Epoch 0/10] [Batch 916/2345] [Iter 916] [G_Loss 1.124736] [D_Loss 1.490537]\n",
      "train [Epoch 0/10] [Batch 917/2345] [Iter 917] [G_Loss 1.691780] [D_Loss 1.261212]\n",
      "train [Epoch 0/10] [Batch 918/2345] [Iter 918] [G_Loss 0.933551] [D_Loss 1.308557]\n",
      "train [Epoch 0/10] [Batch 919/2345] [Iter 919] [G_Loss 1.240162] [D_Loss 1.271639]\n",
      "train [Epoch 0/10] [Batch 920/2345] [Iter 920] [G_Loss 1.269556] [D_Loss 1.011564]\n",
      "train [Epoch 0/10] [Batch 921/2345] [Iter 921] [G_Loss 1.274144] [D_Loss 1.117266]\n",
      "train [Epoch 0/10] [Batch 922/2345] [Iter 922] [G_Loss 1.129866] [D_Loss 0.977747]\n",
      "train [Epoch 0/10] [Batch 923/2345] [Iter 923] [G_Loss 1.187822] [D_Loss 0.978799]\n",
      "train [Epoch 0/10] [Batch 924/2345] [Iter 924] [G_Loss 1.033004] [D_Loss 0.927164]\n",
      "train [Epoch 0/10] [Batch 925/2345] [Iter 925] [G_Loss 1.300173] [D_Loss 0.904030]\n",
      "train [Epoch 0/10] [Batch 926/2345] [Iter 926] [G_Loss 1.232226] [D_Loss 0.678921]\n",
      "train [Epoch 0/10] [Batch 927/2345] [Iter 927] [G_Loss 1.451110] [D_Loss 0.893059]\n",
      "train [Epoch 0/10] [Batch 928/2345] [Iter 928] [G_Loss 1.310472] [D_Loss 0.767249]\n",
      "train [Epoch 0/10] [Batch 929/2345] [Iter 929] [G_Loss 0.992555] [D_Loss 0.891948]\n",
      "train [Epoch 0/10] [Batch 930/2345] [Iter 930] [G_Loss 0.592494] [D_Loss 1.696791]\n",
      "train [Epoch 0/10] [Batch 931/2345] [Iter 931] [G_Loss 1.652975] [D_Loss 1.354325]\n",
      "train [Epoch 0/10] [Batch 932/2345] [Iter 932] [G_Loss 1.526575] [D_Loss 0.971550]\n",
      "train [Epoch 0/10] [Batch 933/2345] [Iter 933] [G_Loss 0.629983] [D_Loss 1.782370]\n",
      "train [Epoch 0/10] [Batch 934/2345] [Iter 934] [G_Loss 1.196836] [D_Loss 1.211257]\n",
      "train [Epoch 0/10] [Batch 935/2345] [Iter 935] [G_Loss 1.046707] [D_Loss 1.226118]\n",
      "train [Epoch 0/10] [Batch 936/2345] [Iter 936] [G_Loss 1.021979] [D_Loss 1.062655]\n",
      "train [Epoch 0/10] [Batch 937/2345] [Iter 937] [G_Loss 1.032025] [D_Loss 1.093876]\n",
      "train [Epoch 0/10] [Batch 938/2345] [Iter 938] [G_Loss 1.241566] [D_Loss 0.965545]\n",
      "train [Epoch 0/10] [Batch 939/2345] [Iter 939] [G_Loss 1.192187] [D_Loss 1.022076]\n",
      "train [Epoch 0/10] [Batch 940/2345] [Iter 940] [G_Loss 1.360058] [D_Loss 0.951924]\n",
      "train [Epoch 0/10] [Batch 941/2345] [Iter 941] [G_Loss 1.287663] [D_Loss 1.052098]\n",
      "train [Epoch 0/10] [Batch 942/2345] [Iter 942] [G_Loss 1.180621] [D_Loss 1.007042]\n",
      "train [Epoch 0/10] [Batch 943/2345] [Iter 943] [G_Loss 1.335677] [D_Loss 0.939434]\n",
      "train [Epoch 0/10] [Batch 944/2345] [Iter 944] [G_Loss 1.088121] [D_Loss 1.146278]\n",
      "train [Epoch 0/10] [Batch 945/2345] [Iter 945] [G_Loss 1.055179] [D_Loss 0.954466]\n",
      "train [Epoch 0/10] [Batch 946/2345] [Iter 946] [G_Loss 1.136687] [D_Loss 0.874541]\n",
      "train [Epoch 0/10] [Batch 947/2345] [Iter 947] [G_Loss 1.598370] [D_Loss 0.765140]\n",
      "train [Epoch 0/10] [Batch 948/2345] [Iter 948] [G_Loss 1.039967] [D_Loss 0.943228]\n",
      "train [Epoch 0/10] [Batch 949/2345] [Iter 949] [G_Loss 1.513987] [D_Loss 0.846957]\n",
      "train [Epoch 0/10] [Batch 950/2345] [Iter 950] [G_Loss 1.599556] [D_Loss 0.805902]\n",
      "train [Epoch 0/10] [Batch 951/2345] [Iter 951] [G_Loss 1.607828] [D_Loss 0.758075]\n",
      "train [Epoch 0/10] [Batch 952/2345] [Iter 952] [G_Loss 1.580733] [D_Loss 0.740902]\n",
      "train [Epoch 0/10] [Batch 953/2345] [Iter 953] [G_Loss 1.416165] [D_Loss 0.686489]\n",
      "train [Epoch 0/10] [Batch 954/2345] [Iter 954] [G_Loss 1.678004] [D_Loss 0.651740]\n",
      "train [Epoch 0/10] [Batch 955/2345] [Iter 955] [G_Loss 1.973155] [D_Loss 0.748942]\n",
      "train [Epoch 0/10] [Batch 956/2345] [Iter 956] [G_Loss 2.185559] [D_Loss 0.935905]\n",
      "train [Epoch 0/10] [Batch 957/2345] [Iter 957] [G_Loss 1.398320] [D_Loss 0.772934]\n",
      "train [Epoch 0/10] [Batch 958/2345] [Iter 958] [G_Loss 2.278388] [D_Loss 0.682868]\n",
      "train [Epoch 0/10] [Batch 959/2345] [Iter 959] [G_Loss 2.326167] [D_Loss 0.518426]\n",
      "train [Epoch 0/10] [Batch 960/2345] [Iter 960] [G_Loss 1.337718] [D_Loss 0.628780]\n",
      "train [Epoch 0/10] [Batch 961/2345] [Iter 961] [G_Loss 2.156604] [D_Loss 0.469519]\n",
      "train [Epoch 0/10] [Batch 962/2345] [Iter 962] [G_Loss 2.170112] [D_Loss 0.456903]\n",
      "train [Epoch 0/10] [Batch 963/2345] [Iter 963] [G_Loss 1.762607] [D_Loss 0.608839]\n",
      "train [Epoch 0/10] [Batch 964/2345] [Iter 964] [G_Loss 1.813772] [D_Loss 0.706043]\n",
      "train [Epoch 0/10] [Batch 965/2345] [Iter 965] [G_Loss 2.270006] [D_Loss 1.128115]\n",
      "train [Epoch 0/10] [Batch 966/2345] [Iter 966] [G_Loss 0.588810] [D_Loss 2.539173]\n",
      "train [Epoch 0/10] [Batch 967/2345] [Iter 967] [G_Loss 2.002202] [D_Loss 2.349297]\n",
      "train [Epoch 0/10] [Batch 968/2345] [Iter 968] [G_Loss 1.561840] [D_Loss 1.431453]\n",
      "train [Epoch 0/10] [Batch 969/2345] [Iter 969] [G_Loss 0.658638] [D_Loss 1.791738]\n",
      "train [Epoch 0/10] [Batch 970/2345] [Iter 970] [G_Loss 1.166362] [D_Loss 1.161646]\n",
      "train [Epoch 0/10] [Batch 971/2345] [Iter 971] [G_Loss 0.971051] [D_Loss 1.382677]\n",
      "train [Epoch 0/10] [Batch 972/2345] [Iter 972] [G_Loss 1.026962] [D_Loss 1.188038]\n",
      "train [Epoch 0/10] [Batch 973/2345] [Iter 973] [G_Loss 1.230241] [D_Loss 1.287134]\n",
      "train [Epoch 0/10] [Batch 974/2345] [Iter 974] [G_Loss 0.889429] [D_Loss 1.211558]\n",
      "train [Epoch 0/10] [Batch 975/2345] [Iter 975] [G_Loss 1.111297] [D_Loss 1.110453]\n",
      "train [Epoch 0/10] [Batch 976/2345] [Iter 976] [G_Loss 1.341909] [D_Loss 1.114259]\n",
      "train [Epoch 0/10] [Batch 977/2345] [Iter 977] [G_Loss 1.103160] [D_Loss 0.987020]\n",
      "train [Epoch 0/10] [Batch 978/2345] [Iter 978] [G_Loss 1.118964] [D_Loss 0.911003]\n",
      "train [Epoch 0/10] [Batch 979/2345] [Iter 979] [G_Loss 2.638648] [D_Loss 1.277534]\n",
      "train [Epoch 0/10] [Batch 980/2345] [Iter 980] [G_Loss 0.053941] [D_Loss 3.882023]\n",
      "train [Epoch 0/10] [Batch 981/2345] [Iter 981] [G_Loss 0.311583] [D_Loss 2.152916]\n",
      "train [Epoch 0/10] [Batch 982/2345] [Iter 982] [G_Loss 1.140214] [D_Loss 1.230847]\n",
      "train [Epoch 0/10] [Batch 983/2345] [Iter 983] [G_Loss 1.716904] [D_Loss 1.426698]\n",
      "train [Epoch 0/10] [Batch 984/2345] [Iter 984] [G_Loss 0.846084] [D_Loss 1.299556]\n",
      "train [Epoch 0/10] [Batch 985/2345] [Iter 985] [G_Loss 0.591880] [D_Loss 1.486845]\n",
      "train [Epoch 0/10] [Batch 986/2345] [Iter 986] [G_Loss 0.809587] [D_Loss 1.357259]\n",
      "train [Epoch 0/10] [Batch 987/2345] [Iter 987] [G_Loss 1.104092] [D_Loss 1.157013]\n",
      "train [Epoch 0/10] [Batch 988/2345] [Iter 988] [G_Loss 0.995398] [D_Loss 1.159907]\n",
      "train [Epoch 0/10] [Batch 989/2345] [Iter 989] [G_Loss 0.868570] [D_Loss 1.281529]\n",
      "train [Epoch 0/10] [Batch 990/2345] [Iter 990] [G_Loss 0.931598] [D_Loss 1.195235]\n",
      "train [Epoch 0/10] [Batch 991/2345] [Iter 991] [G_Loss 0.883844] [D_Loss 1.199405]\n",
      "train [Epoch 0/10] [Batch 992/2345] [Iter 992] [G_Loss 0.984237] [D_Loss 1.152538]\n",
      "train [Epoch 0/10] [Batch 993/2345] [Iter 993] [G_Loss 0.804196] [D_Loss 1.248724]\n",
      "train [Epoch 0/10] [Batch 994/2345] [Iter 994] [G_Loss 0.992269] [D_Loss 1.143729]\n",
      "train [Epoch 0/10] [Batch 995/2345] [Iter 995] [G_Loss 0.924534] [D_Loss 1.183021]\n",
      "train [Epoch 0/10] [Batch 996/2345] [Iter 996] [G_Loss 1.072723] [D_Loss 1.181858]\n",
      "train [Epoch 0/10] [Batch 997/2345] [Iter 997] [G_Loss 1.153237] [D_Loss 1.083491]\n",
      "train [Epoch 0/10] [Batch 998/2345] [Iter 998] [G_Loss 1.025482] [D_Loss 1.092472]\n",
      "train [Epoch 0/10] [Batch 999/2345] [Iter 999] [G_Loss 1.030319] [D_Loss 1.006038]\n",
      "train [Epoch 0/10] [Batch 1000/2345] [Iter 1000] [G_Loss 1.554697] [D_Loss 1.111316]\n",
      "train [Epoch 0/10] [Batch 1001/2345] [Iter 1001] [G_Loss 0.600644] [D_Loss 1.137902]\n",
      "train [Epoch 0/10] [Batch 1002/2345] [Iter 1002] [G_Loss 1.643679] [D_Loss 1.035951]\n",
      "train [Epoch 0/10] [Batch 1003/2345] [Iter 1003] [G_Loss 1.093876] [D_Loss 0.867512]\n",
      "train [Epoch 0/10] [Batch 1004/2345] [Iter 1004] [G_Loss 1.298349] [D_Loss 0.781546]\n",
      "train [Epoch 0/10] [Batch 1005/2345] [Iter 1005] [G_Loss 1.398488] [D_Loss 0.557948]\n",
      "train [Epoch 0/10] [Batch 1006/2345] [Iter 1006] [G_Loss 1.730176] [D_Loss 0.702175]\n",
      "train [Epoch 0/10] [Batch 1007/2345] [Iter 1007] [G_Loss 1.361168] [D_Loss 0.991532]\n",
      "train [Epoch 0/10] [Batch 1008/2345] [Iter 1008] [G_Loss 1.716758] [D_Loss 1.179746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 1009/2345] [Iter 1009] [G_Loss 0.730410] [D_Loss 1.115590]\n",
      "train [Epoch 0/10] [Batch 1010/2345] [Iter 1010] [G_Loss 1.664168] [D_Loss 0.946118]\n",
      "train [Epoch 0/10] [Batch 1011/2345] [Iter 1011] [G_Loss 1.507249] [D_Loss 1.181693]\n",
      "train [Epoch 0/10] [Batch 1012/2345] [Iter 1012] [G_Loss 0.769055] [D_Loss 1.240191]\n",
      "train [Epoch 0/10] [Batch 1013/2345] [Iter 1013] [G_Loss 1.366730] [D_Loss 1.103008]\n",
      "train [Epoch 0/10] [Batch 1014/2345] [Iter 1014] [G_Loss 3.842514] [D_Loss 0.906776]\n",
      "train [Epoch 0/10] [Batch 1015/2345] [Iter 1015] [G_Loss 2.792632] [D_Loss 0.650463]\n",
      "train [Epoch 0/10] [Batch 1016/2345] [Iter 1016] [G_Loss 2.431350] [D_Loss 0.640589]\n",
      "train [Epoch 0/10] [Batch 1017/2345] [Iter 1017] [G_Loss 2.169079] [D_Loss 0.950889]\n",
      "train [Epoch 0/10] [Batch 1018/2345] [Iter 1018] [G_Loss 1.064803] [D_Loss 1.280001]\n",
      "train [Epoch 0/10] [Batch 1019/2345] [Iter 1019] [G_Loss 1.101610] [D_Loss 1.243395]\n",
      "train [Epoch 0/10] [Batch 1020/2345] [Iter 1020] [G_Loss 1.538345] [D_Loss 1.433537]\n",
      "train [Epoch 0/10] [Batch 1021/2345] [Iter 1021] [G_Loss 1.293001] [D_Loss 0.913178]\n",
      "train [Epoch 0/10] [Batch 1022/2345] [Iter 1022] [G_Loss 1.268338] [D_Loss 0.883234]\n",
      "train [Epoch 0/10] [Batch 1023/2345] [Iter 1023] [G_Loss 1.331007] [D_Loss 0.874009]\n",
      "train [Epoch 0/10] [Batch 1024/2345] [Iter 1024] [G_Loss 1.318475] [D_Loss 0.787980]\n",
      "train [Epoch 0/10] [Batch 1025/2345] [Iter 1025] [G_Loss 1.102940] [D_Loss 0.820917]\n",
      "train [Epoch 0/10] [Batch 1026/2345] [Iter 1026] [G_Loss 1.388128] [D_Loss 0.772396]\n",
      "train [Epoch 0/10] [Batch 1027/2345] [Iter 1027] [G_Loss 1.519240] [D_Loss 0.819522]\n",
      "train [Epoch 0/10] [Batch 1028/2345] [Iter 1028] [G_Loss 1.575220] [D_Loss 0.511853]\n",
      "train [Epoch 0/10] [Batch 1029/2345] [Iter 1029] [G_Loss 1.153077] [D_Loss 0.711505]\n",
      "train [Epoch 0/10] [Batch 1030/2345] [Iter 1030] [G_Loss 1.721711] [D_Loss 0.548065]\n",
      "train [Epoch 0/10] [Batch 1031/2345] [Iter 1031] [G_Loss 0.631580] [D_Loss 1.348132]\n",
      "train [Epoch 0/10] [Batch 1032/2345] [Iter 1032] [G_Loss 3.035445] [D_Loss 1.678173]\n",
      "train [Epoch 0/10] [Batch 1033/2345] [Iter 1033] [G_Loss 0.730814] [D_Loss 2.078430]\n",
      "train [Epoch 0/10] [Batch 1034/2345] [Iter 1034] [G_Loss 2.042958] [D_Loss 1.013777]\n",
      "train [Epoch 0/10] [Batch 1035/2345] [Iter 1035] [G_Loss 3.745980] [D_Loss 0.650773]\n",
      "train [Epoch 0/10] [Batch 1036/2345] [Iter 1036] [G_Loss 2.713164] [D_Loss 0.624260]\n",
      "train [Epoch 0/10] [Batch 1037/2345] [Iter 1037] [G_Loss 0.582337] [D_Loss 2.517398]\n",
      "train [Epoch 0/10] [Batch 1038/2345] [Iter 1038] [G_Loss 1.776044] [D_Loss 1.345858]\n",
      "train [Epoch 0/10] [Batch 1039/2345] [Iter 1039] [G_Loss 1.586498] [D_Loss 1.117281]\n",
      "train [Epoch 0/10] [Batch 1040/2345] [Iter 1040] [G_Loss 1.401063] [D_Loss 1.033352]\n",
      "train [Epoch 0/10] [Batch 1041/2345] [Iter 1041] [G_Loss 1.149197] [D_Loss 0.941228]\n",
      "train [Epoch 0/10] [Batch 1042/2345] [Iter 1042] [G_Loss 1.046087] [D_Loss 1.036359]\n",
      "train [Epoch 0/10] [Batch 1043/2345] [Iter 1043] [G_Loss 0.719661] [D_Loss 1.311884]\n",
      "train [Epoch 0/10] [Batch 1044/2345] [Iter 1044] [G_Loss 0.692380] [D_Loss 1.534247]\n",
      "train [Epoch 0/10] [Batch 1045/2345] [Iter 1045] [G_Loss 0.652561] [D_Loss 1.756244]\n",
      "train [Epoch 0/10] [Batch 1046/2345] [Iter 1046] [G_Loss 1.084947] [D_Loss 1.505268]\n",
      "train [Epoch 0/10] [Batch 1047/2345] [Iter 1047] [G_Loss 0.972735] [D_Loss 1.279683]\n",
      "train [Epoch 0/10] [Batch 1048/2345] [Iter 1048] [G_Loss 1.105876] [D_Loss 1.219582]\n",
      "train [Epoch 0/10] [Batch 1049/2345] [Iter 1049] [G_Loss 1.240059] [D_Loss 1.229270]\n",
      "train [Epoch 0/10] [Batch 1050/2345] [Iter 1050] [G_Loss 1.107928] [D_Loss 1.226921]\n",
      "train [Epoch 0/10] [Batch 1051/2345] [Iter 1051] [G_Loss 0.947875] [D_Loss 1.270655]\n",
      "train [Epoch 0/10] [Batch 1052/2345] [Iter 1052] [G_Loss 0.928422] [D_Loss 1.216925]\n",
      "train [Epoch 0/10] [Batch 1053/2345] [Iter 1053] [G_Loss 0.989062] [D_Loss 1.324798]\n",
      "train [Epoch 0/10] [Batch 1054/2345] [Iter 1054] [G_Loss 1.035855] [D_Loss 1.190444]\n",
      "train [Epoch 0/10] [Batch 1055/2345] [Iter 1055] [G_Loss 0.637820] [D_Loss 1.491301]\n",
      "train [Epoch 0/10] [Batch 1056/2345] [Iter 1056] [G_Loss 0.751941] [D_Loss 1.384139]\n",
      "train [Epoch 0/10] [Batch 1057/2345] [Iter 1057] [G_Loss 1.041670] [D_Loss 1.255769]\n",
      "train [Epoch 0/10] [Batch 1058/2345] [Iter 1058] [G_Loss 0.786973] [D_Loss 1.406255]\n",
      "train [Epoch 0/10] [Batch 1059/2345] [Iter 1059] [G_Loss 1.088995] [D_Loss 1.135486]\n",
      "train [Epoch 0/10] [Batch 1060/2345] [Iter 1060] [G_Loss 0.790070] [D_Loss 1.327575]\n",
      "train [Epoch 0/10] [Batch 1061/2345] [Iter 1061] [G_Loss 1.003896] [D_Loss 1.289114]\n",
      "train [Epoch 0/10] [Batch 1062/2345] [Iter 1062] [G_Loss 0.843512] [D_Loss 1.139538]\n",
      "train [Epoch 0/10] [Batch 1063/2345] [Iter 1063] [G_Loss 1.192489] [D_Loss 1.033989]\n",
      "train [Epoch 0/10] [Batch 1064/2345] [Iter 1064] [G_Loss 0.746260] [D_Loss 1.327276]\n",
      "train [Epoch 0/10] [Batch 1065/2345] [Iter 1065] [G_Loss 1.009076] [D_Loss 1.188212]\n",
      "train [Epoch 0/10] [Batch 1066/2345] [Iter 1066] [G_Loss 0.943641] [D_Loss 1.175347]\n",
      "train [Epoch 0/10] [Batch 1067/2345] [Iter 1067] [G_Loss 0.872832] [D_Loss 1.394051]\n",
      "train [Epoch 0/10] [Batch 1068/2345] [Iter 1068] [G_Loss 0.900752] [D_Loss 1.337504]\n",
      "train [Epoch 0/10] [Batch 1069/2345] [Iter 1069] [G_Loss 0.843781] [D_Loss 1.259380]\n",
      "train [Epoch 0/10] [Batch 1070/2345] [Iter 1070] [G_Loss 0.805950] [D_Loss 1.228256]\n",
      "train [Epoch 0/10] [Batch 1071/2345] [Iter 1071] [G_Loss 1.153484] [D_Loss 1.275472]\n",
      "train [Epoch 0/10] [Batch 1072/2345] [Iter 1072] [G_Loss 0.880058] [D_Loss 1.152977]\n",
      "train [Epoch 0/10] [Batch 1073/2345] [Iter 1073] [G_Loss 0.816194] [D_Loss 1.176041]\n",
      "train [Epoch 0/10] [Batch 1074/2345] [Iter 1074] [G_Loss 1.252967] [D_Loss 1.138503]\n",
      "train [Epoch 0/10] [Batch 1075/2345] [Iter 1075] [G_Loss 0.935691] [D_Loss 1.078851]\n",
      "train [Epoch 0/10] [Batch 1076/2345] [Iter 1076] [G_Loss 0.870239] [D_Loss 1.104957]\n",
      "train [Epoch 0/10] [Batch 1077/2345] [Iter 1077] [G_Loss 1.124207] [D_Loss 0.983588]\n",
      "train [Epoch 0/10] [Batch 1078/2345] [Iter 1078] [G_Loss 0.929161] [D_Loss 1.179265]\n",
      "train [Epoch 0/10] [Batch 1079/2345] [Iter 1079] [G_Loss 0.990378] [D_Loss 1.193232]\n",
      "train [Epoch 0/10] [Batch 1080/2345] [Iter 1080] [G_Loss 0.766190] [D_Loss 1.235762]\n",
      "train [Epoch 0/10] [Batch 1081/2345] [Iter 1081] [G_Loss 1.873286] [D_Loss 1.763600]\n",
      "train [Epoch 0/10] [Batch 1082/2345] [Iter 1082] [G_Loss 0.759137] [D_Loss 1.600987]\n",
      "train [Epoch 0/10] [Batch 1083/2345] [Iter 1083] [G_Loss 0.946421] [D_Loss 1.038060]\n",
      "train [Epoch 0/10] [Batch 1084/2345] [Iter 1084] [G_Loss 0.917275] [D_Loss 1.245700]\n",
      "train [Epoch 0/10] [Batch 1085/2345] [Iter 1085] [G_Loss 1.058786] [D_Loss 1.266950]\n",
      "train [Epoch 0/10] [Batch 1086/2345] [Iter 1086] [G_Loss 1.019170] [D_Loss 1.278690]\n",
      "train [Epoch 0/10] [Batch 1087/2345] [Iter 1087] [G_Loss 1.217077] [D_Loss 1.315719]\n",
      "train [Epoch 0/10] [Batch 1088/2345] [Iter 1088] [G_Loss 1.063094] [D_Loss 1.219994]\n",
      "train [Epoch 0/10] [Batch 1089/2345] [Iter 1089] [G_Loss 1.028550] [D_Loss 1.212835]\n",
      "train [Epoch 0/10] [Batch 1090/2345] [Iter 1090] [G_Loss 0.845948] [D_Loss 1.257772]\n",
      "train [Epoch 0/10] [Batch 1091/2345] [Iter 1091] [G_Loss 0.958464] [D_Loss 1.195050]\n",
      "train [Epoch 0/10] [Batch 1092/2345] [Iter 1092] [G_Loss 1.079216] [D_Loss 1.250531]\n",
      "train [Epoch 0/10] [Batch 1093/2345] [Iter 1093] [G_Loss 0.805879] [D_Loss 1.183149]\n",
      "train [Epoch 0/10] [Batch 1094/2345] [Iter 1094] [G_Loss 0.900345] [D_Loss 1.201478]\n",
      "train [Epoch 0/10] [Batch 1095/2345] [Iter 1095] [G_Loss 0.972548] [D_Loss 1.157180]\n",
      "train [Epoch 0/10] [Batch 1096/2345] [Iter 1096] [G_Loss 1.216218] [D_Loss 1.196343]\n",
      "train [Epoch 0/10] [Batch 1097/2345] [Iter 1097] [G_Loss 0.929291] [D_Loss 1.046070]\n",
      "train [Epoch 0/10] [Batch 1098/2345] [Iter 1098] [G_Loss 0.857295] [D_Loss 1.039119]\n",
      "train [Epoch 0/10] [Batch 1099/2345] [Iter 1099] [G_Loss 0.940970] [D_Loss 1.061453]\n",
      "train [Epoch 0/10] [Batch 1100/2345] [Iter 1100] [G_Loss 1.033479] [D_Loss 0.897048]\n",
      "train [Epoch 0/10] [Batch 1101/2345] [Iter 1101] [G_Loss 1.072691] [D_Loss 1.125644]\n",
      "train [Epoch 0/10] [Batch 1102/2345] [Iter 1102] [G_Loss 0.939639] [D_Loss 1.091644]\n",
      "train [Epoch 0/10] [Batch 1103/2345] [Iter 1103] [G_Loss 1.115809] [D_Loss 1.120903]\n",
      "train [Epoch 0/10] [Batch 1104/2345] [Iter 1104] [G_Loss 1.280685] [D_Loss 0.939351]\n",
      "train [Epoch 0/10] [Batch 1105/2345] [Iter 1105] [G_Loss 0.984461] [D_Loss 1.026980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 1106/2345] [Iter 1106] [G_Loss 2.093707] [D_Loss 1.544147]\n",
      "train [Epoch 0/10] [Batch 1107/2345] [Iter 1107] [G_Loss 0.830258] [D_Loss 1.161013]\n",
      "train [Epoch 0/10] [Batch 1108/2345] [Iter 1108] [G_Loss 0.696276] [D_Loss 1.280792]\n",
      "train [Epoch 0/10] [Batch 1109/2345] [Iter 1109] [G_Loss 0.908347] [D_Loss 1.087331]\n",
      "train [Epoch 0/10] [Batch 1110/2345] [Iter 1110] [G_Loss 1.097542] [D_Loss 0.998530]\n",
      "train [Epoch 0/10] [Batch 1111/2345] [Iter 1111] [G_Loss 1.046138] [D_Loss 1.042374]\n",
      "train [Epoch 0/10] [Batch 1112/2345] [Iter 1112] [G_Loss 0.934621] [D_Loss 1.122895]\n",
      "train [Epoch 0/10] [Batch 1113/2345] [Iter 1113] [G_Loss 1.094148] [D_Loss 1.034296]\n",
      "train [Epoch 0/10] [Batch 1114/2345] [Iter 1114] [G_Loss 1.279360] [D_Loss 0.996682]\n",
      "train [Epoch 0/10] [Batch 1115/2345] [Iter 1115] [G_Loss 0.945276] [D_Loss 1.358512]\n",
      "train [Epoch 0/10] [Batch 1116/2345] [Iter 1116] [G_Loss 1.014031] [D_Loss 1.415836]\n",
      "train [Epoch 0/10] [Batch 1117/2345] [Iter 1117] [G_Loss 1.016656] [D_Loss 1.378160]\n",
      "train [Epoch 0/10] [Batch 1118/2345] [Iter 1118] [G_Loss 1.557129] [D_Loss 1.073626]\n",
      "train [Epoch 0/10] [Batch 1119/2345] [Iter 1119] [G_Loss 1.278155] [D_Loss 0.866112]\n",
      "train [Epoch 0/10] [Batch 1120/2345] [Iter 1120] [G_Loss 1.055954] [D_Loss 0.933752]\n",
      "train [Epoch 0/10] [Batch 1121/2345] [Iter 1121] [G_Loss 1.611651] [D_Loss 0.789147]\n",
      "train [Epoch 0/10] [Batch 1122/2345] [Iter 1122] [G_Loss 1.629913] [D_Loss 0.836467]\n",
      "train [Epoch 0/10] [Batch 1123/2345] [Iter 1123] [G_Loss 1.012320] [D_Loss 0.970670]\n",
      "train [Epoch 0/10] [Batch 1124/2345] [Iter 1124] [G_Loss 1.293962] [D_Loss 0.843080]\n",
      "train [Epoch 0/10] [Batch 1125/2345] [Iter 1125] [G_Loss 1.373980] [D_Loss 0.763274]\n",
      "train [Epoch 0/10] [Batch 1126/2345] [Iter 1126] [G_Loss 1.367216] [D_Loss 0.776854]\n",
      "train [Epoch 0/10] [Batch 1127/2345] [Iter 1127] [G_Loss 1.316055] [D_Loss 0.677079]\n",
      "train [Epoch 0/10] [Batch 1128/2345] [Iter 1128] [G_Loss 1.516924] [D_Loss 0.666951]\n",
      "train [Epoch 0/10] [Batch 1129/2345] [Iter 1129] [G_Loss 1.651373] [D_Loss 0.892038]\n",
      "train [Epoch 0/10] [Batch 1130/2345] [Iter 1130] [G_Loss 0.882907] [D_Loss 1.057152]\n",
      "train [Epoch 0/10] [Batch 1131/2345] [Iter 1131] [G_Loss 2.698904] [D_Loss 1.123564]\n",
      "train [Epoch 0/10] [Batch 1132/2345] [Iter 1132] [G_Loss 1.360023] [D_Loss 0.649271]\n",
      "train [Epoch 0/10] [Batch 1133/2345] [Iter 1133] [G_Loss 0.890250] [D_Loss 0.884164]\n",
      "train [Epoch 0/10] [Batch 1134/2345] [Iter 1134] [G_Loss 0.883660] [D_Loss 2.577526]\n",
      "train [Epoch 0/10] [Batch 1135/2345] [Iter 1135] [G_Loss 2.053215] [D_Loss 1.379106]\n",
      "train [Epoch 0/10] [Batch 1136/2345] [Iter 1136] [G_Loss 2.275581] [D_Loss 0.696629]\n",
      "train [Epoch 0/10] [Batch 1137/2345] [Iter 1137] [G_Loss 1.468367] [D_Loss 0.897992]\n",
      "train [Epoch 0/10] [Batch 1138/2345] [Iter 1138] [G_Loss 12.188713] [D_Loss 0.774058]\n",
      "train [Epoch 0/10] [Batch 1139/2345] [Iter 1139] [G_Loss 5.193972] [D_Loss 0.734826]\n",
      "train [Epoch 0/10] [Batch 1140/2345] [Iter 1140] [G_Loss 5.224703] [D_Loss 1.128319]\n",
      "train [Epoch 0/10] [Batch 1141/2345] [Iter 1141] [G_Loss 12.728739] [D_Loss 0.681968]\n",
      "train [Epoch 0/10] [Batch 1142/2345] [Iter 1142] [G_Loss 3.533525] [D_Loss 0.848221]\n",
      "train [Epoch 0/10] [Batch 1143/2345] [Iter 1143] [G_Loss 2.449981] [D_Loss 0.548713]\n",
      "train [Epoch 0/10] [Batch 1144/2345] [Iter 1144] [G_Loss 2.099539] [D_Loss 0.565724]\n",
      "train [Epoch 0/10] [Batch 1145/2345] [Iter 1145] [G_Loss 2.675475] [D_Loss 0.434603]\n",
      "train [Epoch 0/10] [Batch 1146/2345] [Iter 1146] [G_Loss 2.404939] [D_Loss 0.475914]\n",
      "train [Epoch 0/10] [Batch 1147/2345] [Iter 1147] [G_Loss 2.790500] [D_Loss 0.915556]\n",
      "train [Epoch 0/10] [Batch 1148/2345] [Iter 1148] [G_Loss 1.893703] [D_Loss 0.624193]\n",
      "train [Epoch 0/10] [Batch 1149/2345] [Iter 1149] [G_Loss 1.811928] [D_Loss 0.559519]\n",
      "train [Epoch 0/10] [Batch 1150/2345] [Iter 1150] [G_Loss 2.573978] [D_Loss 0.425967]\n",
      "train [Epoch 0/10] [Batch 1151/2345] [Iter 1151] [G_Loss 1.582037] [D_Loss 0.660997]\n",
      "train [Epoch 0/10] [Batch 1152/2345] [Iter 1152] [G_Loss 3.018707] [D_Loss 0.984711]\n",
      "train [Epoch 0/10] [Batch 1153/2345] [Iter 1153] [G_Loss 1.429321] [D_Loss 0.623846]\n",
      "train [Epoch 0/10] [Batch 1154/2345] [Iter 1154] [G_Loss 2.370769] [D_Loss 0.671726]\n",
      "train [Epoch 0/10] [Batch 1155/2345] [Iter 1155] [G_Loss 2.088168] [D_Loss 0.512737]\n",
      "train [Epoch 0/10] [Batch 1156/2345] [Iter 1156] [G_Loss 1.305349] [D_Loss 0.756394]\n",
      "train [Epoch 0/10] [Batch 1157/2345] [Iter 1157] [G_Loss 1.536970] [D_Loss 0.852331]\n",
      "train [Epoch 0/10] [Batch 1158/2345] [Iter 1158] [G_Loss 1.752130] [D_Loss 0.552840]\n",
      "train [Epoch 0/10] [Batch 1159/2345] [Iter 1159] [G_Loss 0.970620] [D_Loss 0.832969]\n",
      "train [Epoch 0/10] [Batch 1160/2345] [Iter 1160] [G_Loss 3.333924] [D_Loss 1.314309]\n",
      "train [Epoch 0/10] [Batch 1161/2345] [Iter 1161] [G_Loss 2.134131] [D_Loss 0.616271]\n",
      "train [Epoch 0/10] [Batch 1162/2345] [Iter 1162] [G_Loss 1.208478] [D_Loss 0.660916]\n",
      "train [Epoch 0/10] [Batch 1163/2345] [Iter 1163] [G_Loss 1.370939] [D_Loss 0.583864]\n",
      "train [Epoch 0/10] [Batch 1164/2345] [Iter 1164] [G_Loss 1.809053] [D_Loss 0.504773]\n",
      "train [Epoch 0/10] [Batch 1165/2345] [Iter 1165] [G_Loss 2.148782] [D_Loss 0.399852]\n",
      "train [Epoch 0/10] [Batch 1166/2345] [Iter 1166] [G_Loss 2.432564] [D_Loss 0.326318]\n",
      "train [Epoch 0/10] [Batch 1167/2345] [Iter 1167] [G_Loss 2.511488] [D_Loss 0.394684]\n",
      "train [Epoch 0/10] [Batch 1168/2345] [Iter 1168] [G_Loss 1.755076] [D_Loss 0.353584]\n",
      "train [Epoch 0/10] [Batch 1169/2345] [Iter 1169] [G_Loss 1.897923] [D_Loss 0.722281]\n",
      "train [Epoch 0/10] [Batch 1170/2345] [Iter 1170] [G_Loss 1.303402] [D_Loss 1.356781]\n",
      "train [Epoch 0/10] [Batch 1171/2345] [Iter 1171] [G_Loss 1.947930] [D_Loss 1.494448]\n",
      "train [Epoch 0/10] [Batch 1172/2345] [Iter 1172] [G_Loss 2.069242] [D_Loss 0.591312]\n",
      "train [Epoch 0/10] [Batch 1173/2345] [Iter 1173] [G_Loss 2.233672] [D_Loss 0.408778]\n",
      "train [Epoch 0/10] [Batch 1174/2345] [Iter 1174] [G_Loss 2.113968] [D_Loss 0.463680]\n",
      "train [Epoch 0/10] [Batch 1175/2345] [Iter 1175] [G_Loss 1.552301] [D_Loss 0.427311]\n",
      "train [Epoch 0/10] [Batch 1176/2345] [Iter 1176] [G_Loss 1.878090] [D_Loss 0.353315]\n",
      "train [Epoch 0/10] [Batch 1177/2345] [Iter 1177] [G_Loss 1.938199] [D_Loss 0.377220]\n",
      "train [Epoch 0/10] [Batch 1178/2345] [Iter 1178] [G_Loss 1.995882] [D_Loss 0.289500]\n",
      "train [Epoch 0/10] [Batch 1179/2345] [Iter 1179] [G_Loss 2.191365] [D_Loss 0.354483]\n",
      "train [Epoch 0/10] [Batch 1180/2345] [Iter 1180] [G_Loss 2.008332] [D_Loss 0.480721]\n",
      "train [Epoch 0/10] [Batch 1181/2345] [Iter 1181] [G_Loss 1.328536] [D_Loss 1.235771]\n",
      "train [Epoch 0/10] [Batch 1182/2345] [Iter 1182] [G_Loss 1.733847] [D_Loss 1.005570]\n",
      "train [Epoch 0/10] [Batch 1183/2345] [Iter 1183] [G_Loss 2.580805] [D_Loss 0.568605]\n",
      "train [Epoch 0/10] [Batch 1184/2345] [Iter 1184] [G_Loss 4.546516] [D_Loss 0.278997]\n",
      "train [Epoch 0/10] [Batch 1185/2345] [Iter 1185] [G_Loss 1.162967] [D_Loss 0.908961]\n",
      "train [Epoch 0/10] [Batch 1186/2345] [Iter 1186] [G_Loss 2.322126] [D_Loss 0.603948]\n",
      "train [Epoch 0/10] [Batch 1187/2345] [Iter 1187] [G_Loss 2.465289] [D_Loss 0.680391]\n",
      "train [Epoch 0/10] [Batch 1188/2345] [Iter 1188] [G_Loss 2.422794] [D_Loss 0.524877]\n",
      "train [Epoch 0/10] [Batch 1189/2345] [Iter 1189] [G_Loss 1.184281] [D_Loss 1.254944]\n",
      "train [Epoch 0/10] [Batch 1190/2345] [Iter 1190] [G_Loss 2.604941] [D_Loss 0.697583]\n",
      "train [Epoch 0/10] [Batch 1191/2345] [Iter 1191] [G_Loss 3.372114] [D_Loss 0.435083]\n",
      "train [Epoch 0/10] [Batch 1192/2345] [Iter 1192] [G_Loss 1.569034] [D_Loss 0.540446]\n",
      "train [Epoch 0/10] [Batch 1193/2345] [Iter 1193] [G_Loss 2.240977] [D_Loss 0.383999]\n",
      "train [Epoch 0/10] [Batch 1194/2345] [Iter 1194] [G_Loss 2.940138] [D_Loss 0.370451]\n",
      "train [Epoch 0/10] [Batch 1195/2345] [Iter 1195] [G_Loss 0.912024] [D_Loss 1.552946]\n",
      "train [Epoch 0/10] [Batch 1196/2345] [Iter 1196] [G_Loss 2.453458] [D_Loss 1.604052]\n",
      "train [Epoch 0/10] [Batch 1197/2345] [Iter 1197] [G_Loss 2.703153] [D_Loss 0.647217]\n",
      "train [Epoch 0/10] [Batch 1198/2345] [Iter 1198] [G_Loss 1.019304] [D_Loss 1.068705]\n",
      "train [Epoch 0/10] [Batch 1199/2345] [Iter 1199] [G_Loss 2.043258] [D_Loss 0.925590]\n",
      "train [Epoch 0/10] [Batch 1200/2345] [Iter 1200] [G_Loss 2.203623] [D_Loss 0.742730]\n",
      "train [Epoch 0/10] [Batch 1201/2345] [Iter 1201] [G_Loss 1.951512] [D_Loss 0.560675]\n",
      "train [Epoch 0/10] [Batch 1202/2345] [Iter 1202] [G_Loss 1.732973] [D_Loss 0.545404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/10] [Batch 1203/2345] [Iter 1203] [G_Loss 2.041769] [D_Loss 0.625051]\n",
      "train [Epoch 0/10] [Batch 1204/2345] [Iter 1204] [G_Loss 2.032548] [D_Loss 0.516094]\n",
      "train [Epoch 0/10] [Batch 1205/2345] [Iter 1205] [G_Loss 1.484222] [D_Loss 0.571554]\n",
      "train [Epoch 0/10] [Batch 1206/2345] [Iter 1206] [G_Loss 2.272410] [D_Loss 0.546885]\n",
      "train [Epoch 0/10] [Batch 1207/2345] [Iter 1207] [G_Loss 1.596835] [D_Loss 0.476830]\n",
      "train [Epoch 0/10] [Batch 1208/2345] [Iter 1208] [G_Loss 1.812124] [D_Loss 0.670621]\n",
      "train [Epoch 0/10] [Batch 1209/2345] [Iter 1209] [G_Loss 1.357813] [D_Loss 0.956234]\n",
      "train [Epoch 0/10] [Batch 1210/2345] [Iter 1210] [G_Loss 2.621870] [D_Loss 1.057586]\n",
      "train [Epoch 0/10] [Batch 1211/2345] [Iter 1211] [G_Loss 1.002512] [D_Loss 1.345656]\n",
      "train [Epoch 0/10] [Batch 1212/2345] [Iter 1212] [G_Loss 7.795066] [D_Loss 1.717058]\n",
      "train [Epoch 0/10] [Batch 1213/2345] [Iter 1213] [G_Loss 1.268586] [D_Loss 1.005058]\n",
      "train [Epoch 0/10] [Batch 1214/2345] [Iter 1214] [G_Loss 2.134971] [D_Loss 0.585386]\n",
      "train [Epoch 0/10] [Batch 1215/2345] [Iter 1215] [G_Loss 2.447913] [D_Loss 0.507141]\n",
      "train [Epoch 0/10] [Batch 1216/2345] [Iter 1216] [G_Loss 1.722509] [D_Loss 0.448034]\n",
      "train [Epoch 0/10] [Batch 1217/2345] [Iter 1217] [G_Loss 2.089132] [D_Loss 0.501950]\n",
      "train [Epoch 0/10] [Batch 1218/2345] [Iter 1218] [G_Loss 2.150759] [D_Loss 0.554352]\n",
      "train [Epoch 0/10] [Batch 1219/2345] [Iter 1219] [G_Loss 1.921623] [D_Loss 0.477379]\n",
      "train [Epoch 0/10] [Batch 1220/2345] [Iter 1220] [G_Loss 2.781518] [D_Loss 0.271828]\n",
      "train [Epoch 0/10] [Batch 1221/2345] [Iter 1221] [G_Loss 3.124372] [D_Loss 0.305062]\n",
      "train [Epoch 0/10] [Batch 1222/2345] [Iter 1222] [G_Loss 2.451574] [D_Loss 0.358147]\n",
      "train [Epoch 0/10] [Batch 1223/2345] [Iter 1223] [G_Loss 1.786140] [D_Loss 0.424457]\n",
      "train [Epoch 0/10] [Batch 1224/2345] [Iter 1224] [G_Loss 1.727056] [D_Loss 0.517941]\n",
      "train [Epoch 0/10] [Batch 1225/2345] [Iter 1225] [G_Loss 3.852185] [D_Loss 0.610193]\n",
      "train [Epoch 0/10] [Batch 1226/2345] [Iter 1226] [G_Loss 2.691626] [D_Loss 0.652552]\n",
      "train [Epoch 0/10] [Batch 1227/2345] [Iter 1227] [G_Loss 1.145006] [D_Loss 0.991225]\n",
      "train [Epoch 0/10] [Batch 1228/2345] [Iter 1228] [G_Loss 3.902816] [D_Loss 1.211267]\n",
      "train [Epoch 0/10] [Batch 1229/2345] [Iter 1229] [G_Loss 3.331066] [D_Loss 0.229574]\n",
      "train [Epoch 0/10] [Batch 1230/2345] [Iter 1230] [G_Loss 2.477906] [D_Loss 0.372510]\n",
      "train [Epoch 0/10] [Batch 1231/2345] [Iter 1231] [G_Loss 2.958450] [D_Loss 0.153549]\n",
      "train [Epoch 0/10] [Batch 1232/2345] [Iter 1232] [G_Loss 2.349636] [D_Loss 0.389983]\n",
      "train [Epoch 0/10] [Batch 1233/2345] [Iter 1233] [G_Loss 2.325107] [D_Loss 0.559943]\n",
      "train [Epoch 0/10] [Batch 1234/2345] [Iter 1234] [G_Loss 2.618171] [D_Loss 0.664845]\n",
      "train [Epoch 0/10] [Batch 1235/2345] [Iter 1235] [G_Loss 1.981005] [D_Loss 0.728061]\n",
      "train [Epoch 0/10] [Batch 1236/2345] [Iter 1236] [G_Loss 1.683363] [D_Loss 0.375424]\n",
      "train [Epoch 0/10] [Batch 1237/2345] [Iter 1237] [G_Loss 2.360318] [D_Loss 0.454607]\n",
      "train [Epoch 0/10] [Batch 1238/2345] [Iter 1238] [G_Loss 2.693166] [D_Loss 0.360088]\n",
      "train [Epoch 0/10] [Batch 1239/2345] [Iter 1239] [G_Loss 2.279955] [D_Loss 0.290102]\n",
      "train [Epoch 0/10] [Batch 1240/2345] [Iter 1240] [G_Loss 1.682507] [D_Loss 0.644793]\n",
      "train [Epoch 0/10] [Batch 1241/2345] [Iter 1241] [G_Loss 1.671570] [D_Loss 0.931692]\n",
      "train [Epoch 0/10] [Batch 1242/2345] [Iter 1242] [G_Loss 3.549563] [D_Loss 1.179251]\n",
      "train [Epoch 0/10] [Batch 1243/2345] [Iter 1243] [G_Loss 3.490274] [D_Loss 0.359074]\n",
      "train [Epoch 0/10] [Batch 1244/2345] [Iter 1244] [G_Loss 2.349131] [D_Loss 0.272667]\n",
      "train [Epoch 0/10] [Batch 1245/2345] [Iter 1245] [G_Loss 3.895456] [D_Loss 0.103914]\n",
      "train [Epoch 0/10] [Batch 1246/2345] [Iter 1246] [G_Loss 3.577294] [D_Loss 0.255541]\n",
      "train [Epoch 0/10] [Batch 1247/2345] [Iter 1247] [G_Loss 4.528778] [D_Loss 0.094703]\n",
      "train [Epoch 0/10] [Batch 1248/2345] [Iter 1248] [G_Loss 4.778095] [D_Loss 0.221615]\n",
      "train [Epoch 0/10] [Batch 1249/2345] [Iter 1249] [G_Loss 3.462180] [D_Loss 0.413042]\n",
      "train [Epoch 0/10] [Batch 1250/2345] [Iter 1250] [G_Loss 2.762111] [D_Loss 0.491633]\n",
      "train [Epoch 0/10] [Batch 1251/2345] [Iter 1251] [G_Loss 2.610972] [D_Loss 0.537586]\n",
      "train [Epoch 0/10] [Batch 1252/2345] [Iter 1252] [G_Loss 2.496128] [D_Loss 0.323102]\n",
      "train [Epoch 0/10] [Batch 1253/2345] [Iter 1253] [G_Loss 2.692511] [D_Loss 0.326668]\n",
      "train [Epoch 0/10] [Batch 1254/2345] [Iter 1254] [G_Loss 2.647869] [D_Loss 0.218306]\n",
      "train [Epoch 0/10] [Batch 1255/2345] [Iter 1255] [G_Loss 2.887839] [D_Loss 0.261325]\n",
      "train [Epoch 0/10] [Batch 1256/2345] [Iter 1256] [G_Loss 2.737181] [D_Loss 0.197032]\n",
      "train [Epoch 0/10] [Batch 1257/2345] [Iter 1257] [G_Loss 3.256059] [D_Loss 0.136179]\n",
      "train [Epoch 0/10] [Batch 1258/2345] [Iter 1258] [G_Loss 3.240627] [D_Loss 0.199023]\n",
      "train [Epoch 0/10] [Batch 1259/2345] [Iter 1259] [G_Loss 2.507584] [D_Loss 0.371861]\n",
      "train [Epoch 0/10] [Batch 1260/2345] [Iter 1260] [G_Loss 2.260095] [D_Loss 0.253092]\n",
      "train [Epoch 0/10] [Batch 1261/2345] [Iter 1261] [G_Loss 1.821441] [D_Loss 0.938846]\n",
      "train [Epoch 0/10] [Batch 1262/2345] [Iter 1262] [G_Loss 2.751185] [D_Loss 0.909660]\n",
      "train [Epoch 0/10] [Batch 1263/2345] [Iter 1263] [G_Loss 4.099159] [D_Loss 0.243525]\n",
      "train [Epoch 0/10] [Batch 1264/2345] [Iter 1264] [G_Loss 1.795881] [D_Loss 0.641393]\n",
      "train [Epoch 0/10] [Batch 1265/2345] [Iter 1265] [G_Loss 3.425200] [D_Loss 0.404795]\n",
      "train [Epoch 0/10] [Batch 1266/2345] [Iter 1266] [G_Loss 2.859147] [D_Loss 0.648009]\n",
      "train [Epoch 0/10] [Batch 1267/2345] [Iter 1267] [G_Loss 2.043876] [D_Loss 0.624207]\n",
      "train [Epoch 0/10] [Batch 1268/2345] [Iter 1268] [G_Loss 2.206552] [D_Loss 0.473331]\n",
      "train [Epoch 0/10] [Batch 1269/2345] [Iter 1269] [G_Loss 2.281821] [D_Loss 0.628073]\n",
      "train [Epoch 0/10] [Batch 1270/2345] [Iter 1270] [G_Loss 2.168423] [D_Loss 0.588466]\n",
      "train [Epoch 0/10] [Batch 1271/2345] [Iter 1271] [G_Loss 1.909145] [D_Loss 0.593349]\n",
      "train [Epoch 0/10] [Batch 1272/2345] [Iter 1272] [G_Loss 2.159078] [D_Loss 0.644223]\n",
      "train [Epoch 0/10] [Batch 1273/2345] [Iter 1273] [G_Loss 2.419181] [D_Loss 0.375285]\n",
      "train [Epoch 0/10] [Batch 1274/2345] [Iter 1274] [G_Loss 2.470312] [D_Loss 0.245314]\n",
      "train [Epoch 0/10] [Batch 1275/2345] [Iter 1275] [G_Loss 1.811554] [D_Loss 0.514536]\n",
      "train [Epoch 0/10] [Batch 1276/2345] [Iter 1276] [G_Loss 2.710189] [D_Loss 0.620684]\n",
      "train [Epoch 0/10] [Batch 1277/2345] [Iter 1277] [G_Loss 1.333436] [D_Loss 1.745157]\n",
      "train [Epoch 0/10] [Batch 1278/2345] [Iter 1278] [G_Loss 2.489576] [D_Loss 0.958811]\n",
      "train [Epoch 0/10] [Batch 1279/2345] [Iter 1279] [G_Loss 4.764297] [D_Loss 0.517527]\n",
      "train [Epoch 0/10] [Batch 1280/2345] [Iter 1280] [G_Loss 2.809115] [D_Loss 0.404360]\n",
      "train [Epoch 0/10] [Batch 1281/2345] [Iter 1281] [G_Loss 2.096000] [D_Loss 0.719624]\n",
      "train [Epoch 0/10] [Batch 1282/2345] [Iter 1282] [G_Loss 1.951290] [D_Loss 0.886638]\n",
      "train [Epoch 0/10] [Batch 1283/2345] [Iter 1283] [G_Loss 1.992563] [D_Loss 1.014620]\n",
      "train [Epoch 0/10] [Batch 1284/2345] [Iter 1284] [G_Loss 2.032185] [D_Loss 0.883192]\n",
      "train [Epoch 0/10] [Batch 1285/2345] [Iter 1285] [G_Loss 2.538279] [D_Loss 0.623582]\n",
      "train [Epoch 0/10] [Batch 1286/2345] [Iter 1286] [G_Loss 1.376286] [D_Loss 0.615389]\n",
      "train [Epoch 0/10] [Batch 1287/2345] [Iter 1287] [G_Loss 3.120432] [D_Loss 1.117840]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_gan import *\n",
    "\n",
    "train_model(generator, discriminator, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.34s)\n",
      "creating index...\n",
      "index created!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "train [Epoch 0/3] [Batch 0/1173] [Iter 0] [G_Loss 0.963419] [D_Loss 2.063443]\n",
      "train [Epoch 0/3] [Batch 1/1173] [Iter 1] [G_Loss 0.001983] [D_Loss 6.480341]\n",
      "train [Epoch 0/3] [Batch 2/1173] [Iter 2] [G_Loss 0.103918] [D_Loss 2.333830]\n",
      "train [Epoch 0/3] [Batch 3/1173] [Iter 3] [G_Loss 0.788496] [D_Loss 0.613585]\n",
      "train [Epoch 0/3] [Batch 4/1173] [Iter 4] [G_Loss 2.168248] [D_Loss 0.582942]\n",
      "train [Epoch 0/3] [Batch 5/1173] [Iter 5] [G_Loss 2.094082] [D_Loss 0.142345]\n",
      "train [Epoch 0/3] [Batch 6/1173] [Iter 6] [G_Loss 1.959488] [D_Loss 0.167410]\n",
      "train [Epoch 0/3] [Batch 7/1173] [Iter 7] [G_Loss 2.201007] [D_Loss 0.132853]\n",
      "train [Epoch 0/3] [Batch 8/1173] [Iter 8] [G_Loss 2.200626] [D_Loss 0.122743]\n",
      "train [Epoch 0/3] [Batch 9/1173] [Iter 9] [G_Loss 3.031490] [D_Loss 0.119927]\n",
      "train [Epoch 0/3] [Batch 10/1173] [Iter 10] [G_Loss 3.198219] [D_Loss 0.041731]\n",
      "train [Epoch 0/3] [Batch 11/1173] [Iter 11] [G_Loss 3.536896] [D_Loss 0.029586]\n",
      "train [Epoch 0/3] [Batch 12/1173] [Iter 12] [G_Loss 3.953818] [D_Loss 0.019497]\n",
      "train [Epoch 0/3] [Batch 13/1173] [Iter 13] [G_Loss 4.378461] [D_Loss 0.012715]\n",
      "train [Epoch 0/3] [Batch 14/1173] [Iter 14] [G_Loss 4.756564] [D_Loss 0.008643]\n",
      "train [Epoch 0/3] [Batch 15/1173] [Iter 15] [G_Loss 5.103434] [D_Loss 0.006094]\n",
      "train [Epoch 0/3] [Batch 16/1173] [Iter 16] [G_Loss 5.393143] [D_Loss 0.004810]\n",
      "train [Epoch 0/3] [Batch 17/1173] [Iter 17] [G_Loss 5.625753] [D_Loss 0.003672]\n",
      "train [Epoch 0/3] [Batch 18/1173] [Iter 18] [G_Loss 5.809891] [D_Loss 0.003010]\n",
      "train [Epoch 0/3] [Batch 19/1173] [Iter 19] [G_Loss 5.952227] [D_Loss 0.002604]\n",
      "train [Epoch 0/3] [Batch 20/1173] [Iter 20] [G_Loss 6.069663] [D_Loss 0.002317]\n",
      "train [Epoch 0/3] [Batch 21/1173] [Iter 21] [G_Loss 6.193470] [D_Loss 0.002047]\n",
      "train [Epoch 0/3] [Batch 22/1173] [Iter 22] [G_Loss 6.312074] [D_Loss 0.001819]\n",
      "train [Epoch 0/3] [Batch 23/1173] [Iter 23] [G_Loss 6.423084] [D_Loss 0.001626]\n",
      "train [Epoch 0/3] [Batch 24/1173] [Iter 24] [G_Loss 6.526975] [D_Loss 0.001466]\n",
      "train [Epoch 0/3] [Batch 25/1173] [Iter 25] [G_Loss 6.623273] [D_Loss 0.001353]\n",
      "train [Epoch 0/3] [Batch 26/1173] [Iter 26] [G_Loss 6.712974] [D_Loss 0.001349]\n",
      "train [Epoch 0/3] [Batch 27/1173] [Iter 27] [G_Loss 6.792664] [D_Loss 0.001125]\n",
      "train [Epoch 0/3] [Batch 28/1173] [Iter 28] [G_Loss 6.877917] [D_Loss 0.014034]\n",
      "train [Epoch 0/3] [Batch 29/1173] [Iter 29] [G_Loss 6.620095] [D_Loss 0.001335]\n",
      "train [Epoch 0/3] [Batch 30/1173] [Iter 30] [G_Loss 6.507710] [D_Loss 0.012005]\n",
      "train [Epoch 0/3] [Batch 31/1173] [Iter 31] [G_Loss 6.141242] [D_Loss 0.002155]\n",
      "train [Epoch 0/3] [Batch 32/1173] [Iter 32] [G_Loss 5.997347] [D_Loss 0.002494]\n",
      "train [Epoch 0/3] [Batch 33/1173] [Iter 33] [G_Loss 6.078612] [D_Loss 0.002311]\n",
      "train [Epoch 0/3] [Batch 34/1173] [Iter 34] [G_Loss 6.259007] [D_Loss 0.001917]\n",
      "train [Epoch 0/3] [Batch 35/1173] [Iter 35] [G_Loss 6.486630] [D_Loss 0.001525]\n",
      "train [Epoch 0/3] [Batch 36/1173] [Iter 36] [G_Loss 6.714742] [D_Loss 0.001214]\n",
      "train [Epoch 0/3] [Batch 37/1173] [Iter 37] [G_Loss 6.910403] [D_Loss 0.000999]\n",
      "train [Epoch 0/3] [Batch 38/1173] [Iter 38] [G_Loss 7.070376] [D_Loss 0.000851]\n",
      "train [Epoch 0/3] [Batch 39/1173] [Iter 39] [G_Loss 7.253565] [D_Loss 0.000916]\n",
      "train [Epoch 0/3] [Batch 40/1173] [Iter 40] [G_Loss 7.407431] [D_Loss 0.020433]\n",
      "train [Epoch 0/3] [Batch 41/1173] [Iter 41] [G_Loss 5.647685] [D_Loss 0.003532]\n",
      "train [Epoch 0/3] [Batch 42/1173] [Iter 42] [G_Loss 6.086493] [D_Loss 0.002276]\n",
      "train [Epoch 0/3] [Batch 43/1173] [Iter 43] [G_Loss 7.176630] [D_Loss 0.000765]\n",
      "train [Epoch 0/3] [Batch 44/1173] [Iter 44] [G_Loss 7.587770] [D_Loss 0.000507]\n",
      "train [Epoch 0/3] [Batch 45/1173] [Iter 45] [G_Loss 7.786532] [D_Loss 0.000416]\n",
      "train [Epoch 0/3] [Batch 46/1173] [Iter 46] [G_Loss 7.913037] [D_Loss 0.000366]\n",
      "train [Epoch 0/3] [Batch 47/1173] [Iter 47] [G_Loss 8.004424] [D_Loss 0.000334]\n",
      "train [Epoch 0/3] [Batch 48/1173] [Iter 48] [G_Loss 7.962471] [D_Loss 0.000349]\n",
      "train [Epoch 0/3] [Batch 49/1173] [Iter 49] [G_Loss 7.135398] [D_Loss 0.000881]\n",
      "train [Epoch 0/3] [Batch 50/1173] [Iter 50] [G_Loss 5.039585] [D_Loss 0.021691]\n",
      "train [Epoch 0/3] [Batch 51/1173] [Iter 51] [G_Loss 0.000000] [D_Loss 38.202976]\n",
      "train [Epoch 0/3] [Batch 52/1173] [Iter 52] [G_Loss 45.510975] [D_Loss 19.659550]\n",
      "train [Epoch 0/3] [Batch 53/1173] [Iter 53] [G_Loss 3.374191] [D_Loss 1.156856]\n",
      "train [Epoch 0/3] [Batch 54/1173] [Iter 54] [G_Loss 0.947189] [D_Loss 1.208318]\n",
      "train [Epoch 0/3] [Batch 55/1173] [Iter 55] [G_Loss 1.204364] [D_Loss 0.475779]\n",
      "train [Epoch 0/3] [Batch 56/1173] [Iter 56] [G_Loss 2.495222] [D_Loss 0.168596]\n",
      "train [Epoch 0/3] [Batch 57/1173] [Iter 57] [G_Loss 5.372956] [D_Loss 0.016538]\n",
      "train [Epoch 0/3] [Batch 58/1173] [Iter 58] [G_Loss 6.229375] [D_Loss 0.050543]\n",
      "train [Epoch 0/3] [Batch 59/1173] [Iter 59] [G_Loss 4.700825] [D_Loss 0.047932]\n",
      "train [Epoch 0/3] [Batch 60/1173] [Iter 60] [G_Loss 4.221356] [D_Loss 0.078310]\n",
      "train [Epoch 0/3] [Batch 61/1173] [Iter 61] [G_Loss 5.089856] [D_Loss 0.076668]\n",
      "train [Epoch 0/3] [Batch 62/1173] [Iter 62] [G_Loss 3.033593] [D_Loss 0.057903]\n",
      "train [Epoch 0/3] [Batch 63/1173] [Iter 63] [G_Loss 14.524882] [D_Loss 0.367696]\n",
      "train [Epoch 0/3] [Batch 64/1173] [Iter 64] [G_Loss 2.118404] [D_Loss 0.180145]\n",
      "train [Epoch 0/3] [Batch 65/1173] [Iter 65] [G_Loss 2.507115] [D_Loss 0.103965]\n",
      "train [Epoch 0/3] [Batch 66/1173] [Iter 66] [G_Loss 3.019234] [D_Loss 0.061548]\n",
      "train [Epoch 0/3] [Batch 67/1173] [Iter 67] [G_Loss 2.440725] [D_Loss 0.094844]\n",
      "train [Epoch 0/3] [Batch 68/1173] [Iter 68] [G_Loss 3.564829] [D_Loss 0.034601]\n",
      "train [Epoch 0/3] [Batch 69/1173] [Iter 69] [G_Loss 4.478971] [D_Loss 0.013971]\n",
      "train [Epoch 0/3] [Batch 70/1173] [Iter 70] [G_Loss 4.710729] [D_Loss 0.011213]\n",
      "train [Epoch 0/3] [Batch 71/1173] [Iter 71] [G_Loss 5.594778] [D_Loss 0.063756]\n",
      "train [Epoch 0/3] [Batch 72/1173] [Iter 72] [G_Loss 3.679385] [D_Loss 0.033308]\n",
      "train [Epoch 0/3] [Batch 73/1173] [Iter 73] [G_Loss 4.904609] [D_Loss 0.009645]\n",
      "train [Epoch 0/3] [Batch 74/1173] [Iter 74] [G_Loss 6.316230] [D_Loss 0.001882]\n",
      "train [Epoch 0/3] [Batch 75/1173] [Iter 75] [G_Loss 6.976305] [D_Loss 0.001295]\n",
      "train [Epoch 0/3] [Batch 76/1173] [Iter 76] [G_Loss 7.332880] [D_Loss 0.050118]\n",
      "train [Epoch 0/3] [Batch 77/1173] [Iter 77] [G_Loss 5.353161] [D_Loss 0.015928]\n",
      "train [Epoch 0/3] [Batch 78/1173] [Iter 78] [G_Loss 3.703942] [D_Loss 0.028718]\n",
      "train [Epoch 0/3] [Batch 79/1173] [Iter 79] [G_Loss 7.596746] [D_Loss 0.000842]\n",
      "train [Epoch 0/3] [Batch 80/1173] [Iter 80] [G_Loss 5.180632] [D_Loss 0.008815]\n",
      "train [Epoch 0/3] [Batch 81/1173] [Iter 81] [G_Loss 4.018729] [D_Loss 0.104445]\n",
      "train [Epoch 0/3] [Batch 82/1173] [Iter 82] [G_Loss 0.026606] [D_Loss 4.550590]\n",
      "train [Epoch 0/3] [Batch 83/1173] [Iter 83] [G_Loss 3.489695] [D_Loss 14.952123]\n",
      "train [Epoch 0/3] [Batch 84/1173] [Iter 84] [G_Loss 1.291119] [D_Loss 1.161335]\n",
      "train [Epoch 0/3] [Batch 85/1173] [Iter 85] [G_Loss 1.195271] [D_Loss 0.683266]\n",
      "train [Epoch 0/3] [Batch 86/1173] [Iter 86] [G_Loss 1.336314] [D_Loss 0.359942]\n",
      "train [Epoch 0/3] [Batch 87/1173] [Iter 87] [G_Loss 1.973210] [D_Loss 0.164139]\n",
      "train [Epoch 0/3] [Batch 88/1173] [Iter 88] [G_Loss 1.987601] [D_Loss 0.202272]\n",
      "train [Epoch 0/3] [Batch 89/1173] [Iter 89] [G_Loss 1.669558] [D_Loss 0.234547]\n",
      "train [Epoch 0/3] [Batch 90/1173] [Iter 90] [G_Loss 2.370820] [D_Loss 0.168786]\n",
      "train [Epoch 0/3] [Batch 91/1173] [Iter 91] [G_Loss 2.594836] [D_Loss 0.108968]\n",
      "train [Epoch 0/3] [Batch 92/1173] [Iter 92] [G_Loss 2.770837] [D_Loss 0.066509]\n",
      "train [Epoch 0/3] [Batch 93/1173] [Iter 93] [G_Loss 3.386977] [D_Loss 0.061454]\n",
      "train [Epoch 0/3] [Batch 94/1173] [Iter 94] [G_Loss 3.689357] [D_Loss 0.067752]\n",
      "train [Epoch 0/3] [Batch 95/1173] [Iter 95] [G_Loss 3.938872] [D_Loss 0.070535]\n",
      "train [Epoch 0/3] [Batch 96/1173] [Iter 96] [G_Loss 3.947364] [D_Loss 0.041491]\n",
      "train [Epoch 0/3] [Batch 97/1173] [Iter 97] [G_Loss 3.742125] [D_Loss 0.093861]\n",
      "train [Epoch 0/3] [Batch 98/1173] [Iter 98] [G_Loss 3.616921] [D_Loss 0.027967]\n",
      "train [Epoch 0/3] [Batch 99/1173] [Iter 99] [G_Loss 3.980679] [D_Loss 0.042874]\n",
      "train [Epoch 0/3] [Batch 100/1173] [Iter 100] [G_Loss 3.960384] [D_Loss 0.027051]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/3] [Batch 101/1173] [Iter 101] [G_Loss 4.038007] [D_Loss 0.023241]\n",
      "train [Epoch 0/3] [Batch 102/1173] [Iter 102] [G_Loss 4.314389] [D_Loss 0.013570]\n",
      "train [Epoch 0/3] [Batch 103/1173] [Iter 103] [G_Loss 4.654500] [D_Loss 0.009749]\n",
      "train [Epoch 0/3] [Batch 104/1173] [Iter 104] [G_Loss 4.974891] [D_Loss 0.007363]\n",
      "train [Epoch 0/3] [Batch 105/1173] [Iter 105] [G_Loss 5.097198] [D_Loss 0.006409]\n",
      "train [Epoch 0/3] [Batch 106/1173] [Iter 106] [G_Loss 5.195259] [D_Loss 0.012560]\n",
      "train [Epoch 0/3] [Batch 107/1173] [Iter 107] [G_Loss 5.142476] [D_Loss 0.008472]\n",
      "train [Epoch 0/3] [Batch 108/1173] [Iter 108] [G_Loss 4.652591] [D_Loss 0.012650]\n",
      "train [Epoch 0/3] [Batch 109/1173] [Iter 109] [G_Loss 3.821538] [D_Loss 0.026219]\n",
      "train [Epoch 0/3] [Batch 110/1173] [Iter 110] [G_Loss 3.512923] [D_Loss 0.035662]\n",
      "train [Epoch 0/3] [Batch 111/1173] [Iter 111] [G_Loss 3.433616] [D_Loss 0.056115]\n",
      "train [Epoch 0/3] [Batch 112/1173] [Iter 112] [G_Loss 3.328620] [D_Loss 0.041244]\n",
      "train [Epoch 0/3] [Batch 113/1173] [Iter 113] [G_Loss 3.188745] [D_Loss 0.069793]\n",
      "train [Epoch 0/3] [Batch 114/1173] [Iter 114] [G_Loss 2.104521] [D_Loss 0.220755]\n",
      "train [Epoch 0/3] [Batch 115/1173] [Iter 115] [G_Loss 2.211251] [D_Loss 0.200155]\n",
      "train [Epoch 0/3] [Batch 116/1173] [Iter 116] [G_Loss 5.922401] [D_Loss 0.945665]\n",
      "train [Epoch 0/3] [Batch 117/1173] [Iter 117] [G_Loss 0.067827] [D_Loss 4.960860]\n",
      "train [Epoch 0/3] [Batch 118/1173] [Iter 118] [G_Loss 8.914749] [D_Loss 0.000208]\n",
      "train [Epoch 0/3] [Batch 119/1173] [Iter 119] [G_Loss 10.400613] [D_Loss 0.000046]\n",
      "train [Epoch 0/3] [Batch 120/1173] [Iter 120] [G_Loss 9.533405] [D_Loss 0.017265]\n",
      "train [Epoch 0/3] [Batch 121/1173] [Iter 121] [G_Loss 8.490371] [D_Loss 0.005012]\n",
      "train [Epoch 0/3] [Batch 122/1173] [Iter 122] [G_Loss 7.600529] [D_Loss 0.025380]\n",
      "train [Epoch 0/3] [Batch 123/1173] [Iter 123] [G_Loss 6.855546] [D_Loss 0.001157]\n",
      "train [Epoch 0/3] [Batch 124/1173] [Iter 124] [G_Loss 6.025727] [D_Loss 0.002573]\n",
      "train [Epoch 0/3] [Batch 125/1173] [Iter 125] [G_Loss 5.423264] [D_Loss 0.004519]\n",
      "train [Epoch 0/3] [Batch 126/1173] [Iter 126] [G_Loss 5.116809] [D_Loss 0.007253]\n",
      "train [Epoch 0/3] [Batch 127/1173] [Iter 127] [G_Loss 4.888361] [D_Loss 0.093081]\n",
      "train [Epoch 0/3] [Batch 128/1173] [Iter 128] [G_Loss 3.906984] [D_Loss 0.020572]\n",
      "train [Epoch 0/3] [Batch 129/1173] [Iter 129] [G_Loss 3.032628] [D_Loss 0.049573]\n",
      "train [Epoch 0/3] [Batch 130/1173] [Iter 130] [G_Loss 2.954368] [D_Loss 0.058347]\n",
      "train [Epoch 0/3] [Batch 131/1173] [Iter 131] [G_Loss 3.175511] [D_Loss 0.043415]\n",
      "train [Epoch 0/3] [Batch 132/1173] [Iter 132] [G_Loss 3.452966] [D_Loss 0.033015]\n",
      "train [Epoch 0/3] [Batch 133/1173] [Iter 133] [G_Loss 3.767301] [D_Loss 0.023430]\n",
      "train [Epoch 0/3] [Batch 134/1173] [Iter 134] [G_Loss 4.053389] [D_Loss 0.017710]\n",
      "train [Epoch 0/3] [Batch 135/1173] [Iter 135] [G_Loss 4.279930] [D_Loss 0.014236]\n",
      "train [Epoch 0/3] [Batch 136/1173] [Iter 136] [G_Loss 4.477297] [D_Loss 0.011434]\n",
      "train [Epoch 0/3] [Batch 137/1173] [Iter 137] [G_Loss 4.014139] [D_Loss 0.019290]\n",
      "train [Epoch 0/3] [Batch 138/1173] [Iter 138] [G_Loss 2.260607] [D_Loss 0.110151]\n",
      "train [Epoch 0/3] [Batch 139/1173] [Iter 139] [G_Loss 4.806365] [D_Loss 0.008243]\n",
      "train [Epoch 0/3] [Batch 140/1173] [Iter 140] [G_Loss 6.500801] [D_Loss 0.028827]\n",
      "train [Epoch 0/3] [Batch 141/1173] [Iter 141] [G_Loss 6.997090] [D_Loss 0.048144]\n",
      "train [Epoch 0/3] [Batch 142/1173] [Iter 142] [G_Loss 6.973102] [D_Loss 0.000941]\n",
      "train [Epoch 0/3] [Batch 143/1173] [Iter 143] [G_Loss 6.889894] [D_Loss 0.134842]\n",
      "train [Epoch 0/3] [Batch 144/1173] [Iter 144] [G_Loss 6.542725] [D_Loss 0.039740]\n",
      "train [Epoch 0/3] [Batch 145/1173] [Iter 145] [G_Loss 6.131688] [D_Loss 0.008443]\n",
      "train [Epoch 0/3] [Batch 146/1173] [Iter 146] [G_Loss 5.843709] [D_Loss 0.002903]\n",
      "train [Epoch 0/3] [Batch 147/1173] [Iter 147] [G_Loss 5.686236] [D_Loss 0.003405]\n",
      "train [Epoch 0/3] [Batch 148/1173] [Iter 148] [G_Loss 5.556229] [D_Loss 0.041348]\n",
      "train [Epoch 0/3] [Batch 149/1173] [Iter 149] [G_Loss 5.444824] [D_Loss 0.004331]\n",
      "train [Epoch 0/3] [Batch 150/1173] [Iter 150] [G_Loss 5.220203] [D_Loss 0.005430]\n",
      "train [Epoch 0/3] [Batch 151/1173] [Iter 151] [G_Loss 5.145517] [D_Loss 0.025333]\n",
      "train [Epoch 0/3] [Batch 152/1173] [Iter 152] [G_Loss 4.883265] [D_Loss 0.007611]\n",
      "train [Epoch 0/3] [Batch 153/1173] [Iter 153] [G_Loss 4.976943] [D_Loss 0.042209]\n",
      "train [Epoch 0/3] [Batch 154/1173] [Iter 154] [G_Loss 5.022571] [D_Loss 0.009236]\n",
      "train [Epoch 0/3] [Batch 155/1173] [Iter 155] [G_Loss 5.241634] [D_Loss 0.005306]\n",
      "train [Epoch 0/3] [Batch 156/1173] [Iter 156] [G_Loss 5.500277] [D_Loss 0.006067]\n",
      "train [Epoch 0/3] [Batch 157/1173] [Iter 157] [G_Loss 5.684626] [D_Loss 0.009739]\n",
      "train [Epoch 0/3] [Batch 158/1173] [Iter 158] [G_Loss 5.771391] [D_Loss 0.003121]\n",
      "train [Epoch 0/3] [Batch 159/1173] [Iter 159] [G_Loss 5.888722] [D_Loss 0.002775]\n",
      "train [Epoch 0/3] [Batch 160/1173] [Iter 160] [G_Loss 6.087379] [D_Loss 0.002275]\n",
      "train [Epoch 0/3] [Batch 161/1173] [Iter 161] [G_Loss 6.259326] [D_Loss 0.029490]\n",
      "train [Epoch 0/3] [Batch 162/1173] [Iter 162] [G_Loss 5.522961] [D_Loss 0.022789]\n",
      "train [Epoch 0/3] [Batch 163/1173] [Iter 163] [G_Loss 5.208377] [D_Loss 0.014721]\n",
      "train [Epoch 0/3] [Batch 164/1173] [Iter 164] [G_Loss 5.219115] [D_Loss 0.005806]\n",
      "train [Epoch 0/3] [Batch 165/1173] [Iter 165] [G_Loss 5.778354] [D_Loss 0.017481]\n",
      "train [Epoch 0/3] [Batch 166/1173] [Iter 166] [G_Loss 5.877790] [D_Loss 0.002820]\n",
      "train [Epoch 0/3] [Batch 167/1173] [Iter 167] [G_Loss 6.513703] [D_Loss 0.001492]\n",
      "train [Epoch 0/3] [Batch 168/1173] [Iter 168] [G_Loss 6.834765] [D_Loss 0.001077]\n",
      "train [Epoch 0/3] [Batch 169/1173] [Iter 169] [G_Loss 7.118535] [D_Loss 0.000812]\n",
      "train [Epoch 0/3] [Batch 170/1173] [Iter 170] [G_Loss 7.260629] [D_Loss 0.053975]\n",
      "train [Epoch 0/3] [Batch 171/1173] [Iter 171] [G_Loss 6.378552] [D_Loss 0.001701]\n",
      "train [Epoch 0/3] [Batch 172/1173] [Iter 172] [G_Loss 6.190555] [D_Loss 0.002521]\n",
      "train [Epoch 0/3] [Batch 173/1173] [Iter 173] [G_Loss 6.746755] [D_Loss 0.001179]\n",
      "train [Epoch 0/3] [Batch 174/1173] [Iter 174] [G_Loss 7.282583] [D_Loss 0.000691]\n",
      "train [Epoch 0/3] [Batch 175/1173] [Iter 175] [G_Loss 7.469624] [D_Loss 0.000580]\n",
      "train [Epoch 0/3] [Batch 176/1173] [Iter 176] [G_Loss 7.565074] [D_Loss 0.000525]\n",
      "train [Epoch 0/3] [Batch 177/1173] [Iter 177] [G_Loss 7.541521] [D_Loss 0.000538]\n",
      "train [Epoch 0/3] [Batch 178/1173] [Iter 178] [G_Loss 7.297778] [D_Loss 0.000704]\n",
      "train [Epoch 0/3] [Batch 179/1173] [Iter 179] [G_Loss 7.786524] [D_Loss 0.018418]\n",
      "train [Epoch 0/3] [Batch 180/1173] [Iter 180] [G_Loss 7.336359] [D_Loss 0.032919]\n",
      "train [Epoch 0/3] [Batch 181/1173] [Iter 181] [G_Loss 0.042979] [D_Loss 3.636288]\n",
      "train [Epoch 0/3] [Batch 182/1173] [Iter 182] [G_Loss 38.590649] [D_Loss 5.629942]\n",
      "train [Epoch 0/3] [Batch 183/1173] [Iter 183] [G_Loss 23.679155] [D_Loss 0.072586]\n",
      "train [Epoch 0/3] [Batch 184/1173] [Iter 184] [G_Loss 18.181030] [D_Loss 0.000000]\n",
      "train [Epoch 0/3] [Batch 185/1173] [Iter 185] [G_Loss 12.848586] [D_Loss 0.000029]\n",
      "train [Epoch 0/3] [Batch 186/1173] [Iter 186] [G_Loss 8.380442] [D_Loss 0.000399]\n",
      "train [Epoch 0/3] [Batch 187/1173] [Iter 187] [G_Loss 7.484374] [D_Loss 0.000950]\n",
      "train [Epoch 0/3] [Batch 188/1173] [Iter 188] [G_Loss 7.595852] [D_Loss 0.000623]\n",
      "train [Epoch 0/3] [Batch 189/1173] [Iter 189] [G_Loss 6.795449] [D_Loss 0.062343]\n",
      "train [Epoch 0/3] [Batch 190/1173] [Iter 190] [G_Loss 5.747631] [D_Loss 0.004176]\n",
      "train [Epoch 0/3] [Batch 191/1173] [Iter 191] [G_Loss 4.558280] [D_Loss 0.015277]\n",
      "train [Epoch 0/3] [Batch 192/1173] [Iter 192] [G_Loss 3.760610] [D_Loss 0.038450]\n",
      "train [Epoch 0/3] [Batch 193/1173] [Iter 193] [G_Loss 2.340688] [D_Loss 0.146076]\n",
      "train [Epoch 0/3] [Batch 194/1173] [Iter 194] [G_Loss 1.923388] [D_Loss 0.200548]\n",
      "train [Epoch 0/3] [Batch 195/1173] [Iter 195] [G_Loss 3.024658] [D_Loss 0.113057]\n",
      "train [Epoch 0/3] [Batch 196/1173] [Iter 196] [G_Loss 0.016906] [D_Loss 8.485934]\n",
      "train [Epoch 0/3] [Batch 197/1173] [Iter 197] [G_Loss 25.875080] [D_Loss 32.470306]\n",
      "train [Epoch 0/3] [Batch 198/1173] [Iter 198] [G_Loss 1.636625] [D_Loss 0.534714]\n",
      "train [Epoch 0/3] [Batch 199/1173] [Iter 199] [G_Loss 2.018675] [D_Loss 0.499331]\n",
      "train [Epoch 0/3] [Batch 200/1173] [Iter 200] [G_Loss 1.243326] [D_Loss 0.534538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/3] [Batch 201/1173] [Iter 201] [G_Loss 0.774668] [D_Loss 0.947228]\n",
      "train [Epoch 0/3] [Batch 202/1173] [Iter 202] [G_Loss 1.147823] [D_Loss 1.148163]\n",
      "train [Epoch 0/3] [Batch 203/1173] [Iter 203] [G_Loss 0.730319] [D_Loss 1.523651]\n",
      "train [Epoch 0/3] [Batch 204/1173] [Iter 204] [G_Loss 0.903654] [D_Loss 1.250882]\n",
      "train [Epoch 0/3] [Batch 205/1173] [Iter 205] [G_Loss 1.296283] [D_Loss 1.229196]\n",
      "train [Epoch 0/3] [Batch 206/1173] [Iter 206] [G_Loss 0.374643] [D_Loss 2.922107]\n",
      "train [Epoch 0/3] [Batch 207/1173] [Iter 207] [G_Loss 12.791457] [D_Loss 3.490171]\n",
      "train [Epoch 0/3] [Batch 208/1173] [Iter 208] [G_Loss 0.997311] [D_Loss 1.383348]\n",
      "train [Epoch 0/3] [Batch 209/1173] [Iter 209] [G_Loss 0.591324] [D_Loss 1.134980]\n",
      "train [Epoch 0/3] [Batch 210/1173] [Iter 210] [G_Loss 0.972574] [D_Loss 0.734773]\n",
      "train [Epoch 0/3] [Batch 211/1173] [Iter 211] [G_Loss 4.359965] [D_Loss 0.262372]\n",
      "train [Epoch 0/3] [Batch 212/1173] [Iter 212] [G_Loss 0.380560] [D_Loss 1.370839]\n",
      "train [Epoch 0/3] [Batch 213/1173] [Iter 213] [G_Loss 0.514922] [D_Loss 1.450486]\n",
      "train [Epoch 0/3] [Batch 214/1173] [Iter 214] [G_Loss 0.828279] [D_Loss 1.273288]\n",
      "train [Epoch 0/3] [Batch 215/1173] [Iter 215] [G_Loss 0.992652] [D_Loss 1.242451]\n",
      "train [Epoch 0/3] [Batch 216/1173] [Iter 216] [G_Loss 0.983975] [D_Loss 1.053090]\n",
      "train [Epoch 0/3] [Batch 217/1173] [Iter 217] [G_Loss 0.960016] [D_Loss 1.035334]\n",
      "train [Epoch 0/3] [Batch 218/1173] [Iter 218] [G_Loss 1.167957] [D_Loss 0.885542]\n",
      "train [Epoch 0/3] [Batch 219/1173] [Iter 219] [G_Loss 1.202960] [D_Loss 0.814925]\n",
      "train [Epoch 0/3] [Batch 220/1173] [Iter 220] [G_Loss 1.207921] [D_Loss 0.702551]\n",
      "train [Epoch 0/3] [Batch 221/1173] [Iter 221] [G_Loss 1.502626] [D_Loss 0.563477]\n",
      "train [Epoch 0/3] [Batch 222/1173] [Iter 222] [G_Loss 1.622197] [D_Loss 0.464644]\n",
      "train [Epoch 0/3] [Batch 223/1173] [Iter 223] [G_Loss 1.709508] [D_Loss 0.345915]\n",
      "train [Epoch 0/3] [Batch 224/1173] [Iter 224] [G_Loss 2.354914] [D_Loss 0.293555]\n",
      "train [Epoch 0/3] [Batch 225/1173] [Iter 225] [G_Loss 1.495761] [D_Loss 0.319517]\n",
      "train [Epoch 0/3] [Batch 226/1173] [Iter 226] [G_Loss 2.823810] [D_Loss 0.388543]\n",
      "train [Epoch 0/3] [Batch 227/1173] [Iter 227] [G_Loss 1.098502] [D_Loss 0.453887]\n",
      "train [Epoch 0/3] [Batch 228/1173] [Iter 228] [G_Loss 2.943880] [D_Loss 0.411495]\n",
      "train [Epoch 0/3] [Batch 232/1173] [Iter 232] [G_Loss 2.911958] [D_Loss 1.343670]\n",
      "train [Epoch 0/3] [Batch 233/1173] [Iter 233] [G_Loss 0.161250] [D_Loss 2.440953]\n",
      "train [Epoch 0/3] [Batch 234/1173] [Iter 234] [G_Loss 1.250220] [D_Loss 0.591203]\n",
      "train [Epoch 0/3] [Batch 235/1173] [Iter 235] [G_Loss 1.736974] [D_Loss 0.594168]\n",
      "train [Epoch 0/3] [Batch 236/1173] [Iter 236] [G_Loss 0.970054] [D_Loss 0.695176]\n",
      "train [Epoch 0/3] [Batch 237/1173] [Iter 237] [G_Loss 1.423887] [D_Loss 0.542615]\n",
      "train [Epoch 0/3] [Batch 238/1173] [Iter 238] [G_Loss 1.506380] [D_Loss 0.494865]\n",
      "train [Epoch 0/3] [Batch 239/1173] [Iter 239] [G_Loss 1.459874] [D_Loss 0.481273]\n",
      "train [Epoch 0/3] [Batch 240/1173] [Iter 240] [G_Loss 1.291005] [D_Loss 0.522006]\n",
      "train [Epoch 0/3] [Batch 241/1173] [Iter 241] [G_Loss 1.241304] [D_Loss 0.513449]\n",
      "train [Epoch 0/3] [Batch 242/1173] [Iter 242] [G_Loss 1.229236] [D_Loss 0.520074]\n",
      "train [Epoch 0/3] [Batch 243/1173] [Iter 243] [G_Loss 1.269972] [D_Loss 0.467847]\n",
      "train [Epoch 0/3] [Batch 244/1173] [Iter 244] [G_Loss 1.227169] [D_Loss 0.540494]\n",
      "train [Epoch 0/3] [Batch 245/1173] [Iter 245] [G_Loss 1.200069] [D_Loss 0.512216]\n",
      "train [Epoch 0/3] [Batch 246/1173] [Iter 246] [G_Loss 1.185798] [D_Loss 0.497989]\n",
      "train [Epoch 0/3] [Batch 247/1173] [Iter 247] [G_Loss 1.222515] [D_Loss 0.424209]\n",
      "train [Epoch 0/3] [Batch 248/1173] [Iter 248] [G_Loss 1.346614] [D_Loss 0.394541]\n",
      "train [Epoch 0/3] [Batch 249/1173] [Iter 249] [G_Loss 1.537308] [D_Loss 0.352396]\n",
      "train [Epoch 0/3] [Batch 250/1173] [Iter 250] [G_Loss 1.664228] [D_Loss 0.254306]\n",
      "train [Epoch 0/3] [Batch 251/1173] [Iter 251] [G_Loss 1.837682] [D_Loss 0.228065]\n",
      "train [Epoch 0/3] [Batch 252/1173] [Iter 252] [G_Loss 2.003863] [D_Loss 0.179575]\n",
      "train [Epoch 0/3] [Batch 253/1173] [Iter 253] [G_Loss 2.188665] [D_Loss 0.155130]\n",
      "train [Epoch 0/3] [Batch 254/1173] [Iter 254] [G_Loss 2.392464] [D_Loss 0.129761]\n",
      "train [Epoch 0/3] [Batch 255/1173] [Iter 255] [G_Loss 2.484003] [D_Loss 0.127650]\n",
      "train [Epoch 0/3] [Batch 256/1173] [Iter 256] [G_Loss 2.489124] [D_Loss 0.126379]\n",
      "train [Epoch 0/3] [Batch 257/1173] [Iter 257] [G_Loss 2.505306] [D_Loss 0.151627]\n",
      "train [Epoch 0/3] [Batch 258/1173] [Iter 258] [G_Loss 2.454326] [D_Loss 0.114805]\n",
      "train [Epoch 0/3] [Batch 259/1173] [Iter 259] [G_Loss 2.426466] [D_Loss 0.113095]\n",
      "train [Epoch 0/3] [Batch 260/1173] [Iter 260] [G_Loss 2.620836] [D_Loss 0.153254]\n",
      "train [Epoch 0/3] [Batch 261/1173] [Iter 261] [G_Loss 2.413964] [D_Loss 0.142168]\n",
      "train [Epoch 0/3] [Batch 262/1173] [Iter 262] [G_Loss 2.542259] [D_Loss 0.124223]\n",
      "train [Epoch 0/3] [Batch 263/1173] [Iter 263] [G_Loss 2.723859] [D_Loss 0.101281]\n",
      "train [Epoch 0/3] [Batch 264/1173] [Iter 264] [G_Loss 2.913043] [D_Loss 0.129077]\n",
      "train [Epoch 0/3] [Batch 265/1173] [Iter 265] [G_Loss 3.223961] [D_Loss 0.122762]\n",
      "train [Epoch 0/3] [Batch 266/1173] [Iter 266] [G_Loss 2.681324] [D_Loss 0.132918]\n",
      "train [Epoch 0/3] [Batch 267/1173] [Iter 267] [G_Loss 3.719992] [D_Loss 0.136182]\n",
      "train [Epoch 0/3] [Batch 268/1173] [Iter 268] [G_Loss 3.844278] [D_Loss 0.099251]\n",
      "train [Epoch 0/3] [Batch 269/1173] [Iter 269] [G_Loss 3.194686] [D_Loss 0.145405]\n",
      "train [Epoch 0/3] [Batch 270/1173] [Iter 270] [G_Loss 3.889018] [D_Loss 0.123221]\n",
      "train [Epoch 0/3] [Batch 271/1173] [Iter 271] [G_Loss 3.465317] [D_Loss 0.093931]\n",
      "train [Epoch 0/3] [Batch 272/1173] [Iter 272] [G_Loss 3.426844] [D_Loss 0.180096]\n",
      "train [Epoch 0/3] [Batch 273/1173] [Iter 273] [G_Loss 4.441396] [D_Loss 0.363504]\n",
      "train [Epoch 0/3] [Batch 274/1173] [Iter 274] [G_Loss 0.581348] [D_Loss 2.784105]\n",
      "train [Epoch 0/3] [Batch 275/1173] [Iter 275] [G_Loss 5.116679] [D_Loss 3.030043]\n",
      "train [Epoch 0/3] [Batch 276/1173] [Iter 276] [G_Loss 1.490954] [D_Loss 0.598391]\n",
      "train [Epoch 0/3] [Batch 277/1173] [Iter 277] [G_Loss 0.652425] [D_Loss 1.252708]\n",
      "train [Epoch 0/3] [Batch 278/1173] [Iter 278] [G_Loss 1.202432] [D_Loss 0.664522]\n",
      "train [Epoch 0/3] [Batch 279/1173] [Iter 279] [G_Loss 2.095423] [D_Loss 0.558910]\n",
      "train [Epoch 0/3] [Batch 280/1173] [Iter 280] [G_Loss 2.255648] [D_Loss 0.920705]\n",
      "train [Epoch 0/3] [Batch 281/1173] [Iter 281] [G_Loss 1.911692] [D_Loss 0.651236]\n",
      "train [Epoch 0/3] [Batch 282/1173] [Iter 282] [G_Loss 1.720363] [D_Loss 0.512496]\n",
      "train [Epoch 0/3] [Batch 283/1173] [Iter 283] [G_Loss 2.021077] [D_Loss 0.381945]\n",
      "train [Epoch 0/3] [Batch 284/1173] [Iter 284] [G_Loss 2.411490] [D_Loss 0.483299]\n",
      "train [Epoch 0/3] [Batch 285/1173] [Iter 285] [G_Loss 2.862241] [D_Loss 0.296034]\n",
      "train [Epoch 0/3] [Batch 286/1173] [Iter 286] [G_Loss 2.695206] [D_Loss 0.304682]\n",
      "train [Epoch 0/3] [Batch 287/1173] [Iter 287] [G_Loss 2.294814] [D_Loss 0.348059]\n",
      "train [Epoch 0/3] [Batch 288/1173] [Iter 288] [G_Loss 3.105606] [D_Loss 0.352318]\n",
      "train [Epoch 0/3] [Batch 289/1173] [Iter 289] [G_Loss 3.036415] [D_Loss 0.385056]\n",
      "train [Epoch 0/3] [Batch 290/1173] [Iter 290] [G_Loss 2.441427] [D_Loss 0.354027]\n",
      "train [Epoch 0/3] [Batch 291/1173] [Iter 291] [G_Loss 0.141604] [D_Loss 3.004214]\n",
      "train [Epoch 0/3] [Batch 292/1173] [Iter 292] [G_Loss 5.803186] [D_Loss 0.220207]\n",
      "train [Epoch 0/3] [Batch 293/1173] [Iter 293] [G_Loss 8.395311] [D_Loss 0.187991]\n",
      "train [Epoch 0/3] [Batch 294/1173] [Iter 294] [G_Loss 5.600525] [D_Loss 0.079012]\n",
      "train [Epoch 0/3] [Batch 295/1173] [Iter 295] [G_Loss 0.042528] [D_Loss 5.653968]\n",
      "train [Epoch 0/3] [Batch 296/1173] [Iter 296] [G_Loss 8.101337] [D_Loss 2.253312]\n",
      "train [Epoch 0/3] [Batch 297/1173] [Iter 297] [G_Loss 6.122441] [D_Loss 1.620942]\n",
      "train [Epoch 0/3] [Batch 298/1173] [Iter 298] [G_Loss 2.703572] [D_Loss 0.940705]\n",
      "train [Epoch 0/3] [Batch 299/1173] [Iter 299] [G_Loss 1.537689] [D_Loss 0.583963]\n",
      "train [Epoch 0/3] [Batch 300/1173] [Iter 300] [G_Loss 0.791913] [D_Loss 0.926720]\n",
      "train [Epoch 0/3] [Batch 301/1173] [Iter 301] [G_Loss 1.569728] [D_Loss 0.561682]\n",
      "train [Epoch 0/3] [Batch 302/1173] [Iter 302] [G_Loss 2.055075] [D_Loss 0.632962]\n",
      "train [Epoch 0/3] [Batch 303/1173] [Iter 303] [G_Loss 2.435844] [D_Loss 0.458381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/3] [Batch 304/1173] [Iter 304] [G_Loss 1.949558] [D_Loss 0.446500]\n",
      "train [Epoch 0/3] [Batch 305/1173] [Iter 305] [G_Loss 2.018413] [D_Loss 0.308283]\n",
      "train [Epoch 0/3] [Batch 306/1173] [Iter 306] [G_Loss 2.090915] [D_Loss 0.275398]\n",
      "train [Epoch 0/3] [Batch 307/1173] [Iter 307] [G_Loss 2.067968] [D_Loss 0.263254]\n",
      "train [Epoch 0/3] [Batch 308/1173] [Iter 308] [G_Loss 2.372608] [D_Loss 0.233332]\n",
      "train [Epoch 0/3] [Batch 309/1173] [Iter 309] [G_Loss 2.869186] [D_Loss 0.261765]\n",
      "train [Epoch 0/3] [Batch 310/1173] [Iter 310] [G_Loss 2.638299] [D_Loss 0.232991]\n",
      "train [Epoch 0/3] [Batch 311/1173] [Iter 311] [G_Loss 2.941142] [D_Loss 0.191568]\n",
      "train [Epoch 0/3] [Batch 312/1173] [Iter 312] [G_Loss 2.743655] [D_Loss 0.214575]\n",
      "train [Epoch 0/3] [Batch 313/1173] [Iter 313] [G_Loss 2.776338] [D_Loss 0.179326]\n",
      "train [Epoch 0/3] [Batch 314/1173] [Iter 314] [G_Loss 2.942082] [D_Loss 0.182761]\n",
      "train [Epoch 0/3] [Batch 315/1173] [Iter 315] [G_Loss 3.115427] [D_Loss 0.169762]\n",
      "train [Epoch 0/3] [Batch 316/1173] [Iter 316] [G_Loss 3.225920] [D_Loss 0.153939]\n",
      "train [Epoch 0/3] [Batch 317/1173] [Iter 317] [G_Loss 2.662117] [D_Loss 0.233171]\n",
      "train [Epoch 0/3] [Batch 318/1173] [Iter 318] [G_Loss 3.543822] [D_Loss 0.263467]\n",
      "train [Epoch 0/3] [Batch 319/1173] [Iter 319] [G_Loss 2.413926] [D_Loss 0.294555]\n",
      "train [Epoch 0/3] [Batch 320/1173] [Iter 320] [G_Loss 3.669732] [D_Loss 0.372225]\n",
      "train [Epoch 0/3] [Batch 321/1173] [Iter 321] [G_Loss 2.024933] [D_Loss 0.384414]\n",
      "train [Epoch 0/3] [Batch 322/1173] [Iter 322] [G_Loss 3.148955] [D_Loss 0.345838]\n",
      "train [Epoch 0/3] [Batch 323/1173] [Iter 323] [G_Loss 2.912433] [D_Loss 0.297776]\n",
      "train [Epoch 0/3] [Batch 324/1173] [Iter 324] [G_Loss 2.756486] [D_Loss 0.306640]\n",
      "train [Epoch 0/3] [Batch 325/1173] [Iter 325] [G_Loss 4.241847] [D_Loss 0.401566]\n",
      "train [Epoch 0/3] [Batch 326/1173] [Iter 326] [G_Loss 1.916526] [D_Loss 0.600614]\n",
      "train [Epoch 0/3] [Batch 327/1173] [Iter 327] [G_Loss 7.361955] [D_Loss 1.709181]\n",
      "train [Epoch 0/3] [Batch 328/1173] [Iter 328] [G_Loss 2.176238] [D_Loss 0.356719]\n",
      "train [Epoch 0/3] [Batch 329/1173] [Iter 329] [G_Loss 0.830643] [D_Loss 0.940057]\n",
      "train [Epoch 0/3] [Batch 330/1173] [Iter 330] [G_Loss 6.766840] [D_Loss 0.824744]\n",
      "train [Epoch 0/3] [Batch 331/1173] [Iter 331] [G_Loss 5.173633] [D_Loss 0.721402]\n",
      "train [Epoch 0/3] [Batch 332/1173] [Iter 332] [G_Loss 2.904409] [D_Loss 0.341225]\n",
      "train [Epoch 0/3] [Batch 333/1173] [Iter 333] [G_Loss 1.132152] [D_Loss 0.636362]\n",
      "train [Epoch 0/3] [Batch 334/1173] [Iter 334] [G_Loss 1.196559] [D_Loss 0.640116]\n",
      "train [Epoch 0/3] [Batch 335/1173] [Iter 335] [G_Loss 4.235999] [D_Loss 0.457083]\n",
      "train [Epoch 0/3] [Batch 336/1173] [Iter 336] [G_Loss 3.528834] [D_Loss 0.333885]\n",
      "train [Epoch 0/3] [Batch 337/1173] [Iter 337] [G_Loss 2.633556] [D_Loss 0.353068]\n",
      "train [Epoch 0/3] [Batch 338/1173] [Iter 338] [G_Loss 2.176590] [D_Loss 0.338727]\n",
      "train [Epoch 0/3] [Batch 339/1173] [Iter 339] [G_Loss 3.407633] [D_Loss 0.327278]\n",
      "train [Epoch 0/3] [Batch 340/1173] [Iter 340] [G_Loss 3.654495] [D_Loss 0.217083]\n",
      "train [Epoch 0/3] [Batch 341/1173] [Iter 341] [G_Loss 2.988683] [D_Loss 0.294632]\n",
      "train [Epoch 0/3] [Batch 342/1173] [Iter 342] [G_Loss 3.474372] [D_Loss 0.243493]\n",
      "train [Epoch 0/3] [Batch 343/1173] [Iter 343] [G_Loss 4.071714] [D_Loss 0.144103]\n",
      "train [Epoch 0/3] [Batch 344/1173] [Iter 344] [G_Loss 4.385373] [D_Loss 0.190708]\n",
      "train [Epoch 0/3] [Batch 345/1173] [Iter 345] [G_Loss 3.392673] [D_Loss 0.184686]\n",
      "train [Epoch 0/3] [Batch 346/1173] [Iter 346] [G_Loss 4.065529] [D_Loss 0.189876]\n",
      "train [Epoch 0/3] [Batch 347/1173] [Iter 347] [G_Loss 4.057123] [D_Loss 0.167670]\n",
      "train [Epoch 0/3] [Batch 348/1173] [Iter 348] [G_Loss 4.958831] [D_Loss 0.145021]\n",
      "train [Epoch 0/3] [Batch 349/1173] [Iter 349] [G_Loss 4.519971] [D_Loss 0.096495]\n",
      "train [Epoch 0/3] [Batch 350/1173] [Iter 350] [G_Loss 4.126654] [D_Loss 0.199513]\n",
      "train [Epoch 0/3] [Batch 351/1173] [Iter 351] [G_Loss 4.040370] [D_Loss 0.092942]\n",
      "train [Epoch 0/3] [Batch 352/1173] [Iter 352] [G_Loss 3.687020] [D_Loss 0.218077]\n",
      "train [Epoch 0/3] [Batch 353/1173] [Iter 353] [G_Loss 3.248655] [D_Loss 0.174424]\n",
      "train [Epoch 0/3] [Batch 354/1173] [Iter 354] [G_Loss 5.072560] [D_Loss 0.151556]\n",
      "train [Epoch 0/3] [Batch 355/1173] [Iter 355] [G_Loss 4.660556] [D_Loss 0.138770]\n",
      "train [Epoch 0/3] [Batch 356/1173] [Iter 356] [G_Loss 3.669607] [D_Loss 0.145496]\n",
      "train [Epoch 0/3] [Batch 357/1173] [Iter 357] [G_Loss 3.658069] [D_Loss 0.099381]\n",
      "train [Epoch 0/3] [Batch 358/1173] [Iter 358] [G_Loss 5.015099] [D_Loss 0.111220]\n",
      "train [Epoch 0/3] [Batch 359/1173] [Iter 359] [G_Loss 5.161286] [D_Loss 0.112295]\n",
      "train [Epoch 0/3] [Batch 360/1173] [Iter 360] [G_Loss 3.924567] [D_Loss 0.132638]\n",
      "train [Epoch 0/3] [Batch 361/1173] [Iter 361] [G_Loss 6.209453] [D_Loss 0.105573]\n",
      "train [Epoch 0/3] [Batch 362/1173] [Iter 362] [G_Loss 5.140626] [D_Loss 0.247466]\n",
      "train [Epoch 0/3] [Batch 363/1173] [Iter 363] [G_Loss 2.018649] [D_Loss 0.608969]\n",
      "train [Epoch 0/3] [Batch 364/1173] [Iter 364] [G_Loss 16.565569] [D_Loss 1.714792]\n",
      "train [Epoch 0/3] [Batch 365/1173] [Iter 365] [G_Loss 8.611643] [D_Loss 0.563617]\n",
      "train [Epoch 0/3] [Batch 366/1173] [Iter 366] [G_Loss 3.972359] [D_Loss 0.097074]\n",
      "train [Epoch 0/3] [Batch 367/1173] [Iter 367] [G_Loss 3.030364] [D_Loss 0.112379]\n",
      "train [Epoch 0/3] [Batch 368/1173] [Iter 368] [G_Loss 1.224533] [D_Loss 0.504711]\n",
      "train [Epoch 0/3] [Batch 369/1173] [Iter 369] [G_Loss 1.877496] [D_Loss 0.315615]\n",
      "train [Epoch 0/3] [Batch 370/1173] [Iter 370] [G_Loss 3.716686] [D_Loss 0.159546]\n",
      "train [Epoch 0/3] [Batch 371/1173] [Iter 371] [G_Loss 4.870363] [D_Loss 0.104383]\n",
      "train [Epoch 0/3] [Batch 372/1173] [Iter 372] [G_Loss 5.195621] [D_Loss 0.155920]\n",
      "train [Epoch 0/3] [Batch 373/1173] [Iter 373] [G_Loss 5.053104] [D_Loss 0.057854]\n",
      "train [Epoch 0/3] [Batch 374/1173] [Iter 374] [G_Loss 4.218746] [D_Loss 0.062615]\n",
      "train [Epoch 0/3] [Batch 375/1173] [Iter 375] [G_Loss 3.543562] [D_Loss 0.126051]\n",
      "train [Epoch 0/3] [Batch 376/1173] [Iter 376] [G_Loss 3.207654] [D_Loss 0.105735]\n",
      "train [Epoch 0/3] [Batch 377/1173] [Iter 377] [G_Loss 3.003698] [D_Loss 0.122692]\n",
      "train [Epoch 0/3] [Batch 378/1173] [Iter 378] [G_Loss 2.896270] [D_Loss 0.127475]\n",
      "train [Epoch 0/3] [Batch 379/1173] [Iter 379] [G_Loss 2.560588] [D_Loss 0.114499]\n",
      "train [Epoch 0/3] [Batch 380/1173] [Iter 380] [G_Loss 2.740406] [D_Loss 0.218899]\n",
      "train [Epoch 0/3] [Batch 381/1173] [Iter 381] [G_Loss 2.725478] [D_Loss 0.097601]\n",
      "train [Epoch 0/3] [Batch 382/1173] [Iter 382] [G_Loss 2.507567] [D_Loss 0.120407]\n",
      "train [Epoch 0/3] [Batch 383/1173] [Iter 383] [G_Loss 2.445525] [D_Loss 0.172200]\n",
      "train [Epoch 0/3] [Batch 384/1173] [Iter 384] [G_Loss 2.456218] [D_Loss 0.181551]\n",
      "train [Epoch 0/3] [Batch 385/1173] [Iter 385] [G_Loss 2.715426] [D_Loss 0.270179]\n",
      "train [Epoch 0/3] [Batch 386/1173] [Iter 386] [G_Loss 2.967482] [D_Loss 0.134229]\n",
      "train [Epoch 0/3] [Batch 387/1173] [Iter 387] [G_Loss 2.855471] [D_Loss 0.279626]\n",
      "train [Epoch 0/3] [Batch 388/1173] [Iter 388] [G_Loss 3.328838] [D_Loss 0.135245]\n",
      "train [Epoch 0/3] [Batch 389/1173] [Iter 389] [G_Loss 3.303600] [D_Loss 0.193783]\n",
      "train [Epoch 0/3] [Batch 390/1173] [Iter 390] [G_Loss 2.678763] [D_Loss 0.224542]\n",
      "train [Epoch 0/3] [Batch 391/1173] [Iter 391] [G_Loss 3.313035] [D_Loss 0.166494]\n",
      "train [Epoch 0/3] [Batch 392/1173] [Iter 392] [G_Loss 3.271181] [D_Loss 0.326930]\n",
      "train [Epoch 0/3] [Batch 393/1173] [Iter 393] [G_Loss 5.239823] [D_Loss 0.154571]\n",
      "train [Epoch 0/3] [Batch 394/1173] [Iter 394] [G_Loss 1.619465] [D_Loss 0.529694]\n",
      "train [Epoch 0/3] [Batch 395/1173] [Iter 395] [G_Loss 7.664756] [D_Loss 0.308724]\n",
      "train [Epoch 0/3] [Batch 396/1173] [Iter 396] [G_Loss 6.344452] [D_Loss 0.257257]\n",
      "train [Epoch 0/3] [Batch 397/1173] [Iter 397] [G_Loss 4.432698] [D_Loss 0.116795]\n",
      "train [Epoch 0/3] [Batch 398/1173] [Iter 398] [G_Loss 1.451878] [D_Loss 0.615174]\n",
      "train [Epoch 0/3] [Batch 399/1173] [Iter 399] [G_Loss 5.078444] [D_Loss 0.583258]\n",
      "train [Epoch 0/3] [Batch 400/1173] [Iter 400] [G_Loss 3.516059] [D_Loss 0.274644]\n",
      "train [Epoch 0/3] [Batch 401/1173] [Iter 401] [G_Loss 1.361031] [D_Loss 0.680355]\n",
      "train [Epoch 0/3] [Batch 402/1173] [Iter 402] [G_Loss 4.059875] [D_Loss 0.480631]\n",
      "train [Epoch 0/3] [Batch 403/1173] [Iter 403] [G_Loss 1.746035] [D_Loss 0.543099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/3] [Batch 404/1173] [Iter 404] [G_Loss 4.960865] [D_Loss 0.773782]\n",
      "train [Epoch 0/3] [Batch 405/1173] [Iter 405] [G_Loss 1.767408] [D_Loss 0.420413]\n",
      "train [Epoch 0/3] [Batch 406/1173] [Iter 406] [G_Loss 2.463580] [D_Loss 0.265113]\n",
      "train [Epoch 0/3] [Batch 407/1173] [Iter 407] [G_Loss 3.882309] [D_Loss 0.422453]\n",
      "train [Epoch 0/3] [Batch 408/1173] [Iter 408] [G_Loss 1.067347] [D_Loss 0.912726]\n",
      "train [Epoch 0/3] [Batch 409/1173] [Iter 409] [G_Loss 8.351288] [D_Loss 4.548718]\n",
      "train [Epoch 0/3] [Batch 410/1173] [Iter 410] [G_Loss 0.496668] [D_Loss 2.639355]\n",
      "train [Epoch 0/3] [Batch 411/1173] [Iter 411] [G_Loss 2.256074] [D_Loss 0.562792]\n",
      "train [Epoch 0/3] [Batch 412/1173] [Iter 412] [G_Loss 2.358259] [D_Loss 0.678496]\n",
      "train [Epoch 0/3] [Batch 413/1173] [Iter 413] [G_Loss 1.862523] [D_Loss 0.663941]\n",
      "train [Epoch 0/3] [Batch 414/1173] [Iter 414] [G_Loss 1.408147] [D_Loss 0.654637]\n",
      "train [Epoch 0/3] [Batch 415/1173] [Iter 415] [G_Loss 1.234680] [D_Loss 0.656248]\n",
      "train [Epoch 0/3] [Batch 416/1173] [Iter 416] [G_Loss 2.016689] [D_Loss 0.706217]\n",
      "train [Epoch 0/3] [Batch 417/1173] [Iter 417] [G_Loss 2.447027] [D_Loss 0.601064]\n",
      "train [Epoch 0/3] [Batch 418/1173] [Iter 418] [G_Loss 1.779632] [D_Loss 0.596093]\n",
      "train [Epoch 0/3] [Batch 419/1173] [Iter 419] [G_Loss 1.976525] [D_Loss 0.455788]\n",
      "train [Epoch 0/3] [Batch 420/1173] [Iter 420] [G_Loss 2.092289] [D_Loss 0.522726]\n",
      "train [Epoch 0/3] [Batch 421/1173] [Iter 421] [G_Loss 2.283035] [D_Loss 0.457874]\n",
      "train [Epoch 0/3] [Batch 422/1173] [Iter 422] [G_Loss 2.097773] [D_Loss 0.496533]\n",
      "train [Epoch 0/3] [Batch 423/1173] [Iter 423] [G_Loss 3.831373] [D_Loss 0.525275]\n",
      "train [Epoch 0/3] [Batch 424/1173] [Iter 424] [G_Loss 1.714453] [D_Loss 0.530103]\n",
      "train [Epoch 0/3] [Batch 425/1173] [Iter 425] [G_Loss 3.378111] [D_Loss 0.406560]\n",
      "train [Epoch 0/3] [Batch 426/1173] [Iter 426] [G_Loss 2.901685] [D_Loss 0.327034]\n",
      "train [Epoch 0/3] [Batch 427/1173] [Iter 427] [G_Loss 1.689806] [D_Loss 0.543213]\n",
      "train [Epoch 0/3] [Batch 428/1173] [Iter 428] [G_Loss 4.605210] [D_Loss 0.602371]\n",
      "train [Epoch 0/3] [Batch 429/1173] [Iter 429] [G_Loss 0.430065] [D_Loss 2.900129]\n",
      "train [Epoch 0/3] [Batch 430/1173] [Iter 430] [G_Loss 12.048596] [D_Loss 3.498776]\n",
      "train [Epoch 0/3] [Batch 431/1173] [Iter 431] [G_Loss 2.392563] [D_Loss 0.337046]\n",
      "train [Epoch 0/3] [Batch 432/1173] [Iter 432] [G_Loss 0.650017] [D_Loss 1.204136]\n",
      "train [Epoch 0/3] [Batch 433/1173] [Iter 433] [G_Loss 2.119686] [D_Loss 0.709627]\n",
      "train [Epoch 0/3] [Batch 434/1173] [Iter 434] [G_Loss 2.545267] [D_Loss 0.766377]\n",
      "train [Epoch 0/3] [Batch 435/1173] [Iter 435] [G_Loss 2.293136] [D_Loss 0.788824]\n",
      "train [Epoch 0/3] [Batch 436/1173] [Iter 436] [G_Loss 1.468085] [D_Loss 0.671150]\n",
      "train [Epoch 0/3] [Batch 437/1173] [Iter 437] [G_Loss 1.353731] [D_Loss 0.718413]\n",
      "train [Epoch 0/3] [Batch 438/1173] [Iter 438] [G_Loss 2.180912] [D_Loss 0.840002]\n",
      "train [Epoch 0/3] [Batch 439/1173] [Iter 439] [G_Loss 1.956590] [D_Loss 0.866279]\n",
      "train [Epoch 0/3] [Batch 440/1173] [Iter 440] [G_Loss 1.841820] [D_Loss 0.841056]\n",
      "train [Epoch 0/3] [Batch 441/1173] [Iter 441] [G_Loss 1.502277] [D_Loss 0.952293]\n",
      "train [Epoch 0/3] [Batch 442/1173] [Iter 442] [G_Loss 2.537253] [D_Loss 0.777391]\n",
      "train [Epoch 0/3] [Batch 443/1173] [Iter 443] [G_Loss 2.607382] [D_Loss 0.736704]\n",
      "train [Epoch 0/3] [Batch 444/1173] [Iter 444] [G_Loss 1.916331] [D_Loss 0.504847]\n",
      "train [Epoch 0/3] [Batch 445/1173] [Iter 445] [G_Loss 2.010553] [D_Loss 0.517208]\n",
      "train [Epoch 0/3] [Batch 446/1173] [Iter 446] [G_Loss 2.780408] [D_Loss 0.377762]\n",
      "train [Epoch 0/3] [Batch 447/1173] [Iter 447] [G_Loss 3.577360] [D_Loss 0.522051]\n",
      "train [Epoch 0/3] [Batch 448/1173] [Iter 448] [G_Loss 1.934461] [D_Loss 0.340637]\n",
      "train [Epoch 0/3] [Batch 449/1173] [Iter 449] [G_Loss 3.148365] [D_Loss 0.418156]\n",
      "train [Epoch 0/3] [Batch 450/1173] [Iter 450] [G_Loss 3.110176] [D_Loss 0.242586]\n",
      "train [Epoch 0/3] [Batch 451/1173] [Iter 451] [G_Loss 2.468304] [D_Loss 0.215850]\n",
      "train [Epoch 0/3] [Batch 452/1173] [Iter 452] [G_Loss 2.963377] [D_Loss 0.223892]\n",
      "train [Epoch 0/3] [Batch 453/1173] [Iter 453] [G_Loss 2.947780] [D_Loss 0.206219]\n",
      "train [Epoch 0/3] [Batch 454/1173] [Iter 454] [G_Loss 3.000238] [D_Loss 0.184211]\n",
      "train [Epoch 0/3] [Batch 455/1173] [Iter 455] [G_Loss 2.870937] [D_Loss 0.172490]\n",
      "train [Epoch 0/3] [Batch 456/1173] [Iter 456] [G_Loss 3.562753] [D_Loss 0.150495]\n",
      "train [Epoch 0/3] [Batch 457/1173] [Iter 457] [G_Loss 2.584182] [D_Loss 0.201508]\n",
      "train [Epoch 0/3] [Batch 458/1173] [Iter 458] [G_Loss 3.007759] [D_Loss 0.205853]\n",
      "train [Epoch 0/3] [Batch 459/1173] [Iter 459] [G_Loss 2.370212] [D_Loss 0.190927]\n",
      "train [Epoch 0/3] [Batch 460/1173] [Iter 460] [G_Loss 4.993317] [D_Loss 0.165395]\n",
      "train [Epoch 0/3] [Batch 461/1173] [Iter 461] [G_Loss 1.848945] [D_Loss 0.318873]\n",
      "train [Epoch 0/3] [Batch 462/1173] [Iter 462] [G_Loss 5.761639] [D_Loss 0.345285]\n",
      "train [Epoch 0/3] [Batch 463/1173] [Iter 463] [G_Loss 1.674427] [D_Loss 0.439057]\n",
      "train [Epoch 0/3] [Batch 464/1173] [Iter 464] [G_Loss 4.068717] [D_Loss 0.146690]\n",
      "train [Epoch 0/3] [Batch 465/1173] [Iter 465] [G_Loss 5.004590] [D_Loss 0.158798]\n",
      "train [Epoch 0/3] [Batch 466/1173] [Iter 466] [G_Loss 11.413648] [D_Loss 0.079723]\n",
      "train [Epoch 0/3] [Batch 467/1173] [Iter 467] [G_Loss 5.486611] [D_Loss 0.034269]\n",
      "train [Epoch 0/3] [Batch 468/1173] [Iter 468] [G_Loss 5.633678] [D_Loss 0.031629]\n",
      "train [Epoch 0/3] [Batch 469/1173] [Iter 469] [G_Loss 5.333107] [D_Loss 0.012041]\n",
      "train [Epoch 0/3] [Batch 470/1173] [Iter 470] [G_Loss 4.294497] [D_Loss 0.115029]\n",
      "train [Epoch 0/3] [Batch 471/1173] [Iter 471] [G_Loss 3.968770] [D_Loss 0.034242]\n",
      "train [Epoch 0/3] [Batch 472/1173] [Iter 472] [G_Loss 4.758808] [D_Loss 0.015985]\n",
      "train [Epoch 0/3] [Batch 473/1173] [Iter 473] [G_Loss 4.724368] [D_Loss 0.019786]\n",
      "train [Epoch 0/3] [Batch 474/1173] [Iter 474] [G_Loss 0.173924] [D_Loss 4.297117]\n",
      "train [Epoch 0/3] [Batch 475/1173] [Iter 475] [G_Loss 25.820351] [D_Loss 25.161684]\n",
      "train [Epoch 0/3] [Batch 476/1173] [Iter 476] [G_Loss 0.202019] [D_Loss 2.131341]\n",
      "train [Epoch 0/3] [Batch 477/1173] [Iter 477] [G_Loss 0.610239] [D_Loss 1.403053]\n",
      "train [Epoch 0/3] [Batch 478/1173] [Iter 478] [G_Loss 1.275868] [D_Loss 1.148234]\n",
      "train [Epoch 0/3] [Batch 479/1173] [Iter 479] [G_Loss 1.419019] [D_Loss 1.186994]\n",
      "train [Epoch 0/3] [Batch 480/1173] [Iter 480] [G_Loss 1.202637] [D_Loss 0.960841]\n",
      "train [Epoch 0/3] [Batch 481/1173] [Iter 481] [G_Loss 0.747392] [D_Loss 1.157857]\n",
      "train [Epoch 0/3] [Batch 482/1173] [Iter 482] [G_Loss 0.843308] [D_Loss 1.164797]\n",
      "train [Epoch 0/3] [Batch 483/1173] [Iter 483] [G_Loss 0.998585] [D_Loss 1.248948]\n",
      "train [Epoch 0/3] [Batch 484/1173] [Iter 484] [G_Loss 0.890419] [D_Loss 1.257229]\n",
      "train [Epoch 0/3] [Batch 485/1173] [Iter 485] [G_Loss 0.832404] [D_Loss 1.389049]\n",
      "train [Epoch 0/3] [Batch 486/1173] [Iter 486] [G_Loss 0.949816] [D_Loss 1.242582]\n",
      "train [Epoch 0/3] [Batch 487/1173] [Iter 487] [G_Loss 1.025298] [D_Loss 1.069373]\n",
      "train [Epoch 0/3] [Batch 488/1173] [Iter 488] [G_Loss 1.002823] [D_Loss 1.035718]\n",
      "train [Epoch 0/3] [Batch 489/1173] [Iter 489] [G_Loss 1.047844] [D_Loss 1.002643]\n",
      "train [Epoch 0/3] [Batch 490/1173] [Iter 490] [G_Loss 1.047435] [D_Loss 0.982969]\n",
      "train [Epoch 0/3] [Batch 491/1173] [Iter 491] [G_Loss 1.052556] [D_Loss 1.002304]\n",
      "train [Epoch 0/3] [Batch 492/1173] [Iter 492] [G_Loss 1.059100] [D_Loss 0.916031]\n",
      "train [Epoch 0/3] [Batch 493/1173] [Iter 493] [G_Loss 1.170875] [D_Loss 0.927421]\n",
      "train [Epoch 0/3] [Batch 494/1173] [Iter 494] [G_Loss 1.016630] [D_Loss 0.912962]\n",
      "train [Epoch 0/3] [Batch 495/1173] [Iter 495] [G_Loss 0.912360] [D_Loss 0.900662]\n",
      "train [Epoch 0/3] [Batch 496/1173] [Iter 496] [G_Loss 0.984056] [D_Loss 0.855081]\n",
      "train [Epoch 0/3] [Batch 497/1173] [Iter 497] [G_Loss 1.041738] [D_Loss 0.768941]\n",
      "train [Epoch 0/3] [Batch 498/1173] [Iter 498] [G_Loss 1.069361] [D_Loss 0.692195]\n",
      "train [Epoch 0/3] [Batch 499/1173] [Iter 499] [G_Loss 1.262519] [D_Loss 0.568448]\n",
      "train [Epoch 0/3] [Batch 500/1173] [Iter 500] [G_Loss 1.523822] [D_Loss 0.642245]\n",
      "train [Epoch 0/3] [Batch 501/1173] [Iter 501] [G_Loss 0.708895] [D_Loss 0.826661]\n",
      "train [Epoch 0/3] [Batch 502/1173] [Iter 502] [G_Loss 1.195629] [D_Loss 0.547019]\n",
      "train [Epoch 0/3] [Batch 503/1173] [Iter 503] [G_Loss 1.392326] [D_Loss 0.633604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/3] [Batch 504/1173] [Iter 504] [G_Loss 1.043586] [D_Loss 0.573842]\n",
      "train [Epoch 0/3] [Batch 505/1173] [Iter 505] [G_Loss 1.005487] [D_Loss 0.586321]\n",
      "train [Epoch 0/3] [Batch 506/1173] [Iter 506] [G_Loss 1.123438] [D_Loss 0.535474]\n",
      "train [Epoch 0/3] [Batch 507/1173] [Iter 507] [G_Loss 1.632195] [D_Loss 0.521496]\n",
      "train [Epoch 0/3] [Batch 508/1173] [Iter 508] [G_Loss 2.074554] [D_Loss 0.571493]\n",
      "train [Epoch 0/3] [Batch 509/1173] [Iter 509] [G_Loss 1.240633] [D_Loss 0.552148]\n",
      "train [Epoch 0/3] [Batch 510/1173] [Iter 510] [G_Loss 5.217177] [D_Loss 1.109472]\n",
      "train [Epoch 0/3] [Batch 511/1173] [Iter 511] [G_Loss 2.080557] [D_Loss 0.290468]\n",
      "train [Epoch 0/3] [Batch 512/1173] [Iter 512] [G_Loss 1.352131] [D_Loss 0.365819]\n",
      "train [Epoch 0/3] [Batch 513/1173] [Iter 513] [G_Loss 1.571382] [D_Loss 0.290296]\n",
      "train [Epoch 0/3] [Batch 514/1173] [Iter 514] [G_Loss 1.642090] [D_Loss 0.290823]\n",
      "train [Epoch 0/3] [Batch 515/1173] [Iter 515] [G_Loss 1.522707] [D_Loss 0.356342]\n",
      "train [Epoch 0/3] [Batch 516/1173] [Iter 516] [G_Loss 2.481154] [D_Loss 0.278686]\n",
      "train [Epoch 0/3] [Batch 517/1173] [Iter 517] [G_Loss 2.230282] [D_Loss 0.348851]\n",
      "train [Epoch 0/3] [Batch 518/1173] [Iter 518] [G_Loss 2.108185] [D_Loss 0.348776]\n",
      "train [Epoch 0/3] [Batch 519/1173] [Iter 519] [G_Loss 2.177963] [D_Loss 0.321506]\n",
      "train [Epoch 0/3] [Batch 520/1173] [Iter 520] [G_Loss 2.214009] [D_Loss 0.452876]\n",
      "train [Epoch 0/3] [Batch 521/1173] [Iter 521] [G_Loss 1.775631] [D_Loss 0.441447]\n",
      "train [Epoch 0/3] [Batch 522/1173] [Iter 522] [G_Loss 2.471813] [D_Loss 0.463430]\n",
      "train [Epoch 0/3] [Batch 523/1173] [Iter 523] [G_Loss 1.502644] [D_Loss 0.559642]\n",
      "train [Epoch 0/3] [Batch 524/1173] [Iter 524] [G_Loss 2.573876] [D_Loss 0.676904]\n",
      "train [Epoch 0/3] [Batch 525/1173] [Iter 525] [G_Loss 0.584439] [D_Loss 1.025568]\n",
      "train [Epoch 0/3] [Batch 526/1173] [Iter 526] [G_Loss 3.201163] [D_Loss 1.692101]\n",
      "train [Epoch 0/3] [Batch 527/1173] [Iter 527] [G_Loss 1.477093] [D_Loss 0.617145]\n",
      "train [Epoch 0/3] [Batch 528/1173] [Iter 528] [G_Loss 0.957689] [D_Loss 0.694910]\n",
      "train [Epoch 0/3] [Batch 529/1173] [Iter 529] [G_Loss 1.119027] [D_Loss 0.701014]\n",
      "train [Epoch 0/3] [Batch 530/1173] [Iter 530] [G_Loss 1.231092] [D_Loss 0.698244]\n",
      "train [Epoch 0/3] [Batch 531/1173] [Iter 531] [G_Loss 1.325686] [D_Loss 0.624459]\n",
      "train [Epoch 0/3] [Batch 532/1173] [Iter 532] [G_Loss 1.503390] [D_Loss 0.657498]\n",
      "train [Epoch 0/3] [Batch 533/1173] [Iter 533] [G_Loss 1.551674] [D_Loss 0.542722]\n",
      "train [Epoch 0/3] [Batch 534/1173] [Iter 534] [G_Loss 1.628512] [D_Loss 0.623370]\n",
      "train [Epoch 0/3] [Batch 535/1173] [Iter 535] [G_Loss 1.516129] [D_Loss 0.657480]\n",
      "train [Epoch 0/3] [Batch 536/1173] [Iter 536] [G_Loss 1.450529] [D_Loss 0.585445]\n",
      "train [Epoch 0/3] [Batch 537/1173] [Iter 537] [G_Loss 1.609407] [D_Loss 0.624331]\n",
      "train [Epoch 0/3] [Batch 538/1173] [Iter 538] [G_Loss 1.537732] [D_Loss 0.581335]\n",
      "train [Epoch 0/3] [Batch 539/1173] [Iter 539] [G_Loss 1.793593] [D_Loss 0.520894]\n",
      "train [Epoch 0/3] [Batch 540/1173] [Iter 540] [G_Loss 1.933434] [D_Loss 0.519053]\n",
      "train [Epoch 0/3] [Batch 541/1173] [Iter 541] [G_Loss 1.892291] [D_Loss 0.454316]\n",
      "train [Epoch 0/3] [Batch 542/1173] [Iter 542] [G_Loss 2.228100] [D_Loss 0.465081]\n",
      "train [Epoch 0/3] [Batch 543/1173] [Iter 543] [G_Loss 2.331545] [D_Loss 0.357789]\n",
      "train [Epoch 0/3] [Batch 544/1173] [Iter 544] [G_Loss 1.471301] [D_Loss 0.500449]\n",
      "train [Epoch 0/3] [Batch 545/1173] [Iter 545] [G_Loss 1.618413] [D_Loss 0.633848]\n",
      "train [Epoch 0/3] [Batch 546/1173] [Iter 546] [G_Loss 2.197432] [D_Loss 0.769106]\n",
      "train [Epoch 0/3] [Batch 547/1173] [Iter 547] [G_Loss 2.197260] [D_Loss 0.640347]\n",
      "train [Epoch 0/3] [Batch 548/1173] [Iter 548] [G_Loss 1.610907] [D_Loss 0.420214]\n",
      "train [Epoch 0/3] [Batch 549/1173] [Iter 549] [G_Loss 1.491223] [D_Loss 0.550797]\n",
      "train [Epoch 0/3] [Batch 550/1173] [Iter 550] [G_Loss 1.448531] [D_Loss 0.698267]\n",
      "train [Epoch 0/3] [Batch 551/1173] [Iter 551] [G_Loss 4.095088] [D_Loss 1.075328]\n",
      "train [Epoch 0/3] [Batch 552/1173] [Iter 552] [G_Loss 2.836481] [D_Loss 0.654611]\n",
      "train [Epoch 0/3] [Batch 553/1173] [Iter 553] [G_Loss 1.354204] [D_Loss 0.566064]\n",
      "train [Epoch 0/3] [Batch 554/1173] [Iter 554] [G_Loss 1.779904] [D_Loss 0.565713]\n",
      "train [Epoch 0/3] [Batch 555/1173] [Iter 555] [G_Loss 1.856444] [D_Loss 0.588953]\n",
      "train [Epoch 0/3] [Batch 556/1173] [Iter 556] [G_Loss 1.729719] [D_Loss 0.746202]\n",
      "train [Epoch 0/3] [Batch 557/1173] [Iter 557] [G_Loss 1.477324] [D_Loss 0.761092]\n",
      "train [Epoch 0/3] [Batch 558/1173] [Iter 558] [G_Loss 0.817731] [D_Loss 1.319291]\n",
      "train [Epoch 0/3] [Batch 559/1173] [Iter 559] [G_Loss 2.352755] [D_Loss 0.827502]\n",
      "train [Epoch 0/3] [Batch 560/1173] [Iter 560] [G_Loss 2.019787] [D_Loss 0.491509]\n",
      "train [Epoch 0/3] [Batch 561/1173] [Iter 561] [G_Loss 1.869835] [D_Loss 0.270595]\n",
      "train [Epoch 0/3] [Batch 562/1173] [Iter 562] [G_Loss 1.698932] [D_Loss 0.313933]\n",
      "train [Epoch 0/3] [Batch 563/1173] [Iter 563] [G_Loss 1.809320] [D_Loss 0.364983]\n",
      "train [Epoch 0/3] [Batch 564/1173] [Iter 564] [G_Loss 3.539616] [D_Loss 0.875273]\n",
      "train [Epoch 0/3] [Batch 565/1173] [Iter 565] [G_Loss 0.074463] [D_Loss 3.454236]\n",
      "train [Epoch 0/3] [Batch 566/1173] [Iter 566] [G_Loss 2.689090] [D_Loss 0.734214]\n",
      "train [Epoch 0/3] [Batch 567/1173] [Iter 567] [G_Loss 1.117164] [D_Loss 1.164768]\n",
      "train [Epoch 0/3] [Batch 568/1173] [Iter 568] [G_Loss 2.024081] [D_Loss 1.158642]\n",
      "train [Epoch 0/3] [Batch 569/1173] [Iter 569] [G_Loss 1.362272] [D_Loss 1.131176]\n",
      "train [Epoch 0/3] [Batch 570/1173] [Iter 570] [G_Loss 0.876405] [D_Loss 0.904955]\n",
      "train [Epoch 0/3] [Batch 571/1173] [Iter 571] [G_Loss 0.820457] [D_Loss 0.946862]\n",
      "train [Epoch 0/3] [Batch 572/1173] [Iter 572] [G_Loss 0.923927] [D_Loss 0.931978]\n",
      "train [Epoch 0/3] [Batch 573/1173] [Iter 573] [G_Loss 1.212462] [D_Loss 0.931703]\n",
      "train [Epoch 0/3] [Batch 574/1173] [Iter 574] [G_Loss 1.533016] [D_Loss 0.840521]\n",
      "train [Epoch 0/3] [Batch 575/1173] [Iter 575] [G_Loss 1.411228] [D_Loss 0.866713]\n",
      "train [Epoch 0/3] [Batch 576/1173] [Iter 576] [G_Loss 1.342421] [D_Loss 0.804196]\n",
      "train [Epoch 0/3] [Batch 577/1173] [Iter 577] [G_Loss 1.356141] [D_Loss 0.821382]\n",
      "train [Epoch 0/3] [Batch 578/1173] [Iter 578] [G_Loss 1.342224] [D_Loss 0.756725]\n",
      "train [Epoch 0/3] [Batch 579/1173] [Iter 579] [G_Loss 1.381161] [D_Loss 0.736511]\n",
      "train [Epoch 0/3] [Batch 580/1173] [Iter 580] [G_Loss 1.358430] [D_Loss 0.614222]\n",
      "train [Epoch 0/3] [Batch 581/1173] [Iter 581] [G_Loss 1.655984] [D_Loss 0.589359]\n",
      "train [Epoch 0/3] [Batch 582/1173] [Iter 582] [G_Loss 1.556804] [D_Loss 0.607200]\n",
      "train [Epoch 0/3] [Batch 583/1173] [Iter 583] [G_Loss 1.232238] [D_Loss 0.807111]\n",
      "train [Epoch 0/3] [Batch 584/1173] [Iter 584] [G_Loss 1.846739] [D_Loss 0.842194]\n",
      "train [Epoch 0/3] [Batch 585/1173] [Iter 585] [G_Loss 1.595781] [D_Loss 0.831311]\n",
      "train [Epoch 0/3] [Batch 586/1173] [Iter 586] [G_Loss 1.211016] [D_Loss 1.124820]\n",
      "train [Epoch 0/3] [Batch 587/1173] [Iter 587] [G_Loss 1.506332] [D_Loss 1.139768]\n",
      "train [Epoch 0/3] [Batch 588/1173] [Iter 588] [G_Loss 2.440059] [D_Loss 0.691263]\n",
      "train [Epoch 0/3] [Batch 589/1173] [Iter 589] [G_Loss 2.428493] [D_Loss 0.718273]\n",
      "train [Epoch 0/3] [Batch 590/1173] [Iter 590] [G_Loss 2.405708] [D_Loss 0.686750]\n",
      "train [Epoch 0/3] [Batch 591/1173] [Iter 591] [G_Loss 2.193001] [D_Loss 0.886062]\n",
      "train [Epoch 0/3] [Batch 592/1173] [Iter 592] [G_Loss 1.715165] [D_Loss 1.292907]\n",
      "train [Epoch 0/3] [Batch 593/1173] [Iter 593] [G_Loss 1.904814] [D_Loss 1.203021]\n",
      "train [Epoch 0/3] [Batch 594/1173] [Iter 594] [G_Loss 1.524516] [D_Loss 1.202495]\n",
      "train [Epoch 0/3] [Batch 595/1173] [Iter 595] [G_Loss 1.457075] [D_Loss 1.233099]\n",
      "train [Epoch 0/3] [Batch 596/1173] [Iter 596] [G_Loss 1.810419] [D_Loss 0.927366]\n",
      "train [Epoch 0/3] [Batch 597/1173] [Iter 597] [G_Loss 2.124816] [D_Loss 0.453146]\n",
      "train [Epoch 0/3] [Batch 598/1173] [Iter 598] [G_Loss 2.300903] [D_Loss 0.472448]\n",
      "train [Epoch 0/3] [Batch 599/1173] [Iter 599] [G_Loss 1.574633] [D_Loss 0.636929]\n",
      "train [Epoch 0/3] [Batch 600/1173] [Iter 600] [G_Loss 3.097293] [D_Loss 0.742299]\n",
      "train [Epoch 0/3] [Batch 601/1173] [Iter 601] [G_Loss 1.920851] [D_Loss 0.601009]\n",
      "train [Epoch 0/3] [Batch 602/1173] [Iter 602] [G_Loss 1.113879] [D_Loss 0.769449]\n",
      "train [Epoch 0/3] [Batch 603/1173] [Iter 603] [G_Loss 2.309304] [D_Loss 0.750509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/3] [Batch 604/1173] [Iter 604] [G_Loss 2.125540] [D_Loss 0.513169]\n",
      "train [Epoch 0/3] [Batch 605/1173] [Iter 605] [G_Loss 1.848718] [D_Loss 0.559540]\n",
      "train [Epoch 0/3] [Batch 606/1173] [Iter 606] [G_Loss 1.709815] [D_Loss 0.474170]\n",
      "train [Epoch 0/3] [Batch 607/1173] [Iter 607] [G_Loss 1.577263] [D_Loss 0.401283]\n",
      "train [Epoch 0/3] [Batch 608/1173] [Iter 608] [G_Loss 1.803940] [D_Loss 0.397365]\n",
      "train [Epoch 0/3] [Batch 609/1173] [Iter 609] [G_Loss 1.623750] [D_Loss 0.544723]\n",
      "train [Epoch 0/3] [Batch 610/1173] [Iter 610] [G_Loss 1.663037] [D_Loss 0.612418]\n",
      "train [Epoch 0/3] [Batch 611/1173] [Iter 611] [G_Loss 2.329571] [D_Loss 0.700997]\n",
      "train [Epoch 0/3] [Batch 612/1173] [Iter 612] [G_Loss 1.941888] [D_Loss 0.924624]\n",
      "train [Epoch 0/3] [Batch 613/1173] [Iter 613] [G_Loss 1.430605] [D_Loss 1.166759]\n",
      "train [Epoch 0/3] [Batch 614/1173] [Iter 614] [G_Loss 1.996296] [D_Loss 0.828974]\n",
      "train [Epoch 0/3] [Batch 615/1173] [Iter 615] [G_Loss 1.736438] [D_Loss 0.928531]\n",
      "train [Epoch 0/3] [Batch 616/1173] [Iter 616] [G_Loss 1.298661] [D_Loss 0.986333]\n",
      "train [Epoch 0/3] [Batch 617/1173] [Iter 617] [G_Loss 1.438236] [D_Loss 0.721149]\n",
      "train [Epoch 0/3] [Batch 618/1173] [Iter 618] [G_Loss 1.451444] [D_Loss 0.710531]\n",
      "train [Epoch 0/3] [Batch 619/1173] [Iter 619] [G_Loss 1.324060] [D_Loss 0.717627]\n",
      "train [Epoch 0/3] [Batch 620/1173] [Iter 620] [G_Loss 1.525909] [D_Loss 0.656807]\n",
      "train [Epoch 0/3] [Batch 621/1173] [Iter 621] [G_Loss 1.531234] [D_Loss 0.549992]\n",
      "train [Epoch 0/3] [Batch 622/1173] [Iter 622] [G_Loss 1.498981] [D_Loss 0.472694]\n",
      "train [Epoch 0/3] [Batch 623/1173] [Iter 623] [G_Loss 1.687974] [D_Loss 0.488510]\n",
      "train [Epoch 0/3] [Batch 624/1173] [Iter 624] [G_Loss 1.898197] [D_Loss 0.428305]\n",
      "train [Epoch 0/3] [Batch 625/1173] [Iter 625] [G_Loss 1.716227] [D_Loss 0.482141]\n",
      "train [Epoch 0/3] [Batch 626/1173] [Iter 626] [G_Loss 1.538735] [D_Loss 0.477228]\n",
      "train [Epoch 0/3] [Batch 627/1173] [Iter 627] [G_Loss 1.904114] [D_Loss 0.381904]\n",
      "train [Epoch 0/3] [Batch 628/1173] [Iter 628] [G_Loss 1.969682] [D_Loss 0.433904]\n",
      "train [Epoch 0/3] [Batch 629/1173] [Iter 629] [G_Loss 1.726041] [D_Loss 0.495028]\n",
      "train [Epoch 0/3] [Batch 630/1173] [Iter 630] [G_Loss 1.858743] [D_Loss 0.443502]\n",
      "train [Epoch 0/3] [Batch 631/1173] [Iter 631] [G_Loss 1.940304] [D_Loss 0.473753]\n",
      "train [Epoch 0/3] [Batch 632/1173] [Iter 632] [G_Loss 1.625965] [D_Loss 0.392050]\n",
      "train [Epoch 0/3] [Batch 633/1173] [Iter 633] [G_Loss 2.034742] [D_Loss 0.425282]\n",
      "train [Epoch 0/3] [Batch 634/1173] [Iter 634] [G_Loss 1.869309] [D_Loss 0.434909]\n",
      "train [Epoch 0/3] [Batch 635/1173] [Iter 635] [G_Loss 2.098196] [D_Loss 0.396318]\n",
      "train [Epoch 0/3] [Batch 636/1173] [Iter 636] [G_Loss 1.714918] [D_Loss 0.412754]\n",
      "train [Epoch 0/3] [Batch 637/1173] [Iter 637] [G_Loss 1.994805] [D_Loss 0.339266]\n",
      "train [Epoch 0/3] [Batch 638/1173] [Iter 638] [G_Loss 2.097897] [D_Loss 0.417612]\n",
      "train [Epoch 0/3] [Batch 639/1173] [Iter 639] [G_Loss 1.553675] [D_Loss 0.411818]\n",
      "train [Epoch 0/3] [Batch 640/1173] [Iter 640] [G_Loss 2.648343] [D_Loss 0.593839]\n",
      "train [Epoch 0/3] [Batch 641/1173] [Iter 641] [G_Loss 1.445423] [D_Loss 0.478004]\n",
      "train [Epoch 0/3] [Batch 642/1173] [Iter 642] [G_Loss 2.319624] [D_Loss 0.583596]\n",
      "train [Epoch 0/3] [Batch 643/1173] [Iter 643] [G_Loss 1.322833] [D_Loss 0.500633]\n",
      "train [Epoch 0/3] [Batch 644/1173] [Iter 644] [G_Loss 2.567463] [D_Loss 0.571007]\n",
      "train [Epoch 0/3] [Batch 645/1173] [Iter 645] [G_Loss 2.157468] [D_Loss 0.436440]\n",
      "train [Epoch 0/3] [Batch 646/1173] [Iter 646] [G_Loss 1.093016] [D_Loss 0.667255]\n",
      "train [Epoch 0/3] [Batch 647/1173] [Iter 647] [G_Loss 2.847723] [D_Loss 0.570608]\n",
      "train [Epoch 0/3] [Batch 648/1173] [Iter 648] [G_Loss 1.740221] [D_Loss 0.625872]\n",
      "train [Epoch 0/3] [Batch 649/1173] [Iter 649] [G_Loss 1.702492] [D_Loss 0.553845]\n",
      "train [Epoch 0/3] [Batch 650/1173] [Iter 650] [G_Loss 2.161384] [D_Loss 0.341422]\n",
      "train [Epoch 0/3] [Batch 651/1173] [Iter 651] [G_Loss 2.046493] [D_Loss 0.337618]\n",
      "train [Epoch 0/3] [Batch 652/1173] [Iter 652] [G_Loss 2.163830] [D_Loss 0.386958]\n",
      "train [Epoch 0/3] [Batch 653/1173] [Iter 653] [G_Loss 2.188786] [D_Loss 0.371766]\n",
      "train [Epoch 0/3] [Batch 654/1173] [Iter 654] [G_Loss 2.123621] [D_Loss 0.306874]\n",
      "train [Epoch 0/3] [Batch 655/1173] [Iter 655] [G_Loss 2.841124] [D_Loss 0.361264]\n",
      "train [Epoch 0/3] [Batch 656/1173] [Iter 656] [G_Loss 1.657921] [D_Loss 0.524569]\n",
      "train [Epoch 0/3] [Batch 657/1173] [Iter 657] [G_Loss 3.494734] [D_Loss 1.122743]\n",
      "train [Epoch 0/3] [Batch 658/1173] [Iter 658] [G_Loss 1.503983] [D_Loss 0.559748]\n",
      "train [Epoch 0/3] [Batch 659/1173] [Iter 659] [G_Loss 2.464623] [D_Loss 0.441534]\n",
      "train [Epoch 0/3] [Batch 660/1173] [Iter 660] [G_Loss 1.851317] [D_Loss 0.509673]\n",
      "train [Epoch 0/3] [Batch 661/1173] [Iter 661] [G_Loss 2.386814] [D_Loss 0.483575]\n",
      "train [Epoch 0/3] [Batch 662/1173] [Iter 662] [G_Loss 2.037666] [D_Loss 0.453580]\n",
      "train [Epoch 0/3] [Batch 663/1173] [Iter 663] [G_Loss 2.417203] [D_Loss 0.404172]\n",
      "train [Epoch 0/3] [Batch 664/1173] [Iter 664] [G_Loss 2.393462] [D_Loss 0.372342]\n",
      "train [Epoch 0/3] [Batch 665/1173] [Iter 665] [G_Loss 1.527045] [D_Loss 0.507747]\n",
      "train [Epoch 0/3] [Batch 666/1173] [Iter 666] [G_Loss 3.466321] [D_Loss 0.451602]\n",
      "train [Epoch 0/3] [Batch 667/1173] [Iter 667] [G_Loss 1.817854] [D_Loss 0.332621]\n",
      "train [Epoch 0/3] [Batch 668/1173] [Iter 668] [G_Loss 2.684175] [D_Loss 0.413149]\n",
      "train [Epoch 0/3] [Batch 669/1173] [Iter 669] [G_Loss 2.612649] [D_Loss 0.295244]\n",
      "train [Epoch 0/3] [Batch 670/1173] [Iter 670] [G_Loss 2.601876] [D_Loss 0.299653]\n",
      "train [Epoch 0/3] [Batch 671/1173] [Iter 671] [G_Loss 2.065234] [D_Loss 0.339380]\n",
      "train [Epoch 0/3] [Batch 672/1173] [Iter 672] [G_Loss 4.976788] [D_Loss 1.371190]\n",
      "train [Epoch 0/3] [Batch 673/1173] [Iter 673] [G_Loss 1.501785] [D_Loss 0.453091]\n",
      "train [Epoch 0/3] [Batch 674/1173] [Iter 674] [G_Loss 1.866185] [D_Loss 0.350515]\n",
      "train [Epoch 0/3] [Batch 675/1173] [Iter 675] [G_Loss 2.053727] [D_Loss 0.284024]\n",
      "train [Epoch 0/3] [Batch 676/1173] [Iter 676] [G_Loss 2.113069] [D_Loss 0.366454]\n",
      "train [Epoch 0/3] [Batch 677/1173] [Iter 677] [G_Loss 2.458862] [D_Loss 0.396990]\n",
      "train [Epoch 0/3] [Batch 678/1173] [Iter 678] [G_Loss 1.591251] [D_Loss 0.506613]\n",
      "train [Epoch 0/3] [Batch 679/1173] [Iter 679] [G_Loss 3.320057] [D_Loss 0.608625]\n",
      "train [Epoch 0/3] [Batch 680/1173] [Iter 680] [G_Loss 0.988253] [D_Loss 0.740278]\n",
      "train [Epoch 0/3] [Batch 681/1173] [Iter 681] [G_Loss 3.310830] [D_Loss 0.948275]\n",
      "train [Epoch 0/3] [Batch 682/1173] [Iter 682] [G_Loss 1.153759] [D_Loss 0.657054]\n",
      "train [Epoch 0/3] [Batch 683/1173] [Iter 683] [G_Loss 2.909958] [D_Loss 0.600306]\n",
      "train [Epoch 0/3] [Batch 684/1173] [Iter 684] [G_Loss 2.285092] [D_Loss 0.751069]\n",
      "train [Epoch 0/3] [Batch 685/1173] [Iter 685] [G_Loss 0.332105] [D_Loss 2.395545]\n",
      "train [Epoch 0/3] [Batch 686/1173] [Iter 686] [G_Loss 4.646049] [D_Loss 1.472341]\n",
      "train [Epoch 0/3] [Batch 687/1173] [Iter 687] [G_Loss 1.486552] [D_Loss 0.732487]\n",
      "train [Epoch 0/3] [Batch 688/1173] [Iter 688] [G_Loss 1.241214] [D_Loss 0.817192]\n",
      "train [Epoch 0/3] [Batch 689/1173] [Iter 689] [G_Loss 2.093192] [D_Loss 0.780921]\n",
      "train [Epoch 0/3] [Batch 690/1173] [Iter 690] [G_Loss 1.598780] [D_Loss 0.636031]\n",
      "train [Epoch 0/3] [Batch 691/1173] [Iter 691] [G_Loss 1.419023] [D_Loss 0.777197]\n",
      "train [Epoch 0/3] [Batch 692/1173] [Iter 692] [G_Loss 2.390647] [D_Loss 0.546257]\n",
      "train [Epoch 0/3] [Batch 693/1173] [Iter 693] [G_Loss 1.378991] [D_Loss 0.486858]\n",
      "train [Epoch 0/3] [Batch 694/1173] [Iter 694] [G_Loss 2.949347] [D_Loss 0.747129]\n",
      "train [Epoch 0/3] [Batch 695/1173] [Iter 695] [G_Loss 0.813828] [D_Loss 1.117518]\n",
      "train [Epoch 0/3] [Batch 696/1173] [Iter 696] [G_Loss 2.610067] [D_Loss 1.168365]\n",
      "train [Epoch 0/3] [Batch 697/1173] [Iter 697] [G_Loss 1.912604] [D_Loss 1.207228]\n",
      "train [Epoch 0/3] [Batch 698/1173] [Iter 698] [G_Loss 1.040866] [D_Loss 0.969706]\n",
      "train [Epoch 0/3] [Batch 699/1173] [Iter 699] [G_Loss 1.095325] [D_Loss 0.803249]\n",
      "train [Epoch 0/3] [Batch 700/1173] [Iter 700] [G_Loss 1.117840] [D_Loss 0.761342]\n",
      "train [Epoch 0/3] [Batch 701/1173] [Iter 701] [G_Loss 1.092651] [D_Loss 0.753160]\n",
      "train [Epoch 0/3] [Batch 702/1173] [Iter 702] [G_Loss 1.459312] [D_Loss 0.724483]\n",
      "train [Epoch 0/3] [Batch 703/1173] [Iter 703] [G_Loss 1.443370] [D_Loss 0.759714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/3] [Batch 704/1173] [Iter 704] [G_Loss 1.281381] [D_Loss 0.773877]\n",
      "train [Epoch 0/3] [Batch 705/1173] [Iter 705] [G_Loss 1.416201] [D_Loss 0.797453]\n",
      "train [Epoch 0/3] [Batch 706/1173] [Iter 706] [G_Loss 1.420591] [D_Loss 0.713587]\n",
      "train [Epoch 0/3] [Batch 707/1173] [Iter 707] [G_Loss 1.413118] [D_Loss 0.640476]\n",
      "train [Epoch 0/3] [Batch 708/1173] [Iter 708] [G_Loss 1.874700] [D_Loss 0.602410]\n",
      "train [Epoch 0/3] [Batch 709/1173] [Iter 709] [G_Loss 1.595543] [D_Loss 0.522335]\n",
      "train [Epoch 0/3] [Batch 710/1173] [Iter 710] [G_Loss 2.120107] [D_Loss 0.524239]\n",
      "train [Epoch 0/3] [Batch 711/1173] [Iter 711] [G_Loss 1.846036] [D_Loss 0.474008]\n",
      "train [Epoch 0/3] [Batch 712/1173] [Iter 712] [G_Loss 1.604513] [D_Loss 0.500646]\n",
      "train [Epoch 0/3] [Batch 713/1173] [Iter 713] [G_Loss 1.592211] [D_Loss 0.408909]\n",
      "train [Epoch 0/3] [Batch 714/1173] [Iter 714] [G_Loss 1.898452] [D_Loss 0.518346]\n",
      "train [Epoch 0/3] [Batch 715/1173] [Iter 715] [G_Loss 1.543923] [D_Loss 0.575338]\n",
      "train [Epoch 0/3] [Batch 716/1173] [Iter 716] [G_Loss 1.895971] [D_Loss 0.641265]\n",
      "train [Epoch 0/3] [Batch 717/1173] [Iter 717] [G_Loss 1.506956] [D_Loss 0.686271]\n",
      "train [Epoch 0/3] [Batch 718/1173] [Iter 718] [G_Loss 1.788457] [D_Loss 0.478179]\n",
      "train [Epoch 0/3] [Batch 719/1173] [Iter 719] [G_Loss 2.095271] [D_Loss 0.333423]\n",
      "train [Epoch 0/3] [Batch 720/1173] [Iter 720] [G_Loss 2.142494] [D_Loss 0.210888]\n",
      "train [Epoch 0/3] [Batch 721/1173] [Iter 721] [G_Loss 2.158632] [D_Loss 0.175728]\n",
      "train [Epoch 0/3] [Batch 722/1173] [Iter 722] [G_Loss 2.167286] [D_Loss 0.199352]\n",
      "train [Epoch 0/3] [Batch 723/1173] [Iter 723] [G_Loss 2.381331] [D_Loss 0.252621]\n",
      "train [Epoch 0/3] [Batch 724/1173] [Iter 724] [G_Loss 0.031091] [D_Loss 6.488274]\n",
      "train [Epoch 0/3] [Batch 725/1173] [Iter 725] [G_Loss 1.534747] [D_Loss 1.593058]\n",
      "train [Epoch 0/3] [Batch 726/1173] [Iter 726] [G_Loss 3.524127] [D_Loss 0.729378]\n",
      "train [Epoch 0/3] [Batch 727/1173] [Iter 727] [G_Loss 8.601036] [D_Loss 0.114171]\n",
      "train [Epoch 0/3] [Batch 728/1173] [Iter 728] [G_Loss 11.241015] [D_Loss 0.328302]\n",
      "train [Epoch 0/3] [Batch 729/1173] [Iter 729] [G_Loss 15.275524] [D_Loss 0.043153]\n",
      "train [Epoch 0/3] [Batch 730/1173] [Iter 730] [G_Loss 6.751945] [D_Loss 0.064081]\n",
      "train [Epoch 0/3] [Batch 731/1173] [Iter 731] [G_Loss 4.289378] [D_Loss 0.052291]\n",
      "train [Epoch 0/3] [Batch 732/1173] [Iter 732] [G_Loss 4.053207] [D_Loss 0.040347]\n",
      "train [Epoch 0/3] [Batch 733/1173] [Iter 733] [G_Loss 3.847773] [D_Loss 0.049413]\n",
      "train [Epoch 0/3] [Batch 734/1173] [Iter 734] [G_Loss 2.053777] [D_Loss 0.246133]\n",
      "train [Epoch 0/3] [Batch 735/1173] [Iter 735] [G_Loss 9.980311] [D_Loss 1.047506]\n",
      "train [Epoch 0/3] [Batch 736/1173] [Iter 736] [G_Loss 0.152185] [D_Loss 2.779007]\n",
      "train [Epoch 0/3] [Batch 737/1173] [Iter 737] [G_Loss 3.640070] [D_Loss 0.110402]\n",
      "train [Epoch 0/3] [Batch 738/1173] [Iter 738] [G_Loss 4.434341] [D_Loss 0.181653]\n",
      "train [Epoch 0/3] [Batch 739/1173] [Iter 739] [G_Loss 0.680036] [D_Loss 0.879800]\n",
      "train [Epoch 0/3] [Batch 740/1173] [Iter 740] [G_Loss 4.249426] [D_Loss 5.030108]\n",
      "train [Epoch 0/3] [Batch 741/1173] [Iter 741] [G_Loss 2.729804] [D_Loss 0.792147]\n",
      "train [Epoch 0/3] [Batch 742/1173] [Iter 742] [G_Loss 1.662134] [D_Loss 0.398157]\n",
      "train [Epoch 0/3] [Batch 743/1173] [Iter 743] [G_Loss 1.118512] [D_Loss 0.510857]\n",
      "train [Epoch 0/3] [Batch 744/1173] [Iter 744] [G_Loss 0.666635] [D_Loss 0.829437]\n",
      "train [Epoch 0/3] [Batch 745/1173] [Iter 745] [G_Loss 0.762679] [D_Loss 0.705713]\n",
      "train [Epoch 0/3] [Batch 746/1173] [Iter 746] [G_Loss 1.440877] [D_Loss 0.404243]\n",
      "train [Epoch 0/3] [Batch 747/1173] [Iter 747] [G_Loss 1.741428] [D_Loss 0.393870]\n",
      "train [Epoch 0/3] [Batch 748/1173] [Iter 748] [G_Loss 1.788089] [D_Loss 0.410836]\n",
      "train [Epoch 0/3] [Batch 749/1173] [Iter 749] [G_Loss 2.001344] [D_Loss 0.445003]\n",
      "train [Epoch 0/3] [Batch 750/1173] [Iter 750] [G_Loss 1.645579] [D_Loss 0.546711]\n",
      "train [Epoch 0/3] [Batch 751/1173] [Iter 751] [G_Loss 1.490521] [D_Loss 0.509186]\n",
      "train [Epoch 0/3] [Batch 752/1173] [Iter 752] [G_Loss 1.985628] [D_Loss 0.305701]\n",
      "train [Epoch 0/3] [Batch 753/1173] [Iter 753] [G_Loss 2.616255] [D_Loss 0.204992]\n",
      "train [Epoch 0/3] [Batch 754/1173] [Iter 754] [G_Loss 2.944844] [D_Loss 0.180746]\n",
      "train [Epoch 0/3] [Batch 755/1173] [Iter 755] [G_Loss 3.031354] [D_Loss 0.167044]\n",
      "train [Epoch 0/3] [Batch 756/1173] [Iter 756] [G_Loss 2.934402] [D_Loss 0.130106]\n",
      "train [Epoch 0/3] [Batch 757/1173] [Iter 757] [G_Loss 2.533476] [D_Loss 0.173975]\n",
      "train [Epoch 0/3] [Batch 758/1173] [Iter 758] [G_Loss 2.039358] [D_Loss 0.234603]\n",
      "train [Epoch 0/3] [Batch 763/1173] [Iter 763] [G_Loss 2.471452] [D_Loss 0.179673]\n",
      "train [Epoch 0/3] [Batch 764/1173] [Iter 764] [G_Loss 2.593151] [D_Loss 0.182640]\n",
      "train [Epoch 0/3] [Batch 765/1173] [Iter 765] [G_Loss 2.843414] [D_Loss 0.140115]\n",
      "train [Epoch 0/3] [Batch 766/1173] [Iter 766] [G_Loss 3.091946] [D_Loss 0.140919]\n",
      "train [Epoch 0/3] [Batch 767/1173] [Iter 767] [G_Loss 2.945639] [D_Loss 0.127341]\n",
      "train [Epoch 0/3] [Batch 768/1173] [Iter 768] [G_Loss 2.885871] [D_Loss 0.159388]\n",
      "train [Epoch 0/3] [Batch 769/1173] [Iter 769] [G_Loss 2.645072] [D_Loss 0.169984]\n",
      "train [Epoch 0/3] [Batch 770/1173] [Iter 770] [G_Loss 2.801799] [D_Loss 0.184536]\n",
      "train [Epoch 0/3] [Batch 771/1173] [Iter 771] [G_Loss 3.323892] [D_Loss 0.193576]\n",
      "train [Epoch 0/3] [Batch 772/1173] [Iter 772] [G_Loss 2.637345] [D_Loss 0.289344]\n",
      "train [Epoch 0/3] [Batch 773/1173] [Iter 773] [G_Loss 2.792066] [D_Loss 0.376507]\n",
      "train [Epoch 0/3] [Batch 774/1173] [Iter 774] [G_Loss 2.504988] [D_Loss 0.323925]\n",
      "train [Epoch 0/3] [Batch 775/1173] [Iter 775] [G_Loss 3.645670] [D_Loss 0.533406]\n",
      "train [Epoch 0/3] [Batch 776/1173] [Iter 776] [G_Loss 1.382687] [D_Loss 0.639838]\n",
      "train [Epoch 0/3] [Batch 777/1173] [Iter 777] [G_Loss 6.019310] [D_Loss 2.063025]\n",
      "train [Epoch 0/3] [Batch 778/1173] [Iter 778] [G_Loss 3.044773] [D_Loss 0.511858]\n",
      "train [Epoch 0/3] [Batch 779/1173] [Iter 779] [G_Loss 1.593933] [D_Loss 0.352465]\n",
      "train [Epoch 0/3] [Batch 780/1173] [Iter 780] [G_Loss 1.719562] [D_Loss 0.354924]\n",
      "train [Epoch 0/3] [Batch 781/1173] [Iter 781] [G_Loss 1.811322] [D_Loss 0.315977]\n",
      "train [Epoch 0/3] [Batch 782/1173] [Iter 782] [G_Loss 1.828239] [D_Loss 0.344345]\n",
      "train [Epoch 0/3] [Batch 783/1173] [Iter 783] [G_Loss 2.114462] [D_Loss 0.430094]\n",
      "train [Epoch 0/3] [Batch 784/1173] [Iter 784] [G_Loss 2.811614] [D_Loss 0.465878]\n",
      "train [Epoch 0/3] [Batch 785/1173] [Iter 785] [G_Loss 2.568222] [D_Loss 0.425241]\n",
      "train [Epoch 0/3] [Batch 786/1173] [Iter 786] [G_Loss 2.384011] [D_Loss 0.390219]\n",
      "train [Epoch 0/3] [Batch 787/1173] [Iter 787] [G_Loss 2.049533] [D_Loss 0.465436]\n",
      "train [Epoch 0/3] [Batch 788/1173] [Iter 788] [G_Loss 2.421322] [D_Loss 0.853896]\n",
      "train [Epoch 0/3] [Batch 789/1173] [Iter 789] [G_Loss 4.369905] [D_Loss 1.287484]\n",
      "train [Epoch 0/3] [Batch 790/1173] [Iter 790] [G_Loss 0.758849] [D_Loss 2.207375]\n",
      "train [Epoch 0/3] [Batch 791/1173] [Iter 791] [G_Loss 4.592243] [D_Loss 0.989565]\n",
      "train [Epoch 0/3] [Batch 792/1173] [Iter 792] [G_Loss 3.258330] [D_Loss 0.407877]\n",
      "train [Epoch 0/3] [Batch 793/1173] [Iter 793] [G_Loss 1.733430] [D_Loss 0.371331]\n",
      "train [Epoch 0/3] [Batch 794/1173] [Iter 794] [G_Loss 2.050528] [D_Loss 0.283729]\n",
      "train [Epoch 0/3] [Batch 795/1173] [Iter 795] [G_Loss 1.980887] [D_Loss 0.381941]\n",
      "train [Epoch 0/3] [Batch 796/1173] [Iter 796] [G_Loss 2.925760] [D_Loss 0.421403]\n",
      "train [Epoch 0/3] [Batch 797/1173] [Iter 797] [G_Loss 2.437769] [D_Loss 0.526952]\n",
      "train [Epoch 0/3] [Batch 798/1173] [Iter 798] [G_Loss 2.694476] [D_Loss 0.251506]\n",
      "train [Epoch 0/3] [Batch 799/1173] [Iter 799] [G_Loss 4.071891] [D_Loss 0.186705]\n",
      "train [Epoch 0/3] [Batch 800/1173] [Iter 800] [G_Loss 0.033641] [D_Loss 18.444288]\n",
      "train [Epoch 0/3] [Batch 801/1173] [Iter 801] [G_Loss 18.347073] [D_Loss 6.357089]\n",
      "train [Epoch 0/3] [Batch 802/1173] [Iter 802] [G_Loss 0.019350] [D_Loss 15.830543]\n",
      "train [Epoch 0/3] [Batch 803/1173] [Iter 803] [G_Loss 5.292712] [D_Loss 3.553257]\n",
      "train [Epoch 0/3] [Batch 804/1173] [Iter 804] [G_Loss 0.418784] [D_Loss 1.276374]\n",
      "train [Epoch 0/3] [Batch 805/1173] [Iter 805] [G_Loss 1.773926] [D_Loss 0.757773]\n",
      "train [Epoch 0/3] [Batch 806/1173] [Iter 806] [G_Loss 0.631141] [D_Loss 0.941161]\n",
      "train [Epoch 0/3] [Batch 807/1173] [Iter 807] [G_Loss 2.899976] [D_Loss 1.488853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/3] [Batch 808/1173] [Iter 808] [G_Loss 0.267712] [D_Loss 1.659635]\n",
      "train [Epoch 0/3] [Batch 809/1173] [Iter 809] [G_Loss 1.220830] [D_Loss 0.732741]\n",
      "train [Epoch 0/3] [Batch 810/1173] [Iter 810] [G_Loss 1.736660] [D_Loss 0.974493]\n",
      "train [Epoch 0/3] [Batch 811/1173] [Iter 811] [G_Loss 0.564726] [D_Loss 0.997347]\n",
      "train [Epoch 0/3] [Batch 812/1173] [Iter 812] [G_Loss 1.252977] [D_Loss 0.803296]\n",
      "train [Epoch 0/3] [Batch 813/1173] [Iter 813] [G_Loss 1.094681] [D_Loss 0.853957]\n",
      "train [Epoch 0/3] [Batch 814/1173] [Iter 814] [G_Loss 0.746848] [D_Loss 0.918262]\n",
      "train [Epoch 0/3] [Batch 815/1173] [Iter 815] [G_Loss 1.154525] [D_Loss 0.989747]\n",
      "train [Epoch 0/3] [Batch 816/1173] [Iter 816] [G_Loss 0.609327] [D_Loss 1.068514]\n",
      "train [Epoch 0/3] [Batch 817/1173] [Iter 817] [G_Loss 1.064635] [D_Loss 1.187748]\n",
      "train [Epoch 0/3] [Batch 818/1173] [Iter 818] [G_Loss 0.433417] [D_Loss 1.312920]\n",
      "train [Epoch 0/3] [Batch 819/1173] [Iter 819] [G_Loss 1.381849] [D_Loss 1.489285]\n",
      "train [Epoch 0/3] [Batch 820/1173] [Iter 820] [G_Loss 0.196618] [D_Loss 1.945157]\n",
      "train [Epoch 0/3] [Batch 821/1173] [Iter 821] [G_Loss 1.228356] [D_Loss 1.084720]\n",
      "train [Epoch 0/3] [Batch 822/1173] [Iter 822] [G_Loss 1.252774] [D_Loss 0.822801]\n",
      "train [Epoch 0/3] [Batch 823/1173] [Iter 823] [G_Loss 1.133900] [D_Loss 0.725938]\n",
      "train [Epoch 0/3] [Batch 824/1173] [Iter 824] [G_Loss 1.421421] [D_Loss 0.487725]\n",
      "train [Epoch 0/3] [Batch 825/1173] [Iter 825] [G_Loss 4.634834] [D_Loss 2.087360]\n",
      "train [Epoch 0/3] [Batch 826/1173] [Iter 826] [G_Loss 0.000144] [D_Loss 10.709541]\n",
      "train [Epoch 0/3] [Batch 827/1173] [Iter 827] [G_Loss 0.505514] [D_Loss 1.190890]\n",
      "train [Epoch 0/3] [Batch 828/1173] [Iter 828] [G_Loss 1.233467] [D_Loss 1.119167]\n",
      "train [Epoch 0/3] [Batch 829/1173] [Iter 829] [G_Loss 1.007317] [D_Loss 1.047276]\n",
      "train [Epoch 0/3] [Batch 830/1173] [Iter 830] [G_Loss 0.772781] [D_Loss 1.063721]\n",
      "train [Epoch 0/3] [Batch 831/1173] [Iter 831] [G_Loss 1.072598] [D_Loss 0.999697]\n",
      "train [Epoch 0/3] [Batch 832/1173] [Iter 832] [G_Loss 1.123721] [D_Loss 0.991123]\n",
      "train [Epoch 0/3] [Batch 833/1173] [Iter 833] [G_Loss 0.869850] [D_Loss 0.926941]\n",
      "train [Epoch 0/3] [Batch 834/1173] [Iter 834] [G_Loss 1.307330] [D_Loss 0.727336]\n",
      "train [Epoch 0/3] [Batch 835/1173] [Iter 835] [G_Loss 1.364249] [D_Loss 0.875859]\n",
      "train [Epoch 0/3] [Batch 836/1173] [Iter 836] [G_Loss 0.825142] [D_Loss 0.871732]\n",
      "train [Epoch 0/3] [Batch 837/1173] [Iter 837] [G_Loss 1.724135] [D_Loss 0.726501]\n",
      "train [Epoch 0/3] [Batch 838/1173] [Iter 838] [G_Loss 1.631876] [D_Loss 0.582072]\n",
      "train [Epoch 0/3] [Batch 839/1173] [Iter 839] [G_Loss 1.304716] [D_Loss 0.640277]\n",
      "train [Epoch 0/3] [Batch 840/1173] [Iter 840] [G_Loss 1.839623] [D_Loss 0.460877]\n",
      "train [Epoch 0/3] [Batch 841/1173] [Iter 841] [G_Loss 1.828213] [D_Loss 0.503697]\n",
      "train [Epoch 0/3] [Batch 842/1173] [Iter 842] [G_Loss 1.503950] [D_Loss 0.513682]\n",
      "train [Epoch 0/3] [Batch 843/1173] [Iter 843] [G_Loss 2.224265] [D_Loss 0.629694]\n",
      "train [Epoch 0/3] [Batch 844/1173] [Iter 844] [G_Loss 1.275889] [D_Loss 0.679630]\n",
      "train [Epoch 0/3] [Batch 845/1173] [Iter 845] [G_Loss 2.144638] [D_Loss 0.717928]\n",
      "train [Epoch 0/3] [Batch 846/1173] [Iter 846] [G_Loss 1.252457] [D_Loss 0.860472]\n",
      "train [Epoch 0/3] [Batch 847/1173] [Iter 847] [G_Loss 2.270373] [D_Loss 0.935252]\n",
      "train [Epoch 0/3] [Batch 848/1173] [Iter 848] [G_Loss 0.261840] [D_Loss 2.051824]\n",
      "train [Epoch 0/3] [Batch 849/1173] [Iter 849] [G_Loss 4.284966] [D_Loss 2.964250]\n",
      "train [Epoch 0/3] [Batch 850/1173] [Iter 850] [G_Loss 0.314531] [D_Loss 1.799593]\n",
      "train [Epoch 0/3] [Batch 851/1173] [Iter 851] [G_Loss 1.353833] [D_Loss 0.772284]\n",
      "train [Epoch 0/3] [Batch 852/1173] [Iter 852] [G_Loss 3.122205] [D_Loss 0.987336]\n",
      "train [Epoch 0/3] [Batch 853/1173] [Iter 853] [G_Loss 0.801584] [D_Loss 0.928456]\n",
      "train [Epoch 0/3] [Batch 854/1173] [Iter 854] [G_Loss 1.259085] [D_Loss 0.812318]\n",
      "train [Epoch 0/3] [Batch 855/1173] [Iter 855] [G_Loss 1.802446] [D_Loss 0.870789]\n",
      "train [Epoch 0/3] [Batch 856/1173] [Iter 856] [G_Loss 0.758049] [D_Loss 0.898184]\n",
      "train [Epoch 0/3] [Batch 857/1173] [Iter 857] [G_Loss 2.724842] [D_Loss 1.091476]\n",
      "train [Epoch 0/3] [Batch 858/1173] [Iter 858] [G_Loss 0.542191] [D_Loss 1.031194]\n",
      "train [Epoch 0/3] [Batch 859/1173] [Iter 859] [G_Loss 3.115732] [D_Loss 1.312261]\n",
      "train [Epoch 0/3] [Batch 860/1173] [Iter 860] [G_Loss 1.413262] [D_Loss 0.608238]\n",
      "train [Epoch 0/3] [Batch 861/1173] [Iter 861] [G_Loss 0.776461] [D_Loss 0.807970]\n",
      "train [Epoch 0/3] [Batch 862/1173] [Iter 862] [G_Loss 2.411777] [D_Loss 0.488426]\n",
      "train [Epoch 0/3] [Batch 863/1173] [Iter 863] [G_Loss 1.716452] [D_Loss 0.449452]\n",
      "train [Epoch 0/3] [Batch 864/1173] [Iter 864] [G_Loss 1.846838] [D_Loss 0.626693]\n",
      "train [Epoch 0/3] [Batch 865/1173] [Iter 865] [G_Loss 2.119780] [D_Loss 0.513475]\n",
      "train [Epoch 0/3] [Batch 866/1173] [Iter 866] [G_Loss 0.888249] [D_Loss 0.846855]\n",
      "train [Epoch 0/3] [Batch 867/1173] [Iter 867] [G_Loss 3.845145] [D_Loss 1.558325]\n",
      "train [Epoch 0/3] [Batch 868/1173] [Iter 868] [G_Loss 1.321694] [D_Loss 0.764134]\n",
      "train [Epoch 0/3] [Batch 869/1173] [Iter 869] [G_Loss 1.526781] [D_Loss 0.552166]\n",
      "train [Epoch 0/3] [Batch 870/1173] [Iter 870] [G_Loss 2.118218] [D_Loss 0.491648]\n",
      "train [Epoch 0/3] [Batch 871/1173] [Iter 871] [G_Loss 2.342242] [D_Loss 0.336067]\n",
      "train [Epoch 0/3] [Batch 872/1173] [Iter 872] [G_Loss 1.771439] [D_Loss 0.461273]\n",
      "train [Epoch 0/3] [Batch 873/1173] [Iter 873] [G_Loss 2.148517] [D_Loss 0.459876]\n",
      "train [Epoch 0/3] [Batch 874/1173] [Iter 874] [G_Loss 2.435119] [D_Loss 0.724408]\n",
      "train [Epoch 0/3] [Batch 875/1173] [Iter 875] [G_Loss 3.119206] [D_Loss 0.329601]\n",
      "train [Epoch 0/3] [Batch 876/1173] [Iter 876] [G_Loss 3.281627] [D_Loss 0.384368]\n",
      "train [Epoch 0/3] [Batch 877/1173] [Iter 877] [G_Loss 3.900383] [D_Loss 0.356128]\n",
      "train [Epoch 0/3] [Batch 878/1173] [Iter 878] [G_Loss 0.917420] [D_Loss 1.748438]\n",
      "train [Epoch 0/3] [Batch 879/1173] [Iter 879] [G_Loss 8.294227] [D_Loss 5.807272]\n",
      "train [Epoch 0/3] [Batch 880/1173] [Iter 880] [G_Loss 3.585694] [D_Loss 2.715378]\n",
      "train [Epoch 0/3] [Batch 881/1173] [Iter 881] [G_Loss 0.271484] [D_Loss 1.979780]\n",
      "train [Epoch 0/3] [Batch 882/1173] [Iter 882] [G_Loss 0.603858] [D_Loss 1.091576]\n",
      "train [Epoch 0/3] [Batch 883/1173] [Iter 883] [G_Loss 1.202912] [D_Loss 0.945232]\n",
      "train [Epoch 0/3] [Batch 884/1173] [Iter 884] [G_Loss 0.951103] [D_Loss 0.995912]\n",
      "train [Epoch 0/3] [Batch 885/1173] [Iter 885] [G_Loss 1.331730] [D_Loss 1.096239]\n",
      "train [Epoch 0/3] [Batch 886/1173] [Iter 886] [G_Loss 0.778221] [D_Loss 1.225981]\n",
      "train [Epoch 0/3] [Batch 887/1173] [Iter 887] [G_Loss 2.074308] [D_Loss 1.463156]\n",
      "train [Epoch 0/3] [Batch 888/1173] [Iter 888] [G_Loss 0.327176] [D_Loss 1.750905]\n",
      "train [Epoch 0/3] [Batch 889/1173] [Iter 889] [G_Loss 1.547987] [D_Loss 1.226598]\n",
      "train [Epoch 0/3] [Batch 890/1173] [Iter 890] [G_Loss 0.971987] [D_Loss 1.201929]\n",
      "train [Epoch 0/3] [Batch 891/1173] [Iter 891] [G_Loss 0.831839] [D_Loss 1.172076]\n",
      "train [Epoch 0/3] [Batch 892/1173] [Iter 892] [G_Loss 0.990458] [D_Loss 1.139084]\n",
      "train [Epoch 0/3] [Batch 893/1173] [Iter 893] [G_Loss 1.115608] [D_Loss 1.067281]\n",
      "train [Epoch 0/3] [Batch 894/1173] [Iter 894] [G_Loss 1.123993] [D_Loss 1.092358]\n",
      "train [Epoch 0/3] [Batch 895/1173] [Iter 895] [G_Loss 1.058462] [D_Loss 1.005110]\n",
      "train [Epoch 0/3] [Batch 896/1173] [Iter 896] [G_Loss 0.964236] [D_Loss 1.002068]\n",
      "train [Epoch 0/3] [Batch 897/1173] [Iter 897] [G_Loss 1.189407] [D_Loss 0.988885]\n",
      "train [Epoch 0/3] [Batch 898/1173] [Iter 898] [G_Loss 1.100162] [D_Loss 0.990762]\n",
      "train [Epoch 0/3] [Batch 899/1173] [Iter 899] [G_Loss 1.031406] [D_Loss 0.960732]\n",
      "train [Epoch 0/3] [Batch 900/1173] [Iter 900] [G_Loss 1.132250] [D_Loss 0.969024]\n",
      "train [Epoch 0/3] [Batch 901/1173] [Iter 901] [G_Loss 1.076048] [D_Loss 0.972417]\n",
      "train [Epoch 0/3] [Batch 902/1173] [Iter 902] [G_Loss 1.105116] [D_Loss 0.964885]\n",
      "train [Epoch 0/3] [Batch 903/1173] [Iter 903] [G_Loss 1.088986] [D_Loss 0.968885]\n",
      "train [Epoch 0/3] [Batch 904/1173] [Iter 904] [G_Loss 1.154214] [D_Loss 0.947678]\n",
      "train [Epoch 0/3] [Batch 905/1173] [Iter 905] [G_Loss 1.133641] [D_Loss 0.989265]\n",
      "train [Epoch 0/3] [Batch 906/1173] [Iter 906] [G_Loss 1.035499] [D_Loss 0.901387]\n",
      "train [Epoch 0/3] [Batch 907/1173] [Iter 907] [G_Loss 1.261890] [D_Loss 0.954661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/3] [Batch 908/1173] [Iter 908] [G_Loss 1.198653] [D_Loss 0.944024]\n",
      "train [Epoch 0/3] [Batch 909/1173] [Iter 909] [G_Loss 1.044720] [D_Loss 0.904678]\n",
      "train [Epoch 0/3] [Batch 910/1173] [Iter 910] [G_Loss 1.154809] [D_Loss 0.875213]\n",
      "train [Epoch 0/3] [Batch 911/1173] [Iter 911] [G_Loss 1.340376] [D_Loss 0.963220]\n",
      "train [Epoch 0/3] [Batch 912/1173] [Iter 912] [G_Loss 1.115141] [D_Loss 0.946150]\n",
      "train [Epoch 0/3] [Batch 913/1173] [Iter 913] [G_Loss 1.075212] [D_Loss 0.917696]\n",
      "train [Epoch 0/3] [Batch 914/1173] [Iter 914] [G_Loss 1.114110] [D_Loss 0.869080]\n",
      "train [Epoch 0/3] [Batch 915/1173] [Iter 915] [G_Loss 1.313028] [D_Loss 0.863188]\n",
      "train [Epoch 0/3] [Batch 916/1173] [Iter 916] [G_Loss 1.279412] [D_Loss 0.795904]\n",
      "train [Epoch 0/3] [Batch 917/1173] [Iter 917] [G_Loss 1.292339] [D_Loss 0.785503]\n",
      "train [Epoch 0/3] [Batch 918/1173] [Iter 918] [G_Loss 1.207390] [D_Loss 0.763093]\n",
      "train [Epoch 0/3] [Batch 919/1173] [Iter 919] [G_Loss 1.505612] [D_Loss 0.821572]\n",
      "train [Epoch 0/3] [Batch 920/1173] [Iter 920] [G_Loss 1.398775] [D_Loss 0.879429]\n",
      "train [Epoch 0/3] [Batch 921/1173] [Iter 921] [G_Loss 1.117864] [D_Loss 0.883935]\n",
      "train [Epoch 0/3] [Batch 922/1173] [Iter 922] [G_Loss 1.322547] [D_Loss 0.866953]\n",
      "train [Epoch 0/3] [Batch 923/1173] [Iter 923] [G_Loss 1.561248] [D_Loss 1.171676]\n",
      "train [Epoch 0/3] [Batch 924/1173] [Iter 924] [G_Loss 0.901429] [D_Loss 1.316925]\n",
      "train [Epoch 0/3] [Batch 925/1173] [Iter 925] [G_Loss 1.229267] [D_Loss 1.142345]\n",
      "train [Epoch 0/3] [Batch 926/1173] [Iter 926] [G_Loss 1.323310] [D_Loss 1.104650]\n",
      "train [Epoch 0/3] [Batch 927/1173] [Iter 927] [G_Loss 1.085011] [D_Loss 1.132688]\n",
      "train [Epoch 0/3] [Batch 928/1173] [Iter 928] [G_Loss 1.673187] [D_Loss 1.001746]\n",
      "train [Epoch 0/3] [Batch 929/1173] [Iter 929] [G_Loss 1.450243] [D_Loss 1.190457]\n",
      "train [Epoch 0/3] [Batch 930/1173] [Iter 930] [G_Loss 1.148785] [D_Loss 1.627694]\n",
      "train [Epoch 0/3] [Batch 931/1173] [Iter 931] [G_Loss 1.991423] [D_Loss 1.841687]\n",
      "train [Epoch 0/3] [Batch 932/1173] [Iter 932] [G_Loss 0.134530] [D_Loss 2.433351]\n",
      "train [Epoch 0/3] [Batch 933/1173] [Iter 933] [G_Loss 0.376925] [D_Loss 1.565789]\n",
      "train [Epoch 0/3] [Batch 934/1173] [Iter 934] [G_Loss 1.010323] [D_Loss 1.135964]\n",
      "train [Epoch 0/3] [Batch 935/1173] [Iter 935] [G_Loss 1.263577] [D_Loss 1.070690]\n",
      "train [Epoch 0/3] [Batch 936/1173] [Iter 936] [G_Loss 1.381885] [D_Loss 1.215692]\n",
      "train [Epoch 0/3] [Batch 937/1173] [Iter 937] [G_Loss 1.124469] [D_Loss 1.252885]\n",
      "train [Epoch 0/3] [Batch 938/1173] [Iter 938] [G_Loss 0.935935] [D_Loss 1.296618]\n",
      "train [Epoch 0/3] [Batch 939/1173] [Iter 939] [G_Loss 1.013731] [D_Loss 1.165000]\n",
      "train [Epoch 0/3] [Batch 940/1173] [Iter 940] [G_Loss 1.212979] [D_Loss 1.224858]\n",
      "train [Epoch 0/3] [Batch 941/1173] [Iter 941] [G_Loss 1.155743] [D_Loss 1.147178]\n",
      "train [Epoch 0/3] [Batch 942/1173] [Iter 942] [G_Loss 1.000869] [D_Loss 1.112085]\n",
      "train [Epoch 0/3] [Batch 943/1173] [Iter 943] [G_Loss 0.983365] [D_Loss 1.101157]\n",
      "train [Epoch 0/3] [Batch 944/1173] [Iter 944] [G_Loss 1.277090] [D_Loss 1.103956]\n",
      "train [Epoch 0/3] [Batch 945/1173] [Iter 945] [G_Loss 1.252455] [D_Loss 1.122792]\n",
      "train [Epoch 0/3] [Batch 946/1173] [Iter 946] [G_Loss 0.975356] [D_Loss 1.092639]\n",
      "train [Epoch 0/3] [Batch 947/1173] [Iter 947] [G_Loss 1.153525] [D_Loss 1.051504]\n",
      "train [Epoch 0/3] [Batch 948/1173] [Iter 948] [G_Loss 1.313616] [D_Loss 1.126951]\n",
      "train [Epoch 0/3] [Batch 949/1173] [Iter 949] [G_Loss 0.906641] [D_Loss 1.096845]\n",
      "train [Epoch 0/3] [Batch 950/1173] [Iter 950] [G_Loss 1.319011] [D_Loss 1.120999]\n",
      "train [Epoch 0/3] [Batch 951/1173] [Iter 951] [G_Loss 1.171967] [D_Loss 1.063450]\n",
      "train [Epoch 0/3] [Batch 952/1173] [Iter 952] [G_Loss 0.961771] [D_Loss 1.023295]\n",
      "train [Epoch 0/3] [Batch 953/1173] [Iter 953] [G_Loss 1.296775] [D_Loss 1.115345]\n",
      "train [Epoch 0/3] [Batch 954/1173] [Iter 954] [G_Loss 1.263332] [D_Loss 1.039055]\n",
      "train [Epoch 0/3] [Batch 955/1173] [Iter 955] [G_Loss 0.961953] [D_Loss 1.100078]\n",
      "train [Epoch 0/3] [Batch 956/1173] [Iter 956] [G_Loss 1.327668] [D_Loss 1.118967]\n",
      "train [Epoch 0/3] [Batch 957/1173] [Iter 957] [G_Loss 1.108560] [D_Loss 1.081259]\n",
      "train [Epoch 0/3] [Batch 958/1173] [Iter 958] [G_Loss 0.935098] [D_Loss 1.166714]\n",
      "train [Epoch 0/3] [Batch 959/1173] [Iter 959] [G_Loss 1.071465] [D_Loss 1.187799]\n",
      "train [Epoch 0/3] [Batch 960/1173] [Iter 960] [G_Loss 1.153471] [D_Loss 1.036734]\n",
      "train [Epoch 0/3] [Batch 961/1173] [Iter 961] [G_Loss 1.070469] [D_Loss 1.015370]\n",
      "train [Epoch 0/3] [Batch 962/1173] [Iter 962] [G_Loss 1.154903] [D_Loss 0.944706]\n",
      "train [Epoch 0/3] [Batch 963/1173] [Iter 963] [G_Loss 1.088997] [D_Loss 1.024387]\n",
      "train [Epoch 0/3] [Batch 964/1173] [Iter 964] [G_Loss 1.075934] [D_Loss 1.007025]\n",
      "train [Epoch 0/3] [Batch 965/1173] [Iter 965] [G_Loss 1.302756] [D_Loss 1.046330]\n",
      "train [Epoch 0/3] [Batch 966/1173] [Iter 966] [G_Loss 1.083527] [D_Loss 1.025458]\n",
      "train [Epoch 0/3] [Batch 967/1173] [Iter 967] [G_Loss 1.082466] [D_Loss 1.013429]\n",
      "train [Epoch 0/3] [Batch 968/1173] [Iter 968] [G_Loss 1.496130] [D_Loss 1.034729]\n",
      "train [Epoch 0/3] [Batch 969/1173] [Iter 969] [G_Loss 1.078588] [D_Loss 0.988314]\n",
      "train [Epoch 0/3] [Batch 970/1173] [Iter 970] [G_Loss 1.272863] [D_Loss 0.994984]\n",
      "train [Epoch 0/3] [Batch 971/1173] [Iter 971] [G_Loss 1.170076] [D_Loss 1.036311]\n",
      "train [Epoch 0/3] [Batch 972/1173] [Iter 972] [G_Loss 1.220164] [D_Loss 1.013026]\n",
      "train [Epoch 0/3] [Batch 973/1173] [Iter 973] [G_Loss 1.254480] [D_Loss 1.043564]\n",
      "train [Epoch 0/3] [Batch 974/1173] [Iter 974] [G_Loss 1.188829] [D_Loss 1.009352]\n",
      "train [Epoch 0/3] [Batch 975/1173] [Iter 975] [G_Loss 1.262844] [D_Loss 0.967694]\n",
      "train [Epoch 0/3] [Batch 976/1173] [Iter 976] [G_Loss 1.294722] [D_Loss 0.897266]\n",
      "train [Epoch 0/3] [Batch 977/1173] [Iter 977] [G_Loss 1.244224] [D_Loss 0.954781]\n",
      "train [Epoch 0/3] [Batch 978/1173] [Iter 978] [G_Loss 1.130731] [D_Loss 1.003526]\n",
      "train [Epoch 0/3] [Batch 979/1173] [Iter 979] [G_Loss 1.066142] [D_Loss 1.067640]\n",
      "train [Epoch 0/3] [Batch 980/1173] [Iter 980] [G_Loss 1.168381] [D_Loss 0.984404]\n",
      "train [Epoch 0/3] [Batch 981/1173] [Iter 981] [G_Loss 1.134309] [D_Loss 0.954255]\n",
      "train [Epoch 0/3] [Batch 982/1173] [Iter 982] [G_Loss 0.953247] [D_Loss 1.003880]\n",
      "train [Epoch 0/3] [Batch 983/1173] [Iter 983] [G_Loss 1.306681] [D_Loss 0.882537]\n",
      "train [Epoch 0/3] [Batch 984/1173] [Iter 984] [G_Loss 1.197926] [D_Loss 0.976413]\n",
      "train [Epoch 0/3] [Batch 985/1173] [Iter 985] [G_Loss 1.022758] [D_Loss 0.979946]\n",
      "train [Epoch 0/3] [Batch 986/1173] [Iter 986] [G_Loss 1.219761] [D_Loss 0.954719]\n",
      "train [Epoch 0/3] [Batch 987/1173] [Iter 987] [G_Loss 1.216532] [D_Loss 0.957550]\n",
      "train [Epoch 0/3] [Batch 988/1173] [Iter 988] [G_Loss 0.945620] [D_Loss 1.068927]\n",
      "train [Epoch 0/3] [Batch 989/1173] [Iter 989] [G_Loss 1.211728] [D_Loss 0.969164]\n",
      "train [Epoch 0/3] [Batch 990/1173] [Iter 990] [G_Loss 1.226966] [D_Loss 0.974221]\n",
      "train [Epoch 0/3] [Batch 991/1173] [Iter 991] [G_Loss 0.976222] [D_Loss 1.035230]\n",
      "train [Epoch 0/3] [Batch 992/1173] [Iter 992] [G_Loss 1.442700] [D_Loss 1.053795]\n",
      "train [Epoch 0/3] [Batch 993/1173] [Iter 993] [G_Loss 0.958042] [D_Loss 1.054707]\n",
      "train [Epoch 0/3] [Batch 994/1173] [Iter 994] [G_Loss 1.306029] [D_Loss 1.074474]\n",
      "train [Epoch 0/3] [Batch 995/1173] [Iter 995] [G_Loss 1.126169] [D_Loss 1.030256]\n",
      "train [Epoch 0/3] [Batch 996/1173] [Iter 996] [G_Loss 0.984002] [D_Loss 1.175755]\n",
      "train [Epoch 0/3] [Batch 997/1173] [Iter 997] [G_Loss 0.853390] [D_Loss 1.335955]\n",
      "train [Epoch 0/3] [Batch 998/1173] [Iter 998] [G_Loss 1.325748] [D_Loss 1.260855]\n",
      "train [Epoch 0/3] [Batch 999/1173] [Iter 999] [G_Loss 0.805390] [D_Loss 1.247747]\n",
      "train [Epoch 0/3] [Batch 1000/1173] [Iter 1000] [G_Loss 0.966054] [D_Loss 1.116966]\n",
      "train [Epoch 0/3] [Batch 1001/1173] [Iter 1001] [G_Loss 1.012408] [D_Loss 1.107911]\n",
      "train [Epoch 0/3] [Batch 1002/1173] [Iter 1002] [G_Loss 1.035907] [D_Loss 1.031104]\n",
      "train [Epoch 0/3] [Batch 1003/1173] [Iter 1003] [G_Loss 0.923945] [D_Loss 1.086190]\n",
      "train [Epoch 0/3] [Batch 1004/1173] [Iter 1004] [G_Loss 1.015634] [D_Loss 1.095992]\n",
      "train [Epoch 0/3] [Batch 1005/1173] [Iter 1005] [G_Loss 1.234431] [D_Loss 0.908200]\n",
      "train [Epoch 0/3] [Batch 1006/1173] [Iter 1006] [G_Loss 1.084511] [D_Loss 0.923386]\n",
      "train [Epoch 0/3] [Batch 1007/1173] [Iter 1007] [G_Loss 1.514900] [D_Loss 0.911698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/3] [Batch 1008/1173] [Iter 1008] [G_Loss 1.065910] [D_Loss 0.952209]\n",
      "train [Epoch 0/3] [Batch 1009/1173] [Iter 1009] [G_Loss 1.459553] [D_Loss 0.910169]\n",
      "train [Epoch 0/3] [Batch 1010/1173] [Iter 1010] [G_Loss 1.010926] [D_Loss 1.037461]\n",
      "train [Epoch 0/3] [Batch 1011/1173] [Iter 1011] [G_Loss 1.498757] [D_Loss 0.925913]\n",
      "train [Epoch 0/3] [Batch 1012/1173] [Iter 1012] [G_Loss 0.901807] [D_Loss 1.070697]\n",
      "train [Epoch 0/3] [Batch 1013/1173] [Iter 1013] [G_Loss 1.520600] [D_Loss 1.095003]\n",
      "train [Epoch 0/3] [Batch 1014/1173] [Iter 1014] [G_Loss 0.627022] [D_Loss 1.160431]\n",
      "train [Epoch 0/3] [Batch 1015/1173] [Iter 1015] [G_Loss 1.679380] [D_Loss 1.174151]\n",
      "train [Epoch 0/3] [Batch 1016/1173] [Iter 1016] [G_Loss 1.084946] [D_Loss 1.015655]\n",
      "train [Epoch 0/3] [Batch 1017/1173] [Iter 1017] [G_Loss 0.729856] [D_Loss 1.037287]\n",
      "train [Epoch 0/3] [Batch 1018/1173] [Iter 1018] [G_Loss 1.342327] [D_Loss 1.101861]\n",
      "train [Epoch 0/3] [Batch 1019/1173] [Iter 1019] [G_Loss 0.975604] [D_Loss 0.947667]\n",
      "train [Epoch 0/3] [Batch 1020/1173] [Iter 1020] [G_Loss 0.951583] [D_Loss 0.952734]\n",
      "train [Epoch 0/3] [Batch 1021/1173] [Iter 1021] [G_Loss 1.161732] [D_Loss 1.023307]\n",
      "train [Epoch 0/3] [Batch 1022/1173] [Iter 1022] [G_Loss 1.210322] [D_Loss 1.067813]\n",
      "train [Epoch 0/3] [Batch 1023/1173] [Iter 1023] [G_Loss 0.975263] [D_Loss 1.017360]\n",
      "train [Epoch 0/3] [Batch 1024/1173] [Iter 1024] [G_Loss 1.479852] [D_Loss 1.029144]\n",
      "train [Epoch 0/3] [Batch 1025/1173] [Iter 1025] [G_Loss 0.779575] [D_Loss 1.077152]\n",
      "train [Epoch 0/3] [Batch 1026/1173] [Iter 1026] [G_Loss 1.710145] [D_Loss 1.103907]\n",
      "train [Epoch 0/3] [Batch 1027/1173] [Iter 1027] [G_Loss 0.953267] [D_Loss 0.989989]\n",
      "train [Epoch 0/3] [Batch 1028/1173] [Iter 1028] [G_Loss 0.970098] [D_Loss 1.037037]\n",
      "train [Epoch 0/3] [Batch 1029/1173] [Iter 1029] [G_Loss 1.032743] [D_Loss 0.981556]\n",
      "train [Epoch 0/3] [Batch 1030/1173] [Iter 1030] [G_Loss 1.115934] [D_Loss 0.883649]\n",
      "train [Epoch 0/3] [Batch 1031/1173] [Iter 1031] [G_Loss 1.294124] [D_Loss 0.880793]\n",
      "train [Epoch 0/3] [Batch 1032/1173] [Iter 1032] [G_Loss 1.305954] [D_Loss 0.995252]\n",
      "train [Epoch 0/3] [Batch 1033/1173] [Iter 1033] [G_Loss 1.347769] [D_Loss 0.998875]\n",
      "train [Epoch 0/3] [Batch 1034/1173] [Iter 1034] [G_Loss 1.010862] [D_Loss 1.022519]\n",
      "train [Epoch 0/3] [Batch 1035/1173] [Iter 1035] [G_Loss 0.825130] [D_Loss 1.201786]\n",
      "train [Epoch 0/3] [Batch 1036/1173] [Iter 1036] [G_Loss 0.814776] [D_Loss 1.788644]\n",
      "train [Epoch 0/3] [Batch 1037/1173] [Iter 1037] [G_Loss 0.596392] [D_Loss 3.352460]\n",
      "train [Epoch 0/3] [Batch 1038/1173] [Iter 1038] [G_Loss 0.389961] [D_Loss 2.112438]\n",
      "train [Epoch 0/3] [Batch 1039/1173] [Iter 1039] [G_Loss 1.402764] [D_Loss 0.777803]\n",
      "train [Epoch 0/3] [Batch 1040/1173] [Iter 1040] [G_Loss 2.081233] [D_Loss 0.720071]\n",
      "train [Epoch 0/3] [Batch 1041/1173] [Iter 1041] [G_Loss 1.235606] [D_Loss 1.534271]\n",
      "train [Epoch 0/3] [Batch 1042/1173] [Iter 1042] [G_Loss 1.315088] [D_Loss 1.688118]\n",
      "train [Epoch 0/3] [Batch 1043/1173] [Iter 1043] [G_Loss 3.907346] [D_Loss 2.430764]\n",
      "train [Epoch 0/3] [Batch 1044/1173] [Iter 1044] [G_Loss 1.261356] [D_Loss 1.273225]\n",
      "train [Epoch 0/3] [Batch 1045/1173] [Iter 1045] [G_Loss 1.128774] [D_Loss 1.141664]\n",
      "train [Epoch 0/3] [Batch 1046/1173] [Iter 1046] [G_Loss 1.237901] [D_Loss 1.093203]\n",
      "train [Epoch 0/3] [Batch 1047/1173] [Iter 1047] [G_Loss 1.035020] [D_Loss 1.118027]\n",
      "train [Epoch 0/3] [Batch 1048/1173] [Iter 1048] [G_Loss 0.967986] [D_Loss 1.037779]\n",
      "train [Epoch 0/3] [Batch 1049/1173] [Iter 1049] [G_Loss 1.126141] [D_Loss 1.024993]\n",
      "train [Epoch 0/3] [Batch 1050/1173] [Iter 1050] [G_Loss 1.034789] [D_Loss 1.038646]\n",
      "train [Epoch 0/3] [Batch 1051/1173] [Iter 1051] [G_Loss 0.926488] [D_Loss 1.054191]\n",
      "train [Epoch 0/3] [Batch 1052/1173] [Iter 1052] [G_Loss 1.221557] [D_Loss 1.024227]\n",
      "train [Epoch 0/3] [Batch 1053/1173] [Iter 1053] [G_Loss 0.818568] [D_Loss 1.098256]\n",
      "train [Epoch 0/3] [Batch 1054/1173] [Iter 1054] [G_Loss 1.054152] [D_Loss 1.076499]\n",
      "train [Epoch 0/3] [Batch 1055/1173] [Iter 1055] [G_Loss 1.260700] [D_Loss 1.111683]\n",
      "train [Epoch 0/3] [Batch 1056/1173] [Iter 1056] [G_Loss 0.746056] [D_Loss 1.151515]\n",
      "train [Epoch 0/3] [Batch 1057/1173] [Iter 1057] [G_Loss 1.290523] [D_Loss 1.041999]\n",
      "train [Epoch 0/3] [Batch 1058/1173] [Iter 1058] [G_Loss 0.941294] [D_Loss 1.116366]\n",
      "train [Epoch 0/3] [Batch 1059/1173] [Iter 1059] [G_Loss 0.807106] [D_Loss 1.117271]\n",
      "train [Epoch 0/3] [Batch 1060/1173] [Iter 1060] [G_Loss 1.292954] [D_Loss 1.160897]\n",
      "train [Epoch 0/3] [Batch 1061/1173] [Iter 1061] [G_Loss 0.579128] [D_Loss 1.304393]\n",
      "train [Epoch 0/3] [Batch 1062/1173] [Iter 1062] [G_Loss 1.330460] [D_Loss 1.189492]\n",
      "train [Epoch 0/3] [Batch 1063/1173] [Iter 1063] [G_Loss 1.174890] [D_Loss 1.191439]\n",
      "train [Epoch 0/3] [Batch 1064/1173] [Iter 1064] [G_Loss 0.619338] [D_Loss 1.240544]\n",
      "train [Epoch 0/3] [Batch 1065/1173] [Iter 1065] [G_Loss 1.373989] [D_Loss 1.249291]\n",
      "train [Epoch 0/3] [Batch 1066/1173] [Iter 1066] [G_Loss 0.911892] [D_Loss 1.143177]\n",
      "train [Epoch 0/3] [Batch 1067/1173] [Iter 1067] [G_Loss 0.785384] [D_Loss 1.096416]\n",
      "train [Epoch 0/3] [Batch 1068/1173] [Iter 1068] [G_Loss 0.947166] [D_Loss 1.123837]\n",
      "train [Epoch 0/3] [Batch 1069/1173] [Iter 1069] [G_Loss 0.975781] [D_Loss 1.166059]\n",
      "train [Epoch 0/3] [Batch 1070/1173] [Iter 1070] [G_Loss 0.891425] [D_Loss 1.123354]\n",
      "train [Epoch 0/3] [Batch 1071/1173] [Iter 1071] [G_Loss 0.827371] [D_Loss 1.102503]\n",
      "train [Epoch 0/3] [Batch 1072/1173] [Iter 1072] [G_Loss 0.966771] [D_Loss 1.116955]\n",
      "train [Epoch 0/3] [Batch 1073/1173] [Iter 1073] [G_Loss 1.003348] [D_Loss 1.069522]\n",
      "train [Epoch 0/3] [Batch 1074/1173] [Iter 1074] [G_Loss 0.834905] [D_Loss 1.045982]\n",
      "train [Epoch 0/3] [Batch 1075/1173] [Iter 1075] [G_Loss 1.002724] [D_Loss 1.067071]\n",
      "train [Epoch 0/3] [Batch 1076/1173] [Iter 1076] [G_Loss 0.980061] [D_Loss 1.049065]\n",
      "train [Epoch 0/3] [Batch 1077/1173] [Iter 1077] [G_Loss 1.056566] [D_Loss 1.058046]\n",
      "train [Epoch 0/3] [Batch 1078/1173] [Iter 1078] [G_Loss 0.866389] [D_Loss 1.008518]\n",
      "train [Epoch 0/3] [Batch 1079/1173] [Iter 1079] [G_Loss 1.129347] [D_Loss 0.981744]\n",
      "train [Epoch 0/3] [Batch 1080/1173] [Iter 1080] [G_Loss 0.852569] [D_Loss 0.997123]\n",
      "train [Epoch 0/3] [Batch 1081/1173] [Iter 1081] [G_Loss 1.072169] [D_Loss 1.021965]\n",
      "train [Epoch 0/3] [Batch 1082/1173] [Iter 1082] [G_Loss 0.930755] [D_Loss 1.054558]\n",
      "train [Epoch 0/3] [Batch 1083/1173] [Iter 1083] [G_Loss 0.925520] [D_Loss 0.970787]\n",
      "train [Epoch 0/3] [Batch 1084/1173] [Iter 1084] [G_Loss 1.040806] [D_Loss 1.037312]\n",
      "train [Epoch 0/3] [Batch 1085/1173] [Iter 1085] [G_Loss 0.880587] [D_Loss 0.967181]\n",
      "train [Epoch 0/3] [Batch 1086/1173] [Iter 1086] [G_Loss 0.942389] [D_Loss 0.966919]\n",
      "train [Epoch 0/3] [Batch 1087/1173] [Iter 1087] [G_Loss 1.061984] [D_Loss 0.932461]\n",
      "train [Epoch 0/3] [Batch 1088/1173] [Iter 1088] [G_Loss 0.941980] [D_Loss 0.983669]\n",
      "train [Epoch 0/3] [Batch 1089/1173] [Iter 1089] [G_Loss 1.109155] [D_Loss 1.010810]\n",
      "train [Epoch 0/3] [Batch 1090/1173] [Iter 1090] [G_Loss 1.069387] [D_Loss 0.919424]\n",
      "train [Epoch 0/3] [Batch 1091/1173] [Iter 1091] [G_Loss 0.911405] [D_Loss 0.910316]\n",
      "train [Epoch 0/3] [Batch 1092/1173] [Iter 1092] [G_Loss 1.319496] [D_Loss 0.929906]\n",
      "train [Epoch 0/3] [Batch 1093/1173] [Iter 1093] [G_Loss 0.831708] [D_Loss 0.937301]\n",
      "train [Epoch 0/3] [Batch 1094/1173] [Iter 1094] [G_Loss 1.273890] [D_Loss 0.962404]\n",
      "train [Epoch 0/3] [Batch 1095/1173] [Iter 1095] [G_Loss 0.926251] [D_Loss 0.962822]\n",
      "train [Epoch 0/3] [Batch 1096/1173] [Iter 1096] [G_Loss 1.151379] [D_Loss 1.045368]\n",
      "train [Epoch 0/3] [Batch 1097/1173] [Iter 1097] [G_Loss 0.688465] [D_Loss 1.099188]\n",
      "train [Epoch 0/3] [Batch 1098/1173] [Iter 1098] [G_Loss 2.450708] [D_Loss 1.719457]\n",
      "train [Epoch 0/3] [Batch 1099/1173] [Iter 1099] [G_Loss 0.976596] [D_Loss 0.933197]\n",
      "train [Epoch 0/3] [Batch 1100/1173] [Iter 1100] [G_Loss 0.500927] [D_Loss 1.190106]\n",
      "train [Epoch 0/3] [Batch 1101/1173] [Iter 1101] [G_Loss 2.003989] [D_Loss 0.905666]\n",
      "train [Epoch 0/3] [Batch 1102/1173] [Iter 1102] [G_Loss 0.854686] [D_Loss 0.833123]\n",
      "train [Epoch 0/3] [Batch 1103/1173] [Iter 1103] [G_Loss 0.896342] [D_Loss 0.796732]\n",
      "train [Epoch 0/3] [Batch 1104/1173] [Iter 1104] [G_Loss 1.096485] [D_Loss 0.845044]\n",
      "train [Epoch 0/3] [Batch 1105/1173] [Iter 1105] [G_Loss 1.150810] [D_Loss 1.026302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/3] [Batch 1106/1173] [Iter 1106] [G_Loss 1.107726] [D_Loss 1.039561]\n",
      "train [Epoch 0/3] [Batch 1107/1173] [Iter 1107] [G_Loss 1.212166] [D_Loss 0.924400]\n",
      "train [Epoch 0/3] [Batch 1108/1173] [Iter 1108] [G_Loss 1.351444] [D_Loss 0.923656]\n",
      "train [Epoch 0/3] [Batch 1109/1173] [Iter 1109] [G_Loss 1.154356] [D_Loss 0.955602]\n",
      "train [Epoch 0/3] [Batch 1110/1173] [Iter 1110] [G_Loss 0.988220] [D_Loss 0.936144]\n",
      "train [Epoch 0/3] [Batch 1111/1173] [Iter 1111] [G_Loss 1.954491] [D_Loss 1.123429]\n",
      "train [Epoch 0/3] [Batch 1112/1173] [Iter 1112] [G_Loss 0.845591] [D_Loss 0.986725]\n",
      "train [Epoch 0/3] [Batch 1113/1173] [Iter 1113] [G_Loss 0.835885] [D_Loss 1.039277]\n",
      "train [Epoch 0/3] [Batch 1114/1173] [Iter 1114] [G_Loss 1.007400] [D_Loss 0.954922]\n",
      "train [Epoch 0/3] [Batch 1115/1173] [Iter 1115] [G_Loss 1.298639] [D_Loss 0.672107]\n",
      "train [Epoch 0/3] [Batch 1116/1173] [Iter 1116] [G_Loss 1.699323] [D_Loss 0.672460]\n",
      "train [Epoch 0/3] [Batch 1117/1173] [Iter 1117] [G_Loss 1.141146] [D_Loss 0.856180]\n",
      "train [Epoch 0/3] [Batch 1118/1173] [Iter 1118] [G_Loss 2.156642] [D_Loss 1.285715]\n",
      "train [Epoch 0/3] [Batch 1119/1173] [Iter 1119] [G_Loss 0.937273] [D_Loss 1.163056]\n",
      "train [Epoch 0/3] [Batch 1120/1173] [Iter 1120] [G_Loss 1.957293] [D_Loss 0.996185]\n",
      "train [Epoch 0/3] [Batch 1121/1173] [Iter 1121] [G_Loss 1.266355] [D_Loss 0.915388]\n",
      "train [Epoch 0/3] [Batch 1122/1173] [Iter 1122] [G_Loss 0.792198] [D_Loss 1.092264]\n",
      "train [Epoch 0/3] [Batch 1123/1173] [Iter 1123] [G_Loss 2.275995] [D_Loss 1.343346]\n",
      "train [Epoch 0/3] [Batch 1124/1173] [Iter 1124] [G_Loss 0.802081] [D_Loss 1.006578]\n",
      "train [Epoch 0/3] [Batch 1125/1173] [Iter 1125] [G_Loss 0.894071] [D_Loss 0.872128]\n",
      "train [Epoch 0/3] [Batch 1126/1173] [Iter 1126] [G_Loss 1.702603] [D_Loss 0.872568]\n",
      "train [Epoch 0/3] [Batch 1127/1173] [Iter 1127] [G_Loss 0.974754] [D_Loss 0.933903]\n",
      "train [Epoch 0/3] [Batch 1128/1173] [Iter 1128] [G_Loss 1.238519] [D_Loss 0.952230]\n",
      "train [Epoch 0/3] [Batch 1129/1173] [Iter 1129] [G_Loss 1.023064] [D_Loss 0.981214]\n",
      "train [Epoch 0/3] [Batch 1130/1173] [Iter 1130] [G_Loss 1.204327] [D_Loss 1.055736]\n",
      "train [Epoch 0/3] [Batch 1131/1173] [Iter 1131] [G_Loss 0.716534] [D_Loss 1.295499]\n",
      "train [Epoch 0/3] [Batch 1132/1173] [Iter 1132] [G_Loss 1.487244] [D_Loss 1.359726]\n",
      "train [Epoch 0/3] [Batch 1133/1173] [Iter 1133] [G_Loss 0.780333] [D_Loss 1.052403]\n",
      "train [Epoch 0/3] [Batch 1134/1173] [Iter 1134] [G_Loss 2.308181] [D_Loss 0.852354]\n",
      "train [Epoch 0/3] [Batch 1135/1173] [Iter 1135] [G_Loss 1.311537] [D_Loss 0.629693]\n",
      "train [Epoch 0/3] [Batch 1136/1173] [Iter 1136] [G_Loss 1.152031] [D_Loss 0.567059]\n",
      "train [Epoch 0/3] [Batch 1137/1173] [Iter 1137] [G_Loss 1.308677] [D_Loss 0.500878]\n",
      "train [Epoch 0/3] [Batch 1138/1173] [Iter 1138] [G_Loss 1.264664] [D_Loss 0.878895]\n",
      "train [Epoch 0/3] [Batch 1139/1173] [Iter 1139] [G_Loss 1.148642] [D_Loss 1.222316]\n",
      "train [Epoch 0/3] [Batch 1140/1173] [Iter 1140] [G_Loss 3.206059] [D_Loss 1.985221]\n",
      "train [Epoch 0/3] [Batch 1141/1173] [Iter 1141] [G_Loss 0.630161] [D_Loss 1.333247]\n",
      "train [Epoch 0/3] [Batch 1142/1173] [Iter 1142] [G_Loss 1.479496] [D_Loss 0.819747]\n",
      "train [Epoch 0/3] [Batch 1143/1173] [Iter 1143] [G_Loss 1.699065] [D_Loss 0.740786]\n",
      "train [Epoch 0/3] [Batch 1144/1173] [Iter 1144] [G_Loss 1.138287] [D_Loss 1.026231]\n",
      "train [Epoch 0/3] [Batch 1145/1173] [Iter 1145] [G_Loss 1.350601] [D_Loss 1.063412]\n",
      "train [Epoch 0/3] [Batch 1146/1173] [Iter 1146] [G_Loss 0.897811] [D_Loss 1.204432]\n",
      "train [Epoch 0/3] [Batch 1147/1173] [Iter 1147] [G_Loss 0.930265] [D_Loss 1.271222]\n",
      "train [Epoch 0/3] [Batch 1148/1173] [Iter 1148] [G_Loss 1.337478] [D_Loss 0.827672]\n",
      "train [Epoch 0/3] [Batch 1149/1173] [Iter 1149] [G_Loss 3.270561] [D_Loss 0.352306]\n",
      "train [Epoch 0/3] [Batch 1150/1173] [Iter 1150] [G_Loss 1.098736] [D_Loss 1.311309]\n",
      "train [Epoch 0/3] [Batch 1151/1173] [Iter 1151] [G_Loss 1.522500] [D_Loss 1.174062]\n",
      "train [Epoch 0/3] [Batch 1152/1173] [Iter 1152] [G_Loss 2.512999] [D_Loss 0.934802]\n",
      "train [Epoch 0/3] [Batch 1153/1173] [Iter 1153] [G_Loss 1.843751] [D_Loss 0.701480]\n",
      "train [Epoch 0/3] [Batch 1154/1173] [Iter 1154] [G_Loss 0.593185] [D_Loss 1.280630]\n",
      "train [Epoch 0/3] [Batch 1155/1173] [Iter 1155] [G_Loss 2.683095] [D_Loss 1.347432]\n",
      "train [Epoch 0/3] [Batch 1156/1173] [Iter 1156] [G_Loss 1.364363] [D_Loss 0.871447]\n",
      "train [Epoch 0/3] [Batch 1157/1173] [Iter 1157] [G_Loss 0.905871] [D_Loss 1.012362]\n",
      "train [Epoch 0/3] [Batch 1158/1173] [Iter 1158] [G_Loss 1.187244] [D_Loss 0.787022]\n",
      "train [Epoch 0/3] [Batch 1159/1173] [Iter 1159] [G_Loss 1.285565] [D_Loss 0.657189]\n",
      "train [Epoch 0/3] [Batch 1160/1173] [Iter 1160] [G_Loss 1.283593] [D_Loss 0.744203]\n",
      "train [Epoch 0/3] [Batch 1161/1173] [Iter 1161] [G_Loss 1.501857] [D_Loss 0.666914]\n",
      "train [Epoch 0/3] [Batch 1162/1173] [Iter 1162] [G_Loss 1.921228] [D_Loss 0.753302]\n",
      "train [Epoch 0/3] [Batch 1163/1173] [Iter 1163] [G_Loss 0.918195] [D_Loss 0.957055]\n",
      "train [Epoch 0/3] [Batch 1164/1173] [Iter 1164] [G_Loss 1.166537] [D_Loss 0.907456]\n",
      "train [Epoch 0/3] [Batch 1165/1173] [Iter 1165] [G_Loss 1.178450] [D_Loss 0.868801]\n",
      "train [Epoch 0/3] [Batch 1166/1173] [Iter 1166] [G_Loss 1.099574] [D_Loss 0.892015]\n",
      "train [Epoch 0/3] [Batch 1167/1173] [Iter 1167] [G_Loss 1.424536] [D_Loss 0.909692]\n",
      "train [Epoch 0/3] [Batch 1168/1173] [Iter 1168] [G_Loss 1.295033] [D_Loss 0.987205]\n",
      "train [Epoch 0/3] [Batch 1169/1173] [Iter 1169] [G_Loss 1.495586] [D_Loss 1.065623]\n",
      "train [Epoch 0/3] [Batch 1170/1173] [Iter 1170] [G_Loss 1.124111] [D_Loss 1.031114]\n",
      "train [Epoch 0/3] [Batch 1171/1173] [Iter 1171] [G_Loss 1.742633] [D_Loss 1.099394]\n",
      "train [Epoch 0/3] [Batch 1172/1173] [Iter 1172] [G_Loss 1.225526] [D_Loss 0.900754]\n",
      "val [Epoch 0/3] [Batch 0/31] [Iter 0] [G_Loss 1.339462] [D_Loss 1.052707]\n",
      "val [Epoch 0/3] [Batch 1/31] [Iter 1] [G_Loss 1.488439] [D_Loss 0.989500]\n",
      "val [Epoch 0/3] [Batch 2/31] [Iter 2] [G_Loss 1.375487] [D_Loss 1.038994]\n",
      "val [Epoch 0/3] [Batch 3/31] [Iter 3] [G_Loss 1.274134] [D_Loss 0.984114]\n",
      "val [Epoch 0/3] [Batch 4/31] [Iter 4] [G_Loss 1.326038] [D_Loss 1.062379]\n",
      "val [Epoch 0/3] [Batch 5/31] [Iter 5] [G_Loss 1.347188] [D_Loss 0.999230]\n",
      "val [Epoch 0/3] [Batch 6/31] [Iter 6] [G_Loss 1.407664] [D_Loss 0.967791]\n",
      "val [Epoch 0/3] [Batch 7/31] [Iter 7] [G_Loss 1.526020] [D_Loss 1.028737]\n",
      "val [Epoch 0/3] [Batch 8/31] [Iter 8] [G_Loss 1.390096] [D_Loss 0.922728]\n",
      "val [Epoch 0/3] [Batch 9/31] [Iter 9] [G_Loss 1.405514] [D_Loss 0.850458]\n",
      "val [Epoch 0/3] [Batch 10/31] [Iter 10] [G_Loss 1.355904] [D_Loss 0.932158]\n",
      "val [Epoch 0/3] [Batch 11/31] [Iter 11] [G_Loss 1.344051] [D_Loss 1.025605]\n",
      "val [Epoch 0/3] [Batch 12/31] [Iter 12] [G_Loss 1.303816] [D_Loss 0.924238]\n",
      "val [Epoch 0/3] [Batch 13/31] [Iter 13] [G_Loss 1.348622] [D_Loss 1.072226]\n",
      "val [Epoch 0/3] [Batch 14/31] [Iter 14] [G_Loss 1.293029] [D_Loss 0.974495]\n",
      "val [Epoch 0/3] [Batch 15/31] [Iter 15] [G_Loss 1.231114] [D_Loss 0.980269]\n",
      "val [Epoch 0/3] [Batch 16/31] [Iter 16] [G_Loss 1.414138] [D_Loss 0.933856]\n",
      "val [Epoch 0/3] [Batch 17/31] [Iter 17] [G_Loss 1.585837] [D_Loss 1.058348]\n",
      "val [Epoch 0/3] [Batch 18/31] [Iter 18] [G_Loss 1.253016] [D_Loss 1.000300]\n",
      "val [Epoch 0/3] [Batch 19/31] [Iter 19] [G_Loss 1.277875] [D_Loss 0.923918]\n",
      "val [Epoch 0/3] [Batch 20/31] [Iter 20] [G_Loss 1.362782] [D_Loss 0.975816]\n",
      "val [Epoch 0/3] [Batch 21/31] [Iter 21] [G_Loss 1.347536] [D_Loss 1.053335]\n",
      "val [Epoch 0/3] [Batch 22/31] [Iter 22] [G_Loss 1.533429] [D_Loss 0.950282]\n",
      "val [Epoch 0/3] [Batch 23/31] [Iter 23] [G_Loss 1.409652] [D_Loss 1.007355]\n",
      "val [Epoch 0/3] [Batch 24/31] [Iter 24] [G_Loss 1.341449] [D_Loss 1.018952]\n",
      "val [Epoch 0/3] [Batch 25/31] [Iter 25] [G_Loss 1.304779] [D_Loss 1.073251]\n",
      "val [Epoch 0/3] [Batch 26/31] [Iter 26] [G_Loss 1.326188] [D_Loss 0.810604]\n",
      "val [Epoch 0/3] [Batch 27/31] [Iter 27] [G_Loss 1.551854] [D_Loss 1.000969]\n",
      "val [Epoch 0/3] [Batch 28/31] [Iter 28] [G_Loss 1.489406] [D_Loss 1.069423]\n",
      "val [Epoch 0/3] [Batch 29/31] [Iter 29] [G_Loss 1.396655] [D_Loss 0.985664]\n",
      "val [Epoch 0/3] [Batch 30/31] [Iter 30] [G_Loss 1.314765] [D_Loss 0.958052]\n",
      "train [Epoch 1/3] [Batch 0/1173] [Iter 1173] [G_Loss 1.306287] [D_Loss 1.027698]\n",
      "train [Epoch 1/3] [Batch 1/1173] [Iter 1174] [G_Loss 1.127169] [D_Loss 0.960473]\n",
      "train [Epoch 1/3] [Batch 2/1173] [Iter 1175] [G_Loss 1.127079] [D_Loss 0.994105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 1/3] [Batch 3/1173] [Iter 1176] [G_Loss 1.141376] [D_Loss 0.974812]\n",
      "train [Epoch 1/3] [Batch 4/1173] [Iter 1177] [G_Loss 1.004604] [D_Loss 0.931741]\n",
      "train [Epoch 1/3] [Batch 5/1173] [Iter 1178] [G_Loss 1.145715] [D_Loss 0.944066]\n",
      "train [Epoch 1/3] [Batch 6/1173] [Iter 1179] [G_Loss 1.206495] [D_Loss 0.759868]\n",
      "train [Epoch 1/3] [Batch 7/1173] [Iter 1180] [G_Loss 1.153592] [D_Loss 0.738641]\n",
      "train [Epoch 1/3] [Batch 8/1173] [Iter 1181] [G_Loss 1.352145] [D_Loss 0.780535]\n",
      "train [Epoch 1/3] [Batch 9/1173] [Iter 1182] [G_Loss 1.554079] [D_Loss 0.726457]\n",
      "train [Epoch 1/3] [Batch 10/1173] [Iter 1183] [G_Loss 1.106181] [D_Loss 0.775389]\n",
      "train [Epoch 1/3] [Batch 11/1173] [Iter 1184] [G_Loss 2.236646] [D_Loss 0.814147]\n",
      "train [Epoch 1/3] [Batch 12/1173] [Iter 1185] [G_Loss 1.540255] [D_Loss 0.617431]\n",
      "train [Epoch 1/3] [Batch 13/1173] [Iter 1186] [G_Loss 1.925873] [D_Loss 0.617785]\n",
      "train [Epoch 1/3] [Batch 14/1173] [Iter 1187] [G_Loss 1.081514] [D_Loss 0.732524]\n",
      "train [Epoch 1/3] [Batch 15/1173] [Iter 1188] [G_Loss 2.165253] [D_Loss 1.408990]\n",
      "train [Epoch 1/3] [Batch 16/1173] [Iter 1189] [G_Loss 0.129489] [D_Loss 3.034100]\n",
      "train [Epoch 1/3] [Batch 17/1173] [Iter 1190] [G_Loss 0.856789] [D_Loss 0.777710]\n",
      "train [Epoch 1/3] [Batch 18/1173] [Iter 1191] [G_Loss 1.204614] [D_Loss 0.677452]\n",
      "train [Epoch 1/3] [Batch 19/1173] [Iter 1192] [G_Loss 0.852071] [D_Loss 0.796745]\n",
      "train [Epoch 1/3] [Batch 20/1173] [Iter 1193] [G_Loss 0.801154] [D_Loss 1.001874]\n",
      "train [Epoch 1/3] [Batch 21/1173] [Iter 1194] [G_Loss 1.240071] [D_Loss 1.148588]\n",
      "train [Epoch 1/3] [Batch 22/1173] [Iter 1195] [G_Loss 1.014066] [D_Loss 1.401498]\n",
      "train [Epoch 1/3] [Batch 23/1173] [Iter 1196] [G_Loss 0.969109] [D_Loss 1.463672]\n",
      "train [Epoch 1/3] [Batch 24/1173] [Iter 1197] [G_Loss 1.498155] [D_Loss 1.448123]\n",
      "train [Epoch 1/3] [Batch 25/1173] [Iter 1198] [G_Loss 0.561989] [D_Loss 1.447321]\n",
      "train [Epoch 1/3] [Batch 26/1173] [Iter 1199] [G_Loss 2.780583] [D_Loss 1.512507]\n",
      "train [Epoch 1/3] [Batch 27/1173] [Iter 1200] [G_Loss 0.499513] [D_Loss 1.640301]\n",
      "train [Epoch 1/3] [Batch 28/1173] [Iter 1201] [G_Loss 1.741350] [D_Loss 1.330596]\n",
      "train [Epoch 1/3] [Batch 29/1173] [Iter 1202] [G_Loss 1.070033] [D_Loss 1.378896]\n",
      "train [Epoch 1/3] [Batch 30/1173] [Iter 1203] [G_Loss 0.780989] [D_Loss 1.302403]\n",
      "train [Epoch 1/3] [Batch 31/1173] [Iter 1204] [G_Loss 0.962127] [D_Loss 1.157719]\n",
      "train [Epoch 1/3] [Batch 32/1173] [Iter 1205] [G_Loss 1.391435] [D_Loss 1.031737]\n",
      "train [Epoch 1/3] [Batch 33/1173] [Iter 1206] [G_Loss 1.401892] [D_Loss 1.028194]\n",
      "train [Epoch 1/3] [Batch 34/1173] [Iter 1207] [G_Loss 0.704366] [D_Loss 1.184302]\n",
      "train [Epoch 1/3] [Batch 35/1173] [Iter 1208] [G_Loss 0.635254] [D_Loss 1.154544]\n",
      "train [Epoch 1/3] [Batch 36/1173] [Iter 1209] [G_Loss 0.668500] [D_Loss 1.205837]\n",
      "train [Epoch 1/3] [Batch 37/1173] [Iter 1210] [G_Loss 0.722763] [D_Loss 1.119903]\n",
      "train [Epoch 1/3] [Batch 38/1173] [Iter 1211] [G_Loss 0.961163] [D_Loss 0.999303]\n",
      "train [Epoch 1/3] [Batch 39/1173] [Iter 1212] [G_Loss 1.248331] [D_Loss 0.937923]\n",
      "train [Epoch 1/3] [Batch 40/1173] [Iter 1213] [G_Loss 1.104428] [D_Loss 1.039978]\n",
      "train [Epoch 1/3] [Batch 41/1173] [Iter 1214] [G_Loss 0.839753] [D_Loss 1.175324]\n",
      "train [Epoch 1/3] [Batch 42/1173] [Iter 1215] [G_Loss 1.456419] [D_Loss 1.157090]\n",
      "train [Epoch 1/3] [Batch 43/1173] [Iter 1216] [G_Loss 1.586158] [D_Loss 1.041927]\n",
      "train [Epoch 1/3] [Batch 44/1173] [Iter 1217] [G_Loss 1.113036] [D_Loss 1.001564]\n",
      "train [Epoch 1/3] [Batch 45/1173] [Iter 1218] [G_Loss 1.294012] [D_Loss 0.957515]\n",
      "train [Epoch 1/3] [Batch 46/1173] [Iter 1219] [G_Loss 1.554518] [D_Loss 0.893994]\n",
      "train [Epoch 1/3] [Batch 47/1173] [Iter 1220] [G_Loss 1.312100] [D_Loss 1.090036]\n",
      "train [Epoch 1/3] [Batch 48/1173] [Iter 1221] [G_Loss 0.981065] [D_Loss 1.148639]\n",
      "train [Epoch 1/3] [Batch 49/1173] [Iter 1222] [G_Loss 0.955866] [D_Loss 0.986738]\n",
      "train [Epoch 1/3] [Batch 50/1173] [Iter 1223] [G_Loss 0.987837] [D_Loss 0.874352]\n",
      "train [Epoch 1/3] [Batch 51/1173] [Iter 1224] [G_Loss 1.231009] [D_Loss 0.712342]\n",
      "train [Epoch 1/3] [Batch 52/1173] [Iter 1225] [G_Loss 1.514317] [D_Loss 0.558404]\n",
      "train [Epoch 1/3] [Batch 53/1173] [Iter 1226] [G_Loss 0.911127] [D_Loss 1.010663]\n",
      "train [Epoch 1/3] [Batch 54/1173] [Iter 1227] [G_Loss 0.762326] [D_Loss 1.643692]\n",
      "train [Epoch 1/3] [Batch 55/1173] [Iter 1228] [G_Loss 0.638111] [D_Loss 1.852762]\n",
      "train [Epoch 1/3] [Batch 56/1173] [Iter 1229] [G_Loss 1.081489] [D_Loss 1.254585]\n",
      "train [Epoch 1/3] [Batch 57/1173] [Iter 1230] [G_Loss 1.414664] [D_Loss 1.045440]\n",
      "train [Epoch 1/3] [Batch 58/1173] [Iter 1231] [G_Loss 1.485423] [D_Loss 0.749312]\n",
      "train [Epoch 1/3] [Batch 59/1173] [Iter 1232] [G_Loss 2.262160] [D_Loss 0.549608]\n",
      "train [Epoch 1/3] [Batch 60/1173] [Iter 1233] [G_Loss 1.741375] [D_Loss 0.598096]\n",
      "train [Epoch 1/3] [Batch 61/1173] [Iter 1234] [G_Loss 1.512257] [D_Loss 0.766783]\n",
      "train [Epoch 1/3] [Batch 62/1173] [Iter 1235] [G_Loss 1.075264] [D_Loss 1.117530]\n",
      "train [Epoch 1/3] [Batch 63/1173] [Iter 1236] [G_Loss 0.921448] [D_Loss 1.094612]\n",
      "train [Epoch 1/3] [Batch 64/1173] [Iter 1237] [G_Loss 0.737693] [D_Loss 1.255083]\n",
      "train [Epoch 1/3] [Batch 65/1173] [Iter 1238] [G_Loss 0.660403] [D_Loss 1.154654]\n",
      "train [Epoch 1/3] [Batch 66/1173] [Iter 1239] [G_Loss 0.747916] [D_Loss 0.877579]\n",
      "train [Epoch 1/3] [Batch 67/1173] [Iter 1240] [G_Loss 0.898187] [D_Loss 0.689576]\n",
      "train [Epoch 1/3] [Batch 68/1173] [Iter 1241] [G_Loss 1.135792] [D_Loss 0.575566]\n",
      "train [Epoch 1/3] [Batch 69/1173] [Iter 1242] [G_Loss 1.447095] [D_Loss 0.538558]\n",
      "train [Epoch 1/3] [Batch 70/1173] [Iter 1243] [G_Loss 1.902305] [D_Loss 0.527380]\n",
      "train [Epoch 1/3] [Batch 71/1173] [Iter 1244] [G_Loss 1.582302] [D_Loss 0.832111]\n",
      "train [Epoch 1/3] [Batch 72/1173] [Iter 1245] [G_Loss 1.026131] [D_Loss 1.252527]\n",
      "train [Epoch 1/3] [Batch 73/1173] [Iter 1246] [G_Loss 0.724157] [D_Loss 1.307373]\n",
      "train [Epoch 1/3] [Batch 74/1173] [Iter 1247] [G_Loss 1.634630] [D_Loss 0.933066]\n",
      "train [Epoch 1/3] [Batch 75/1173] [Iter 1248] [G_Loss 1.655889] [D_Loss 0.794925]\n",
      "train [Epoch 1/3] [Batch 76/1173] [Iter 1249] [G_Loss 0.951890] [D_Loss 0.892395]\n",
      "train [Epoch 1/3] [Batch 77/1173] [Iter 1250] [G_Loss 2.798155] [D_Loss 0.995389]\n",
      "train [Epoch 1/3] [Batch 78/1173] [Iter 1251] [G_Loss 1.374002] [D_Loss 0.677354]\n",
      "train [Epoch 1/3] [Batch 79/1173] [Iter 1252] [G_Loss 1.314641] [D_Loss 0.674942]\n",
      "train [Epoch 1/3] [Batch 80/1173] [Iter 1253] [G_Loss 2.207444] [D_Loss 0.880060]\n",
      "train [Epoch 1/3] [Batch 81/1173] [Iter 1254] [G_Loss 0.963807] [D_Loss 0.803563]\n",
      "train [Epoch 1/3] [Batch 82/1173] [Iter 1255] [G_Loss 1.018552] [D_Loss 0.933528]\n",
      "train [Epoch 1/3] [Batch 83/1173] [Iter 1256] [G_Loss 1.104982] [D_Loss 1.022162]\n",
      "train [Epoch 1/3] [Batch 84/1173] [Iter 1257] [G_Loss 0.973140] [D_Loss 0.977203]\n",
      "train [Epoch 1/3] [Batch 85/1173] [Iter 1258] [G_Loss 0.925418] [D_Loss 0.846119]\n",
      "train [Epoch 1/3] [Batch 86/1173] [Iter 1259] [G_Loss 0.802136] [D_Loss 0.794981]\n",
      "train [Epoch 1/3] [Batch 87/1173] [Iter 1260] [G_Loss 0.859019] [D_Loss 0.834864]\n",
      "train [Epoch 1/3] [Batch 88/1173] [Iter 1261] [G_Loss 1.144966] [D_Loss 0.806660]\n",
      "train [Epoch 1/3] [Batch 89/1173] [Iter 1262] [G_Loss 1.180903] [D_Loss 0.768598]\n",
      "train [Epoch 1/3] [Batch 90/1173] [Iter 1263] [G_Loss 1.194298] [D_Loss 0.762389]\n",
      "train [Epoch 1/3] [Batch 91/1173] [Iter 1264] [G_Loss 1.274529] [D_Loss 0.763602]\n",
      "train [Epoch 1/3] [Batch 92/1173] [Iter 1265] [G_Loss 1.164721] [D_Loss 0.842883]\n",
      "train [Epoch 1/3] [Batch 93/1173] [Iter 1266] [G_Loss 2.289135] [D_Loss 1.023123]\n",
      "train [Epoch 1/3] [Batch 94/1173] [Iter 1267] [G_Loss 0.759600] [D_Loss 1.042533]\n",
      "train [Epoch 1/3] [Batch 95/1173] [Iter 1268] [G_Loss 2.710345] [D_Loss 1.371652]\n",
      "train [Epoch 1/3] [Batch 96/1173] [Iter 1269] [G_Loss 1.143596] [D_Loss 0.854465]\n",
      "train [Epoch 1/3] [Batch 97/1173] [Iter 1270] [G_Loss 0.626582] [D_Loss 1.134858]\n",
      "train [Epoch 1/3] [Batch 98/1173] [Iter 1271] [G_Loss 1.897975] [D_Loss 0.933881]\n",
      "train [Epoch 1/3] [Batch 99/1173] [Iter 1272] [G_Loss 1.566133] [D_Loss 0.951399]\n",
      "train [Epoch 1/3] [Batch 100/1173] [Iter 1273] [G_Loss 0.815165] [D_Loss 0.917480]\n",
      "train [Epoch 1/3] [Batch 101/1173] [Iter 1274] [G_Loss 1.203814] [D_Loss 0.805929]\n",
      "train [Epoch 1/3] [Batch 102/1173] [Iter 1275] [G_Loss 1.471626] [D_Loss 0.772983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 1/3] [Batch 103/1173] [Iter 1276] [G_Loss 1.696830] [D_Loss 0.788022]\n",
      "train [Epoch 1/3] [Batch 104/1173] [Iter 1277] [G_Loss 1.216717] [D_Loss 0.849751]\n",
      "train [Epoch 1/3] [Batch 105/1173] [Iter 1278] [G_Loss 1.467762] [D_Loss 0.798004]\n",
      "train [Epoch 1/3] [Batch 106/1173] [Iter 1279] [G_Loss 1.065578] [D_Loss 0.933720]\n",
      "train [Epoch 1/3] [Batch 107/1173] [Iter 1280] [G_Loss 1.325557] [D_Loss 0.931595]\n",
      "train [Epoch 1/3] [Batch 108/1173] [Iter 1281] [G_Loss 1.133980] [D_Loss 0.879011]\n",
      "train [Epoch 1/3] [Batch 109/1173] [Iter 1282] [G_Loss 1.556634] [D_Loss 0.774093]\n",
      "train [Epoch 1/3] [Batch 110/1173] [Iter 1283] [G_Loss 1.810306] [D_Loss 0.650753]\n",
      "train [Epoch 1/3] [Batch 111/1173] [Iter 1284] [G_Loss 1.486558] [D_Loss 0.581337]\n",
      "train [Epoch 1/3] [Batch 112/1173] [Iter 1285] [G_Loss 3.510173] [D_Loss 0.803900]\n",
      "train [Epoch 1/3] [Batch 113/1173] [Iter 1286] [G_Loss 0.186549] [D_Loss 2.833260]\n",
      "train [Epoch 1/3] [Batch 114/1173] [Iter 1287] [G_Loss 2.744639] [D_Loss 0.953546]\n",
      "train [Epoch 1/3] [Batch 115/1173] [Iter 1288] [G_Loss 0.506869] [D_Loss 1.874143]\n",
      "train [Epoch 1/3] [Batch 116/1173] [Iter 1289] [G_Loss 7.248938] [D_Loss 2.467585]\n",
      "train [Epoch 1/3] [Batch 117/1173] [Iter 1290] [G_Loss 1.341659] [D_Loss 1.706178]\n",
      "train [Epoch 1/3] [Batch 118/1173] [Iter 1291] [G_Loss 0.040839] [D_Loss 4.423317]\n",
      "train [Epoch 1/3] [Batch 119/1173] [Iter 1292] [G_Loss 3.497154] [D_Loss 2.280030]\n",
      "train [Epoch 1/3] [Batch 120/1173] [Iter 1293] [G_Loss 2.676641] [D_Loss 2.039150]\n",
      "train [Epoch 1/3] [Batch 121/1173] [Iter 1294] [G_Loss 1.042331] [D_Loss 1.296361]\n",
      "train [Epoch 1/3] [Batch 122/1173] [Iter 1295] [G_Loss 1.107769] [D_Loss 0.977298]\n",
      "train [Epoch 1/3] [Batch 123/1173] [Iter 1296] [G_Loss 1.999549] [D_Loss 0.661605]\n",
      "train [Epoch 1/3] [Batch 124/1173] [Iter 1297] [G_Loss 1.995318] [D_Loss 0.720045]\n",
      "train [Epoch 1/3] [Batch 125/1173] [Iter 1298] [G_Loss 1.074831] [D_Loss 0.849217]\n",
      "train [Epoch 1/3] [Batch 126/1173] [Iter 1299] [G_Loss 2.042858] [D_Loss 0.488027]\n",
      "train [Epoch 1/3] [Batch 127/1173] [Iter 1300] [G_Loss 3.355364] [D_Loss 0.229933]\n",
      "train [Epoch 1/3] [Batch 128/1173] [Iter 1301] [G_Loss 1.653567] [D_Loss 0.724996]\n",
      "train [Epoch 1/3] [Batch 129/1173] [Iter 1302] [G_Loss 4.104152] [D_Loss 2.046343]\n",
      "train [Epoch 1/3] [Batch 130/1173] [Iter 1303] [G_Loss 1.566271] [D_Loss 0.554124]\n",
      "train [Epoch 1/3] [Batch 131/1173] [Iter 1304] [G_Loss 0.614267] [D_Loss 1.206269]\n",
      "train [Epoch 1/3] [Batch 132/1173] [Iter 1305] [G_Loss 2.605305] [D_Loss 1.348554]\n",
      "train [Epoch 1/3] [Batch 133/1173] [Iter 1306] [G_Loss 0.434315] [D_Loss 1.920218]\n",
      "train [Epoch 1/3] [Batch 134/1173] [Iter 1307] [G_Loss 1.193156] [D_Loss 1.258603]\n",
      "train [Epoch 1/3] [Batch 135/1173] [Iter 1308] [G_Loss 3.392153] [D_Loss 1.095803]\n",
      "train [Epoch 1/3] [Batch 136/1173] [Iter 1309] [G_Loss 1.347643] [D_Loss 1.452205]\n",
      "train [Epoch 1/3] [Batch 137/1173] [Iter 1310] [G_Loss 0.912547] [D_Loss 1.002192]\n",
      "train [Epoch 1/3] [Batch 138/1173] [Iter 1311] [G_Loss 1.001330] [D_Loss 0.862406]\n",
      "train [Epoch 1/3] [Batch 139/1173] [Iter 1312] [G_Loss 1.405759] [D_Loss 0.679368]\n",
      "train [Epoch 1/3] [Batch 140/1173] [Iter 1313] [G_Loss 1.418162] [D_Loss 0.648652]\n",
      "train [Epoch 1/3] [Batch 141/1173] [Iter 1314] [G_Loss 1.268664] [D_Loss 0.553992]\n",
      "train [Epoch 1/3] [Batch 142/1173] [Iter 1315] [G_Loss 1.555502] [D_Loss 0.607574]\n",
      "train [Epoch 1/3] [Batch 143/1173] [Iter 1316] [G_Loss 1.528963] [D_Loss 0.588417]\n",
      "train [Epoch 1/3] [Batch 144/1173] [Iter 1317] [G_Loss 1.272081] [D_Loss 0.661867]\n",
      "train [Epoch 1/3] [Batch 145/1173] [Iter 1318] [G_Loss 1.185490] [D_Loss 0.769328]\n",
      "train [Epoch 1/3] [Batch 146/1173] [Iter 1319] [G_Loss 1.484938] [D_Loss 0.814296]\n",
      "train [Epoch 1/3] [Batch 147/1173] [Iter 1320] [G_Loss 1.494220] [D_Loss 0.933237]\n",
      "train [Epoch 1/3] [Batch 148/1173] [Iter 1321] [G_Loss 1.316184] [D_Loss 1.223115]\n",
      "train [Epoch 1/3] [Batch 149/1173] [Iter 1322] [G_Loss 0.988462] [D_Loss 1.482792]\n",
      "train [Epoch 1/3] [Batch 150/1173] [Iter 1323] [G_Loss 1.062196] [D_Loss 1.296489]\n",
      "train [Epoch 1/3] [Batch 151/1173] [Iter 1324] [G_Loss 1.224042] [D_Loss 1.127882]\n",
      "train [Epoch 1/3] [Batch 152/1173] [Iter 1325] [G_Loss 1.424367] [D_Loss 1.170169]\n",
      "train [Epoch 1/3] [Batch 153/1173] [Iter 1326] [G_Loss 1.566553] [D_Loss 1.099893]\n",
      "train [Epoch 1/3] [Batch 154/1173] [Iter 1327] [G_Loss 1.489769] [D_Loss 0.991024]\n",
      "train [Epoch 1/3] [Batch 155/1173] [Iter 1328] [G_Loss 1.292608] [D_Loss 1.051904]\n",
      "train [Epoch 1/3] [Batch 156/1173] [Iter 1329] [G_Loss 1.426059] [D_Loss 1.020536]\n",
      "train [Epoch 1/3] [Batch 157/1173] [Iter 1330] [G_Loss 1.232914] [D_Loss 1.092922]\n",
      "train [Epoch 1/3] [Batch 158/1173] [Iter 1331] [G_Loss 1.207920] [D_Loss 1.110775]\n",
      "train [Epoch 1/3] [Batch 159/1173] [Iter 1332] [G_Loss 1.078543] [D_Loss 1.059348]\n",
      "train [Epoch 1/3] [Batch 160/1173] [Iter 1333] [G_Loss 1.379144] [D_Loss 1.000151]\n",
      "train [Epoch 1/3] [Batch 161/1173] [Iter 1334] [G_Loss 1.327122] [D_Loss 0.929130]\n",
      "train [Epoch 1/3] [Batch 162/1173] [Iter 1335] [G_Loss 1.368635] [D_Loss 0.914005]\n",
      "train [Epoch 1/3] [Batch 163/1173] [Iter 1336] [G_Loss 1.383006] [D_Loss 0.694492]\n",
      "train [Epoch 1/3] [Batch 164/1173] [Iter 1337] [G_Loss 1.381430] [D_Loss 0.607574]\n",
      "train [Epoch 1/3] [Batch 165/1173] [Iter 1338] [G_Loss 1.326446] [D_Loss 0.651818]\n",
      "train [Epoch 1/3] [Batch 166/1173] [Iter 1339] [G_Loss 1.386521] [D_Loss 0.841442]\n",
      "train [Epoch 1/3] [Batch 167/1173] [Iter 1340] [G_Loss 1.485293] [D_Loss 0.815503]\n",
      "train [Epoch 1/3] [Batch 168/1173] [Iter 1341] [G_Loss 1.325186] [D_Loss 0.915947]\n",
      "train [Epoch 1/3] [Batch 169/1173] [Iter 1342] [G_Loss 1.286880] [D_Loss 1.014014]\n",
      "train [Epoch 1/3] [Batch 170/1173] [Iter 1343] [G_Loss 1.247517] [D_Loss 0.883747]\n",
      "train [Epoch 1/3] [Batch 171/1173] [Iter 1344] [G_Loss 1.559936] [D_Loss 0.859475]\n",
      "train [Epoch 1/3] [Batch 172/1173] [Iter 1345] [G_Loss 1.547628] [D_Loss 0.855205]\n",
      "train [Epoch 1/3] [Batch 173/1173] [Iter 1346] [G_Loss 1.487148] [D_Loss 0.802906]\n",
      "train [Epoch 1/3] [Batch 174/1173] [Iter 1347] [G_Loss 1.199905] [D_Loss 0.741142]\n",
      "train [Epoch 1/3] [Batch 175/1173] [Iter 1348] [G_Loss 1.758187] [D_Loss 0.696567]\n",
      "train [Epoch 1/3] [Batch 176/1173] [Iter 1349] [G_Loss 1.469888] [D_Loss 0.623154]\n",
      "train [Epoch 1/3] [Batch 177/1173] [Iter 1350] [G_Loss 1.389488] [D_Loss 0.601684]\n",
      "train [Epoch 1/3] [Batch 178/1173] [Iter 1351] [G_Loss 1.677355] [D_Loss 0.718865]\n",
      "train [Epoch 1/3] [Batch 179/1173] [Iter 1352] [G_Loss 1.479390] [D_Loss 0.493309]\n",
      "train [Epoch 1/3] [Batch 180/1173] [Iter 1353] [G_Loss 1.717447] [D_Loss 0.476994]\n",
      "train [Epoch 1/3] [Batch 181/1173] [Iter 1354] [G_Loss 1.766331] [D_Loss 0.556603]\n",
      "train [Epoch 1/3] [Batch 182/1173] [Iter 1355] [G_Loss 1.533833] [D_Loss 0.479463]\n",
      "train [Epoch 1/3] [Batch 183/1173] [Iter 1356] [G_Loss 1.814123] [D_Loss 0.392512]\n",
      "train [Epoch 1/3] [Batch 184/1173] [Iter 1357] [G_Loss 1.847419] [D_Loss 0.436225]\n",
      "train [Epoch 1/3] [Batch 185/1173] [Iter 1358] [G_Loss 1.960393] [D_Loss 0.387784]\n",
      "train [Epoch 1/3] [Batch 186/1173] [Iter 1359] [G_Loss 1.811862] [D_Loss 0.319397]\n",
      "train [Epoch 1/3] [Batch 187/1173] [Iter 1360] [G_Loss 1.853201] [D_Loss 0.304377]\n",
      "train [Epoch 1/3] [Batch 188/1173] [Iter 1361] [G_Loss 2.169833] [D_Loss 0.270842]\n",
      "train [Epoch 1/3] [Batch 189/1173] [Iter 1362] [G_Loss 1.864325] [D_Loss 0.367423]\n",
      "train [Epoch 1/3] [Batch 190/1173] [Iter 1363] [G_Loss 0.587556] [D_Loss 2.184034]\n",
      "train [Epoch 1/3] [Batch 191/1173] [Iter 1364] [G_Loss 0.220483] [D_Loss 2.542384]\n",
      "train [Epoch 1/3] [Batch 192/1173] [Iter 1365] [G_Loss 3.053182] [D_Loss 2.979835]\n",
      "train [Epoch 1/3] [Batch 193/1173] [Iter 1366] [G_Loss 1.532323] [D_Loss 1.586206]\n",
      "train [Epoch 1/3] [Batch 194/1173] [Iter 1367] [G_Loss 0.244905] [D_Loss 2.007134]\n",
      "train [Epoch 1/3] [Batch 195/1173] [Iter 1368] [G_Loss 1.106981] [D_Loss 1.363428]\n",
      "train [Epoch 1/3] [Batch 196/1173] [Iter 1369] [G_Loss 1.248539] [D_Loss 1.405952]\n",
      "train [Epoch 1/3] [Batch 197/1173] [Iter 1370] [G_Loss 0.859441] [D_Loss 1.270031]\n",
      "train [Epoch 1/3] [Batch 198/1173] [Iter 1371] [G_Loss 0.646983] [D_Loss 1.250730]\n",
      "train [Epoch 1/3] [Batch 199/1173] [Iter 1372] [G_Loss 0.904281] [D_Loss 1.138074]\n",
      "train [Epoch 1/3] [Batch 200/1173] [Iter 1373] [G_Loss 1.044641] [D_Loss 1.161034]\n",
      "train [Epoch 1/3] [Batch 201/1173] [Iter 1374] [G_Loss 0.870723] [D_Loss 1.093529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 1/3] [Batch 202/1173] [Iter 1375] [G_Loss 0.861702] [D_Loss 1.008024]\n",
      "train [Epoch 1/3] [Batch 203/1173] [Iter 1376] [G_Loss 1.254926] [D_Loss 0.827750]\n",
      "train [Epoch 1/3] [Batch 204/1173] [Iter 1377] [G_Loss 1.560489] [D_Loss 0.515031]\n",
      "train [Epoch 1/3] [Batch 205/1173] [Iter 1378] [G_Loss 2.122089] [D_Loss 0.330750]\n",
      "train [Epoch 1/3] [Batch 206/1173] [Iter 1379] [G_Loss 2.278960] [D_Loss 0.170238]\n",
      "train [Epoch 1/3] [Batch 207/1173] [Iter 1380] [G_Loss 2.187581] [D_Loss 0.319444]\n",
      "train [Epoch 1/3] [Batch 208/1173] [Iter 1381] [G_Loss 2.277297] [D_Loss 0.513053]\n",
      "train [Epoch 1/3] [Batch 209/1173] [Iter 1382] [G_Loss 0.067752] [D_Loss 4.287023]\n",
      "train [Epoch 1/3] [Batch 210/1173] [Iter 1383] [G_Loss 5.342066] [D_Loss 4.313495]\n",
      "train [Epoch 1/3] [Batch 211/1173] [Iter 1384] [G_Loss 2.313807] [D_Loss 2.114814]\n",
      "train [Epoch 1/3] [Batch 212/1173] [Iter 1385] [G_Loss 0.648692] [D_Loss 1.184313]\n",
      "train [Epoch 1/3] [Batch 213/1173] [Iter 1386] [G_Loss 0.508665] [D_Loss 1.248957]\n",
      "train [Epoch 1/3] [Batch 214/1173] [Iter 1387] [G_Loss 0.728837] [D_Loss 0.982162]\n",
      "train [Epoch 1/3] [Batch 215/1173] [Iter 1388] [G_Loss 1.189513] [D_Loss 0.941660]\n",
      "train [Epoch 1/3] [Batch 216/1173] [Iter 1389] [G_Loss 1.224035] [D_Loss 0.894554]\n",
      "train [Epoch 1/3] [Batch 217/1173] [Iter 1390] [G_Loss 1.122989] [D_Loss 0.813838]\n",
      "train [Epoch 1/3] [Batch 218/1173] [Iter 1391] [G_Loss 1.127911] [D_Loss 0.857264]\n",
      "train [Epoch 1/3] [Batch 219/1173] [Iter 1392] [G_Loss 1.125494] [D_Loss 0.800580]\n",
      "train [Epoch 1/3] [Batch 220/1173] [Iter 1393] [G_Loss 1.371631] [D_Loss 0.712907]\n",
      "train [Epoch 1/3] [Batch 221/1173] [Iter 1394] [G_Loss 1.457802] [D_Loss 0.692801]\n",
      "train [Epoch 1/3] [Batch 222/1173] [Iter 1395] [G_Loss 1.464877] [D_Loss 0.897992]\n",
      "train [Epoch 1/3] [Batch 223/1173] [Iter 1396] [G_Loss 1.349462] [D_Loss 0.944978]\n",
      "train [Epoch 1/3] [Batch 224/1173] [Iter 1397] [G_Loss 1.421776] [D_Loss 0.760728]\n",
      "train [Epoch 1/3] [Batch 225/1173] [Iter 1398] [G_Loss 2.143198] [D_Loss 0.746715]\n",
      "train [Epoch 1/3] [Batch 226/1173] [Iter 1399] [G_Loss 1.704038] [D_Loss 0.749903]\n",
      "train [Epoch 1/3] [Batch 227/1173] [Iter 1400] [G_Loss 1.444083] [D_Loss 0.881364]\n",
      "train [Epoch 1/3] [Batch 228/1173] [Iter 1401] [G_Loss 1.521629] [D_Loss 0.998509]\n",
      "train [Epoch 1/3] [Batch 229/1173] [Iter 1402] [G_Loss 1.466366] [D_Loss 0.882595]\n",
      "train [Epoch 1/3] [Batch 230/1173] [Iter 1403] [G_Loss 4.489732] [D_Loss 0.809886]\n",
      "train [Epoch 1/3] [Batch 231/1173] [Iter 1404] [G_Loss 0.193388] [D_Loss 1.905489]\n",
      "train [Epoch 1/3] [Batch 232/1173] [Iter 1405] [G_Loss 0.856408] [D_Loss 0.662417]\n",
      "train [Epoch 1/3] [Batch 233/1173] [Iter 1406] [G_Loss 2.515490] [D_Loss 0.316370]\n",
      "train [Epoch 1/3] [Batch 234/1173] [Iter 1407] [G_Loss 3.397605] [D_Loss 0.356758]\n",
      "train [Epoch 1/3] [Batch 235/1173] [Iter 1408] [G_Loss 1.280198] [D_Loss 0.721588]\n",
      "train [Epoch 1/3] [Batch 236/1173] [Iter 1409] [G_Loss 1.093818] [D_Loss 1.084296]\n",
      "train [Epoch 1/3] [Batch 237/1173] [Iter 1410] [G_Loss 2.036197] [D_Loss 1.298762]\n",
      "train [Epoch 1/3] [Batch 238/1173] [Iter 1411] [G_Loss 1.237077] [D_Loss 0.998722]\n",
      "train [Epoch 1/3] [Batch 239/1173] [Iter 1412] [G_Loss 1.459453] [D_Loss 0.787492]\n",
      "train [Epoch 1/3] [Batch 240/1173] [Iter 1413] [G_Loss 2.125003] [D_Loss 0.949727]\n",
      "train [Epoch 1/3] [Batch 241/1173] [Iter 1414] [G_Loss 0.521116] [D_Loss 1.742924]\n",
      "train [Epoch 1/3] [Batch 242/1173] [Iter 1415] [G_Loss 3.734591] [D_Loss 2.165426]\n",
      "train [Epoch 1/3] [Batch 243/1173] [Iter 1416] [G_Loss 4.745790] [D_Loss 2.066348]\n",
      "train [Epoch 1/3] [Batch 244/1173] [Iter 1417] [G_Loss 5.207142] [D_Loss 0.691915]\n",
      "train [Epoch 1/3] [Batch 245/1173] [Iter 1418] [G_Loss 6.805257] [D_Loss 0.186070]\n",
      "train [Epoch 1/3] [Batch 246/1173] [Iter 1419] [G_Loss 7.257632] [D_Loss 0.109493]\n",
      "train [Epoch 1/3] [Batch 247/1173] [Iter 1420] [G_Loss 7.008147] [D_Loss 0.055357]\n",
      "train [Epoch 1/3] [Batch 248/1173] [Iter 1421] [G_Loss 6.949640] [D_Loss 0.033126]\n",
      "train [Epoch 1/3] [Batch 249/1173] [Iter 1422] [G_Loss 6.878747] [D_Loss 0.027697]\n",
      "train [Epoch 1/3] [Batch 250/1173] [Iter 1423] [G_Loss 6.774563] [D_Loss 0.115903]\n",
      "train [Epoch 1/3] [Batch 251/1173] [Iter 1424] [G_Loss 5.628291] [D_Loss 0.031580]\n",
      "train [Epoch 1/3] [Batch 252/1173] [Iter 1425] [G_Loss 5.326014] [D_Loss 0.034496]\n",
      "train [Epoch 1/3] [Batch 253/1173] [Iter 1426] [G_Loss 5.513071] [D_Loss 0.025466]\n",
      "train [Epoch 1/3] [Batch 254/1173] [Iter 1427] [G_Loss 5.952003] [D_Loss 0.019444]\n",
      "train [Epoch 1/3] [Batch 255/1173] [Iter 1428] [G_Loss 6.395401] [D_Loss 0.024780]\n",
      "train [Epoch 1/3] [Batch 256/1173] [Iter 1429] [G_Loss 6.642375] [D_Loss 0.019647]\n",
      "train [Epoch 1/3] [Batch 257/1173] [Iter 1430] [G_Loss 6.755435] [D_Loss 0.015418]\n",
      "train [Epoch 1/3] [Batch 258/1173] [Iter 1431] [G_Loss 6.909980] [D_Loss 0.057846]\n",
      "train [Epoch 1/3] [Batch 259/1173] [Iter 1432] [G_Loss 6.641698] [D_Loss 0.015059]\n",
      "train [Epoch 1/3] [Batch 260/1173] [Iter 1433] [G_Loss 6.601829] [D_Loss 0.019163]\n",
      "train [Epoch 1/3] [Batch 261/1173] [Iter 1434] [G_Loss 6.683871] [D_Loss 0.034588]\n",
      "train [Epoch 1/3] [Batch 265/1173] [Iter 1438] [G_Loss 6.925197] [D_Loss 0.012437]\n",
      "train [Epoch 1/3] [Batch 266/1173] [Iter 1439] [G_Loss 7.050417] [D_Loss 0.046893]\n",
      "train [Epoch 1/3] [Batch 267/1173] [Iter 1440] [G_Loss 6.503486] [D_Loss 0.025003]\n",
      "train [Epoch 1/3] [Batch 268/1173] [Iter 1441] [G_Loss 6.342998] [D_Loss 0.021713]\n",
      "train [Epoch 1/3] [Batch 269/1173] [Iter 1442] [G_Loss 6.213501] [D_Loss 0.016102]\n",
      "train [Epoch 1/3] [Batch 270/1173] [Iter 1443] [G_Loss 6.284472] [D_Loss 0.006446]\n",
      "train [Epoch 1/3] [Batch 271/1173] [Iter 1444] [G_Loss 6.480321] [D_Loss 0.008199]\n",
      "train [Epoch 1/3] [Batch 272/1173] [Iter 1445] [G_Loss 6.717222] [D_Loss 0.011533]\n",
      "train [Epoch 1/3] [Batch 273/1173] [Iter 1446] [G_Loss 6.930899] [D_Loss 0.026102]\n",
      "train [Epoch 1/3] [Batch 274/1173] [Iter 1447] [G_Loss 6.769421] [D_Loss 0.015129]\n",
      "train [Epoch 1/3] [Batch 275/1173] [Iter 1448] [G_Loss 6.552470] [D_Loss 0.008315]\n",
      "train [Epoch 1/3] [Batch 276/1173] [Iter 1449] [G_Loss 6.574036] [D_Loss 0.039373]\n",
      "train [Epoch 1/3] [Batch 277/1173] [Iter 1450] [G_Loss 6.710204] [D_Loss 0.007661]\n",
      "train [Epoch 1/3] [Batch 278/1173] [Iter 1451] [G_Loss 6.888793] [D_Loss 0.013246]\n",
      "train [Epoch 1/3] [Batch 279/1173] [Iter 1452] [G_Loss 7.013633] [D_Loss 0.014176]\n",
      "train [Epoch 1/3] [Batch 280/1173] [Iter 1453] [G_Loss 6.793612] [D_Loss 0.027051]\n",
      "train [Epoch 1/3] [Batch 281/1173] [Iter 1454] [G_Loss 6.625811] [D_Loss 0.009739]\n",
      "train [Epoch 1/3] [Batch 282/1173] [Iter 1455] [G_Loss 6.630636] [D_Loss 0.009569]\n",
      "train [Epoch 1/3] [Batch 283/1173] [Iter 1456] [G_Loss 6.761279] [D_Loss 0.006052]\n",
      "train [Epoch 1/3] [Batch 284/1173] [Iter 1457] [G_Loss 6.929789] [D_Loss 0.010082]\n",
      "train [Epoch 1/3] [Batch 285/1173] [Iter 1458] [G_Loss 7.061600] [D_Loss 0.005804]\n",
      "train [Epoch 1/3] [Batch 286/1173] [Iter 1459] [G_Loss 7.202286] [D_Loss 0.004339]\n",
      "train [Epoch 1/3] [Batch 287/1173] [Iter 1460] [G_Loss 7.343808] [D_Loss 0.005160]\n",
      "train [Epoch 1/3] [Batch 288/1173] [Iter 1461] [G_Loss 7.476038] [D_Loss 0.006077]\n",
      "train [Epoch 1/3] [Batch 289/1173] [Iter 1462] [G_Loss 7.594077] [D_Loss 0.005749]\n",
      "train [Epoch 1/3] [Batch 290/1173] [Iter 1463] [G_Loss 7.692643] [D_Loss 0.013737]\n",
      "train [Epoch 1/3] [Batch 291/1173] [Iter 1464] [G_Loss 7.726770] [D_Loss 0.029170]\n",
      "train [Epoch 1/3] [Batch 292/1173] [Iter 1465] [G_Loss 7.245861] [D_Loss 0.006761]\n",
      "train [Epoch 1/3] [Batch 293/1173] [Iter 1466] [G_Loss 6.998314] [D_Loss 0.011886]\n",
      "train [Epoch 1/3] [Batch 294/1173] [Iter 1467] [G_Loss 6.861284] [D_Loss 0.048099]\n",
      "train [Epoch 1/3] [Batch 295/1173] [Iter 1468] [G_Loss 6.857530] [D_Loss 0.012219]\n",
      "train [Epoch 1/3] [Batch 296/1173] [Iter 1469] [G_Loss 6.831003] [D_Loss 0.019023]\n",
      "train [Epoch 1/3] [Batch 297/1173] [Iter 1470] [G_Loss 6.478542] [D_Loss 0.005628]\n",
      "train [Epoch 1/3] [Batch 298/1173] [Iter 1471] [G_Loss 6.345775] [D_Loss 0.003451]\n",
      "train [Epoch 1/3] [Batch 299/1173] [Iter 1472] [G_Loss 6.351083] [D_Loss 0.006651]\n",
      "train [Epoch 1/3] [Batch 300/1173] [Iter 1473] [G_Loss 6.389518] [D_Loss 0.007154]\n",
      "train [Epoch 1/3] [Batch 301/1173] [Iter 1474] [G_Loss 6.402368] [D_Loss 0.003997]\n",
      "train [Epoch 1/3] [Batch 302/1173] [Iter 1475] [G_Loss 6.477996] [D_Loss 0.010179]\n",
      "train [Epoch 1/3] [Batch 303/1173] [Iter 1476] [G_Loss 6.403670] [D_Loss 0.010860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 1/3] [Batch 304/1173] [Iter 1477] [G_Loss 6.420223] [D_Loss 0.006090]\n",
      "train [Epoch 1/3] [Batch 305/1173] [Iter 1478] [G_Loss 6.482988] [D_Loss 0.003234]\n",
      "train [Epoch 1/3] [Batch 306/1173] [Iter 1479] [G_Loss 6.603266] [D_Loss 0.005203]\n",
      "train [Epoch 1/3] [Batch 307/1173] [Iter 1480] [G_Loss 6.693844] [D_Loss 0.006526]\n",
      "train [Epoch 1/3] [Batch 308/1173] [Iter 1481] [G_Loss 6.855746] [D_Loss 0.003471]\n",
      "train [Epoch 1/3] [Batch 309/1173] [Iter 1482] [G_Loss 7.086655] [D_Loss 0.012107]\n",
      "train [Epoch 1/3] [Batch 310/1173] [Iter 1483] [G_Loss 6.956149] [D_Loss 0.004891]\n",
      "train [Epoch 1/3] [Batch 311/1173] [Iter 1484] [G_Loss 7.008330] [D_Loss 0.020855]\n",
      "train [Epoch 1/3] [Batch 312/1173] [Iter 1485] [G_Loss 6.681633] [D_Loss 0.005603]\n",
      "train [Epoch 1/3] [Batch 313/1173] [Iter 1486] [G_Loss 6.699888] [D_Loss 0.005913]\n",
      "train [Epoch 1/3] [Batch 314/1173] [Iter 1487] [G_Loss 6.891635] [D_Loss 0.005436]\n",
      "train [Epoch 1/3] [Batch 315/1173] [Iter 1488] [G_Loss 7.102704] [D_Loss 0.007334]\n",
      "train [Epoch 1/3] [Batch 316/1173] [Iter 1489] [G_Loss 7.300404] [D_Loss 0.002892]\n",
      "train [Epoch 1/3] [Batch 317/1173] [Iter 1490] [G_Loss 7.492970] [D_Loss 0.007564]\n",
      "train [Epoch 1/3] [Batch 318/1173] [Iter 1491] [G_Loss 7.675281] [D_Loss 0.003237]\n",
      "train [Epoch 1/3] [Batch 319/1173] [Iter 1492] [G_Loss 7.853235] [D_Loss 0.015108]\n",
      "train [Epoch 1/3] [Batch 320/1173] [Iter 1493] [G_Loss 7.520049] [D_Loss 0.002603]\n",
      "train [Epoch 1/3] [Batch 321/1173] [Iter 1494] [G_Loss 7.450787] [D_Loss 0.005675]\n",
      "train [Epoch 1/3] [Batch 322/1173] [Iter 1495] [G_Loss 7.505531] [D_Loss 0.002829]\n",
      "train [Epoch 1/3] [Batch 323/1173] [Iter 1496] [G_Loss 7.631742] [D_Loss 0.063188]\n",
      "train [Epoch 1/3] [Batch 324/1173] [Iter 1497] [G_Loss 6.713818] [D_Loss 0.007005]\n",
      "train [Epoch 1/3] [Batch 325/1173] [Iter 1498] [G_Loss 6.434536] [D_Loss 0.008918]\n",
      "train [Epoch 1/3] [Batch 326/1173] [Iter 1499] [G_Loss 6.586829] [D_Loss 0.004816]\n",
      "train [Epoch 1/3] [Batch 327/1173] [Iter 1500] [G_Loss 6.919941] [D_Loss 0.028171]\n",
      "train [Epoch 1/3] [Batch 328/1173] [Iter 1501] [G_Loss 6.671374] [D_Loss 0.002822]\n",
      "train [Epoch 1/3] [Batch 329/1173] [Iter 1502] [G_Loss 6.798872] [D_Loss 0.010560]\n",
      "train [Epoch 1/3] [Batch 330/1173] [Iter 1503] [G_Loss 6.839027] [D_Loss 0.002471]\n",
      "train [Epoch 1/3] [Batch 331/1173] [Iter 1504] [G_Loss 7.078712] [D_Loss 0.036256]\n",
      "train [Epoch 1/3] [Batch 332/1173] [Iter 1505] [G_Loss 7.367447] [D_Loss 0.006604]\n",
      "train [Epoch 1/3] [Batch 333/1173] [Iter 1506] [G_Loss 7.607832] [D_Loss 0.002702]\n",
      "train [Epoch 1/3] [Batch 334/1173] [Iter 1507] [G_Loss 7.826562] [D_Loss 0.001765]\n",
      "train [Epoch 1/3] [Batch 335/1173] [Iter 1508] [G_Loss 8.015044] [D_Loss 0.002088]\n",
      "train [Epoch 1/3] [Batch 336/1173] [Iter 1509] [G_Loss 8.169904] [D_Loss 0.004956]\n",
      "train [Epoch 1/3] [Batch 337/1173] [Iter 1510] [G_Loss 8.283627] [D_Loss 0.001820]\n",
      "train [Epoch 1/3] [Batch 338/1173] [Iter 1511] [G_Loss 8.380174] [D_Loss 0.017888]\n",
      "train [Epoch 1/3] [Batch 339/1173] [Iter 1512] [G_Loss 7.804995] [D_Loss 0.056334]\n",
      "train [Epoch 1/3] [Batch 340/1173] [Iter 1513] [G_Loss 6.282738] [D_Loss 0.005020]\n",
      "train [Epoch 1/3] [Batch 341/1173] [Iter 1514] [G_Loss 5.977789] [D_Loss 0.012174]\n",
      "train [Epoch 1/3] [Batch 342/1173] [Iter 1515] [G_Loss 6.422871] [D_Loss 0.003491]\n",
      "train [Epoch 1/3] [Batch 343/1173] [Iter 1516] [G_Loss 7.029330] [D_Loss 0.004453]\n",
      "train [Epoch 1/3] [Batch 344/1173] [Iter 1517] [G_Loss 7.540199] [D_Loss 0.002933]\n",
      "train [Epoch 1/3] [Batch 345/1173] [Iter 1518] [G_Loss 7.921566] [D_Loss 0.002773]\n",
      "train [Epoch 1/3] [Batch 346/1173] [Iter 1519] [G_Loss 8.197416] [D_Loss 0.003354]\n",
      "train [Epoch 1/3] [Batch 347/1173] [Iter 1520] [G_Loss 8.396070] [D_Loss 0.002383]\n",
      "train [Epoch 1/3] [Batch 348/1173] [Iter 1521] [G_Loss 8.535819] [D_Loss 0.001401]\n",
      "train [Epoch 1/3] [Batch 349/1173] [Iter 1522] [G_Loss 8.634449] [D_Loss 0.005214]\n",
      "train [Epoch 1/3] [Batch 350/1173] [Iter 1523] [G_Loss 8.611023] [D_Loss 0.003531]\n",
      "train [Epoch 1/3] [Batch 351/1173] [Iter 1524] [G_Loss 8.626218] [D_Loss 0.004590]\n",
      "train [Epoch 1/3] [Batch 352/1173] [Iter 1525] [G_Loss 8.636467] [D_Loss 0.001806]\n",
      "train [Epoch 1/3] [Batch 353/1173] [Iter 1526] [G_Loss 8.651027] [D_Loss 0.014802]\n",
      "train [Epoch 1/3] [Batch 354/1173] [Iter 1527] [G_Loss 8.156191] [D_Loss 0.001801]\n",
      "train [Epoch 1/3] [Batch 355/1173] [Iter 1528] [G_Loss 7.963972] [D_Loss 0.014658]\n",
      "train [Epoch 1/3] [Batch 356/1173] [Iter 1529] [G_Loss 7.618543] [D_Loss 0.001235]\n",
      "train [Epoch 1/3] [Batch 357/1173] [Iter 1530] [G_Loss 7.555606] [D_Loss 0.002607]\n",
      "train [Epoch 1/3] [Batch 358/1173] [Iter 1531] [G_Loss 7.651730] [D_Loss 0.013524]\n",
      "train [Epoch 1/3] [Batch 359/1173] [Iter 1532] [G_Loss 7.827853] [D_Loss 0.002922]\n",
      "train [Epoch 1/3] [Batch 360/1173] [Iter 1533] [G_Loss 8.004940] [D_Loss 0.001203]\n",
      "train [Epoch 1/3] [Batch 361/1173] [Iter 1534] [G_Loss 8.169248] [D_Loss 0.002159]\n",
      "train [Epoch 1/3] [Batch 362/1173] [Iter 1535] [G_Loss 8.320484] [D_Loss 0.000920]\n",
      "train [Epoch 1/3] [Batch 363/1173] [Iter 1536] [G_Loss 8.458327] [D_Loss 0.001304]\n",
      "train [Epoch 1/3] [Batch 364/1173] [Iter 1537] [G_Loss 8.569013] [D_Loss 0.000755]\n",
      "train [Epoch 1/3] [Batch 365/1173] [Iter 1538] [G_Loss 8.669425] [D_Loss 0.007919]\n",
      "train [Epoch 1/3] [Batch 366/1173] [Iter 1539] [G_Loss 8.573767] [D_Loss 0.000874]\n",
      "train [Epoch 1/3] [Batch 367/1173] [Iter 1540] [G_Loss 8.566868] [D_Loss 0.001122]\n",
      "train [Epoch 1/3] [Batch 368/1173] [Iter 1541] [G_Loss 8.597734] [D_Loss 0.000934]\n",
      "train [Epoch 1/3] [Batch 369/1173] [Iter 1542] [G_Loss 8.654904] [D_Loss 0.023693]\n",
      "train [Epoch 1/3] [Batch 370/1173] [Iter 1543] [G_Loss 7.974958] [D_Loss 0.002335]\n",
      "train [Epoch 1/3] [Batch 371/1173] [Iter 1544] [G_Loss 7.662038] [D_Loss 0.001794]\n",
      "train [Epoch 1/3] [Batch 372/1173] [Iter 1545] [G_Loss 7.588526] [D_Loss 0.005818]\n",
      "train [Epoch 1/3] [Batch 373/1173] [Iter 1546] [G_Loss 7.569345] [D_Loss 0.012256]\n",
      "train [Epoch 1/3] [Batch 374/1173] [Iter 1547] [G_Loss 6.962337] [D_Loss 0.006147]\n",
      "train [Epoch 1/3] [Batch 375/1173] [Iter 1548] [G_Loss 6.670954] [D_Loss 0.002568]\n",
      "train [Epoch 1/3] [Batch 376/1173] [Iter 1549] [G_Loss 6.898050] [D_Loss 0.003266]\n",
      "train [Epoch 1/3] [Batch 377/1173] [Iter 1550] [G_Loss 7.253140] [D_Loss 0.001131]\n",
      "train [Epoch 1/3] [Batch 378/1173] [Iter 1551] [G_Loss 7.647660] [D_Loss 0.001373]\n",
      "train [Epoch 1/3] [Batch 379/1173] [Iter 1552] [G_Loss 7.968174] [D_Loss 0.000846]\n",
      "train [Epoch 1/3] [Batch 380/1173] [Iter 1553] [G_Loss 8.196573] [D_Loss 0.000871]\n",
      "train [Epoch 1/3] [Batch 381/1173] [Iter 1554] [G_Loss 8.361150] [D_Loss 0.001365]\n",
      "train [Epoch 1/3] [Batch 382/1173] [Iter 1555] [G_Loss 8.405726] [D_Loss 0.002448]\n",
      "train [Epoch 1/3] [Batch 383/1173] [Iter 1556] [G_Loss 8.466140] [D_Loss 0.001075]\n",
      "train [Epoch 1/3] [Batch 384/1173] [Iter 1557] [G_Loss 8.492652] [D_Loss 0.001411]\n",
      "train [Epoch 1/3] [Batch 385/1173] [Iter 1558] [G_Loss 8.530479] [D_Loss 0.000911]\n",
      "train [Epoch 1/3] [Batch 386/1173] [Iter 1559] [G_Loss 8.587909] [D_Loss 0.001223]\n",
      "train [Epoch 1/3] [Batch 387/1173] [Iter 1560] [G_Loss 8.692310] [D_Loss 0.002196]\n",
      "train [Epoch 1/3] [Batch 388/1173] [Iter 1561] [G_Loss 8.725766] [D_Loss 0.008206]\n",
      "train [Epoch 1/3] [Batch 389/1173] [Iter 1562] [G_Loss 8.171148] [D_Loss 0.001159]\n",
      "train [Epoch 1/3] [Batch 390/1173] [Iter 1563] [G_Loss 7.970418] [D_Loss 0.000681]\n",
      "train [Epoch 1/3] [Batch 391/1173] [Iter 1564] [G_Loss 7.968860] [D_Loss 0.000946]\n",
      "train [Epoch 1/3] [Batch 392/1173] [Iter 1565] [G_Loss 8.105446] [D_Loss 0.002212]\n",
      "train [Epoch 1/3] [Batch 393/1173] [Iter 1566] [G_Loss 8.245355] [D_Loss 0.010311]\n",
      "train [Epoch 1/3] [Batch 394/1173] [Iter 1567] [G_Loss 8.416535] [D_Loss 0.000616]\n",
      "train [Epoch 1/3] [Batch 395/1173] [Iter 1568] [G_Loss 8.568678] [D_Loss 0.000440]\n",
      "train [Epoch 1/3] [Batch 396/1173] [Iter 1569] [G_Loss 8.695816] [D_Loss 0.000539]\n",
      "train [Epoch 1/3] [Batch 397/1173] [Iter 1570] [G_Loss 8.823988] [D_Loss 0.000777]\n",
      "train [Epoch 1/3] [Batch 398/1173] [Iter 1571] [G_Loss 8.942588] [D_Loss 0.006380]\n",
      "train [Epoch 1/3] [Batch 399/1173] [Iter 1572] [G_Loss 8.916921] [D_Loss 0.000799]\n",
      "train [Epoch 1/3] [Batch 400/1173] [Iter 1573] [G_Loss 8.982630] [D_Loss 0.025428]\n",
      "train [Epoch 1/3] [Batch 401/1173] [Iter 1574] [G_Loss 7.574522] [D_Loss 0.002622]\n",
      "train [Epoch 1/3] [Batch 402/1173] [Iter 1575] [G_Loss 7.120538] [D_Loss 0.001895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 1/3] [Batch 403/1173] [Iter 1576] [G_Loss 7.167981] [D_Loss 0.018061]\n",
      "train [Epoch 1/3] [Batch 404/1173] [Iter 1577] [G_Loss 7.329278] [D_Loss 0.001272]\n",
      "train [Epoch 1/3] [Batch 405/1173] [Iter 1578] [G_Loss 7.629812] [D_Loss 0.003407]\n",
      "train [Epoch 1/3] [Batch 406/1173] [Iter 1579] [G_Loss 7.867303] [D_Loss 0.001013]\n",
      "train [Epoch 1/3] [Batch 407/1173] [Iter 1580] [G_Loss 8.074408] [D_Loss 0.001984]\n",
      "train [Epoch 1/3] [Batch 408/1173] [Iter 1581] [G_Loss 8.287231] [D_Loss 0.000784]\n",
      "train [Epoch 1/3] [Batch 409/1173] [Iter 1582] [G_Loss 8.500709] [D_Loss 0.042605]\n",
      "train [Epoch 1/3] [Batch 410/1173] [Iter 1583] [G_Loss 7.084324] [D_Loss 0.003389]\n",
      "train [Epoch 1/3] [Batch 411/1173] [Iter 1584] [G_Loss 6.694813] [D_Loss 0.018339]\n",
      "train [Epoch 1/3] [Batch 412/1173] [Iter 1585] [G_Loss 6.392760] [D_Loss 0.002257]\n",
      "train [Epoch 1/3] [Batch 413/1173] [Iter 1586] [G_Loss 6.899122] [D_Loss 0.001226]\n",
      "train [Epoch 1/3] [Batch 414/1173] [Iter 1587] [G_Loss 7.531378] [D_Loss 0.001061]\n",
      "train [Epoch 1/3] [Batch 415/1173] [Iter 1588] [G_Loss 8.036392] [D_Loss 0.001196]\n",
      "train [Epoch 1/3] [Batch 416/1173] [Iter 1589] [G_Loss 8.411989] [D_Loss 0.001482]\n",
      "train [Epoch 1/3] [Batch 417/1173] [Iter 1590] [G_Loss 8.578746] [D_Loss 0.000884]\n",
      "train [Epoch 1/3] [Batch 418/1173] [Iter 1591] [G_Loss 8.785764] [D_Loss 0.000786]\n",
      "train [Epoch 1/3] [Batch 419/1173] [Iter 1592] [G_Loss 8.887625] [D_Loss 0.000545]\n",
      "train [Epoch 1/3] [Batch 420/1173] [Iter 1593] [G_Loss 8.992827] [D_Loss 0.003444]\n",
      "train [Epoch 1/3] [Batch 421/1173] [Iter 1594] [G_Loss 9.035718] [D_Loss 0.000682]\n",
      "train [Epoch 1/3] [Batch 422/1173] [Iter 1595] [G_Loss 9.099147] [D_Loss 0.000505]\n",
      "train [Epoch 1/3] [Batch 423/1173] [Iter 1596] [G_Loss 9.140117] [D_Loss 0.000390]\n",
      "train [Epoch 1/3] [Batch 424/1173] [Iter 1597] [G_Loss 9.215580] [D_Loss 0.002729]\n",
      "train [Epoch 1/3] [Batch 425/1173] [Iter 1598] [G_Loss 9.262952] [D_Loss 0.000582]\n",
      "train [Epoch 1/3] [Batch 426/1173] [Iter 1599] [G_Loss 9.292274] [D_Loss 0.002134]\n",
      "train [Epoch 1/3] [Batch 427/1173] [Iter 1600] [G_Loss 9.235755] [D_Loss 0.000274]\n",
      "train [Epoch 1/3] [Batch 428/1173] [Iter 1601] [G_Loss 9.269327] [D_Loss 0.000738]\n",
      "train [Epoch 1/3] [Batch 429/1173] [Iter 1602] [G_Loss 9.314742] [D_Loss 0.000646]\n",
      "train [Epoch 1/3] [Batch 430/1173] [Iter 1603] [G_Loss 9.364156] [D_Loss 0.001350]\n",
      "train [Epoch 1/3] [Batch 431/1173] [Iter 1604] [G_Loss 9.380075] [D_Loss 0.003010]\n",
      "train [Epoch 1/3] [Batch 432/1173] [Iter 1605] [G_Loss 9.304969] [D_Loss 0.002795]\n",
      "train [Epoch 1/3] [Batch 433/1173] [Iter 1606] [G_Loss 9.210905] [D_Loss 0.036085]\n",
      "train [Epoch 1/3] [Batch 434/1173] [Iter 1607] [G_Loss 7.977899] [D_Loss 0.001012]\n",
      "train [Epoch 1/3] [Batch 435/1173] [Iter 1608] [G_Loss 7.503005] [D_Loss 0.000949]\n",
      "train [Epoch 1/3] [Batch 436/1173] [Iter 1609] [G_Loss 7.446733] [D_Loss 0.002192]\n",
      "train [Epoch 1/3] [Batch 437/1173] [Iter 1610] [G_Loss 7.632013] [D_Loss 0.004841]\n",
      "train [Epoch 1/3] [Batch 438/1173] [Iter 1611] [G_Loss 7.807837] [D_Loss 0.001131]\n",
      "train [Epoch 1/3] [Batch 439/1173] [Iter 1612] [G_Loss 8.081750] [D_Loss 0.000429]\n",
      "train [Epoch 1/3] [Batch 440/1173] [Iter 1613] [G_Loss 8.302726] [D_Loss 0.001466]\n",
      "train [Epoch 1/3] [Batch 441/1173] [Iter 1614] [G_Loss 8.497342] [D_Loss 0.000642]\n",
      "train [Epoch 1/3] [Batch 442/1173] [Iter 1615] [G_Loss 8.701350] [D_Loss 0.005725]\n",
      "train [Epoch 1/3] [Batch 443/1173] [Iter 1616] [G_Loss 8.395001] [D_Loss 0.000690]\n",
      "train [Epoch 1/3] [Batch 444/1173] [Iter 1617] [G_Loss 8.299021] [D_Loss 0.000938]\n",
      "train [Epoch 1/3] [Batch 445/1173] [Iter 1618] [G_Loss 8.407229] [D_Loss 0.000768]\n",
      "train [Epoch 1/3] [Batch 446/1173] [Iter 1619] [G_Loss 8.426282] [D_Loss 0.000292]\n",
      "train [Epoch 1/3] [Batch 447/1173] [Iter 1620] [G_Loss 8.583867] [D_Loss 0.001993]\n",
      "train [Epoch 1/3] [Batch 448/1173] [Iter 1621] [G_Loss 8.671715] [D_Loss 0.000644]\n",
      "train [Epoch 1/3] [Batch 449/1173] [Iter 1622] [G_Loss 8.753631] [D_Loss 0.000354]\n",
      "train [Epoch 1/3] [Batch 450/1173] [Iter 1623] [G_Loss 8.853242] [D_Loss 0.000302]\n",
      "train [Epoch 1/3] [Batch 451/1173] [Iter 1624] [G_Loss 8.931048] [D_Loss 0.001100]\n",
      "train [Epoch 1/3] [Batch 452/1173] [Iter 1625] [G_Loss 8.939277] [D_Loss 0.000563]\n",
      "train [Epoch 1/3] [Batch 453/1173] [Iter 1626] [G_Loss 9.029663] [D_Loss 0.000951]\n",
      "train [Epoch 1/3] [Batch 454/1173] [Iter 1627] [G_Loss 9.065595] [D_Loss 0.001515]\n",
      "train [Epoch 1/3] [Batch 455/1173] [Iter 1628] [G_Loss 9.114392] [D_Loss 0.000451]\n",
      "train [Epoch 1/3] [Batch 456/1173] [Iter 1629] [G_Loss 9.233032] [D_Loss 0.000297]\n",
      "train [Epoch 1/3] [Batch 457/1173] [Iter 1630] [G_Loss 9.293038] [D_Loss 0.000814]\n",
      "train [Epoch 1/3] [Batch 458/1173] [Iter 1631] [G_Loss 9.301574] [D_Loss 0.000333]\n",
      "train [Epoch 1/3] [Batch 459/1173] [Iter 1632] [G_Loss 9.377136] [D_Loss 0.007814]\n",
      "train [Epoch 1/3] [Batch 460/1173] [Iter 1633] [G_Loss 9.377099] [D_Loss 0.000645]\n",
      "train [Epoch 1/3] [Batch 461/1173] [Iter 1634] [G_Loss 9.426675] [D_Loss 0.000846]\n",
      "train [Epoch 1/3] [Batch 462/1173] [Iter 1635] [G_Loss 9.461752] [D_Loss 0.000691]\n",
      "train [Epoch 1/3] [Batch 463/1173] [Iter 1636] [G_Loss 9.437534] [D_Loss 0.000389]\n",
      "train [Epoch 1/3] [Batch 464/1173] [Iter 1637] [G_Loss 9.515235] [D_Loss 0.000311]\n",
      "train [Epoch 1/3] [Batch 465/1173] [Iter 1638] [G_Loss 9.585754] [D_Loss 0.009267]\n",
      "train [Epoch 1/3] [Batch 466/1173] [Iter 1639] [G_Loss 8.763438] [D_Loss 0.000704]\n",
      "train [Epoch 1/3] [Batch 467/1173] [Iter 1640] [G_Loss 8.396025] [D_Loss 0.000347]\n",
      "train [Epoch 1/3] [Batch 468/1173] [Iter 1641] [G_Loss 8.389749] [D_Loss 0.000787]\n",
      "train [Epoch 1/3] [Batch 469/1173] [Iter 1642] [G_Loss 8.384273] [D_Loss 0.000562]\n",
      "train [Epoch 1/3] [Batch 470/1173] [Iter 1643] [G_Loss 8.542079] [D_Loss 0.000427]\n",
      "train [Epoch 1/3] [Batch 471/1173] [Iter 1644] [G_Loss 8.682429] [D_Loss 0.001667]\n",
      "train [Epoch 1/3] [Batch 472/1173] [Iter 1645] [G_Loss 8.778085] [D_Loss 0.002049]\n",
      "train [Epoch 1/3] [Batch 473/1173] [Iter 1646] [G_Loss 8.928669] [D_Loss 0.001191]\n",
      "train [Epoch 1/3] [Batch 474/1173] [Iter 1647] [G_Loss 8.848351] [D_Loss 0.000644]\n",
      "train [Epoch 1/3] [Batch 475/1173] [Iter 1648] [G_Loss 9.061417] [D_Loss 0.001913]\n",
      "train [Epoch 1/3] [Batch 476/1173] [Iter 1649] [G_Loss 9.012998] [D_Loss 0.000764]\n",
      "train [Epoch 1/3] [Batch 477/1173] [Iter 1650] [G_Loss 9.184720] [D_Loss 0.002221]\n",
      "train [Epoch 1/3] [Batch 478/1173] [Iter 1651] [G_Loss 9.091889] [D_Loss 0.000229]\n",
      "train [Epoch 1/3] [Batch 479/1173] [Iter 1652] [G_Loss 9.168210] [D_Loss 0.000432]\n",
      "train [Epoch 1/3] [Batch 480/1173] [Iter 1653] [G_Loss 9.297523] [D_Loss 0.000979]\n",
      "train [Epoch 1/3] [Batch 481/1173] [Iter 1654] [G_Loss 9.333101] [D_Loss 0.000261]\n",
      "train [Epoch 1/3] [Batch 482/1173] [Iter 1655] [G_Loss 9.457275] [D_Loss 0.000272]\n",
      "train [Epoch 1/3] [Batch 483/1173] [Iter 1656] [G_Loss 9.511192] [D_Loss 0.000489]\n",
      "train [Epoch 1/3] [Batch 484/1173] [Iter 1657] [G_Loss 9.705092] [D_Loss 0.000407]\n",
      "train [Epoch 1/3] [Batch 485/1173] [Iter 1658] [G_Loss 9.737750] [D_Loss 0.000837]\n",
      "train [Epoch 1/3] [Batch 486/1173] [Iter 1659] [G_Loss 9.818575] [D_Loss 0.005053]\n",
      "train [Epoch 1/3] [Batch 487/1173] [Iter 1660] [G_Loss 9.153113] [D_Loss 0.000259]\n",
      "train [Epoch 1/3] [Batch 488/1173] [Iter 1661] [G_Loss 8.898341] [D_Loss 0.000335]\n",
      "train [Epoch 1/3] [Batch 489/1173] [Iter 1662] [G_Loss 8.852077] [D_Loss 0.000521]\n",
      "train [Epoch 1/3] [Batch 490/1173] [Iter 1663] [G_Loss 8.901450] [D_Loss 0.000328]\n",
      "train [Epoch 1/3] [Batch 491/1173] [Iter 1664] [G_Loss 9.038601] [D_Loss 0.001197]\n",
      "train [Epoch 1/3] [Batch 492/1173] [Iter 1665] [G_Loss 9.158271] [D_Loss 0.001106]\n",
      "train [Epoch 1/3] [Batch 493/1173] [Iter 1666] [G_Loss 9.188243] [D_Loss 0.000302]\n",
      "train [Epoch 1/3] [Batch 494/1173] [Iter 1667] [G_Loss 9.418367] [D_Loss 0.000826]\n",
      "train [Epoch 1/3] [Batch 495/1173] [Iter 1668] [G_Loss 9.419583] [D_Loss 0.012223]\n",
      "train [Epoch 1/3] [Batch 496/1173] [Iter 1669] [G_Loss 9.514152] [D_Loss 0.000331]\n",
      "train [Epoch 1/3] [Batch 497/1173] [Iter 1670] [G_Loss 9.621649] [D_Loss 0.000387]\n",
      "train [Epoch 1/3] [Batch 498/1173] [Iter 1671] [G_Loss 9.687760] [D_Loss 0.000247]\n",
      "train [Epoch 1/3] [Batch 499/1173] [Iter 1672] [G_Loss 9.853354] [D_Loss 0.000717]\n",
      "train [Epoch 1/3] [Batch 500/1173] [Iter 1673] [G_Loss 9.841521] [D_Loss 0.010934]\n",
      "train [Epoch 1/3] [Batch 501/1173] [Iter 1674] [G_Loss 9.033237] [D_Loss 0.001406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 1/3] [Batch 502/1173] [Iter 1675] [G_Loss 8.649711] [D_Loss 0.001004]\n",
      "train [Epoch 1/3] [Batch 503/1173] [Iter 1676] [G_Loss 8.584244] [D_Loss 0.000494]\n",
      "train [Epoch 1/3] [Batch 504/1173] [Iter 1677] [G_Loss 8.602368] [D_Loss 0.000389]\n",
      "train [Epoch 1/3] [Batch 505/1173] [Iter 1678] [G_Loss 8.811377] [D_Loss 0.000716]\n",
      "train [Epoch 1/3] [Batch 506/1173] [Iter 1679] [G_Loss 8.969628] [D_Loss 0.000376]\n",
      "train [Epoch 1/3] [Batch 507/1173] [Iter 1680] [G_Loss 9.068771] [D_Loss 0.000237]\n",
      "train [Epoch 1/3] [Batch 508/1173] [Iter 1681] [G_Loss 9.270639] [D_Loss 0.000224]\n",
      "train [Epoch 1/3] [Batch 509/1173] [Iter 1682] [G_Loss 9.390615] [D_Loss 0.000482]\n",
      "train [Epoch 1/3] [Batch 510/1173] [Iter 1683] [G_Loss 9.520664] [D_Loss 0.000158]\n",
      "train [Epoch 1/3] [Batch 511/1173] [Iter 1684] [G_Loss 9.625791] [D_Loss 0.000304]\n",
      "train [Epoch 1/3] [Batch 512/1173] [Iter 1685] [G_Loss 9.676461] [D_Loss 0.000143]\n",
      "train [Epoch 1/3] [Batch 513/1173] [Iter 1686] [G_Loss 9.822668] [D_Loss 0.000176]\n",
      "train [Epoch 1/3] [Batch 514/1173] [Iter 1687] [G_Loss 9.866054] [D_Loss 0.000382]\n",
      "train [Epoch 1/3] [Batch 515/1173] [Iter 1688] [G_Loss 9.966315] [D_Loss 0.000234]\n",
      "train [Epoch 1/3] [Batch 516/1173] [Iter 1689] [G_Loss 10.027440] [D_Loss 0.000365]\n",
      "train [Epoch 1/3] [Batch 517/1173] [Iter 1690] [G_Loss 10.058820] [D_Loss 0.000202]\n",
      "train [Epoch 1/3] [Batch 518/1173] [Iter 1691] [G_Loss 10.130834] [D_Loss 0.003941]\n",
      "train [Epoch 1/3] [Batch 519/1173] [Iter 1692] [G_Loss 10.161200] [D_Loss 0.000122]\n",
      "train [Epoch 1/3] [Batch 520/1173] [Iter 1693] [G_Loss 10.212471] [D_Loss 0.001757]\n",
      "train [Epoch 1/3] [Batch 521/1173] [Iter 1694] [G_Loss 10.266900] [D_Loss 0.000195]\n",
      "train [Epoch 1/3] [Batch 522/1173] [Iter 1695] [G_Loss 10.258929] [D_Loss 0.000069]\n",
      "train [Epoch 1/3] [Batch 523/1173] [Iter 1696] [G_Loss 10.333314] [D_Loss 0.000328]\n",
      "train [Epoch 1/3] [Batch 524/1173] [Iter 1697] [G_Loss 10.340611] [D_Loss 0.000238]\n",
      "train [Epoch 1/3] [Batch 525/1173] [Iter 1698] [G_Loss 10.418728] [D_Loss 0.024303]\n",
      "train [Epoch 1/3] [Batch 526/1173] [Iter 1699] [G_Loss 9.112012] [D_Loss 0.000643]\n",
      "train [Epoch 1/3] [Batch 527/1173] [Iter 1700] [G_Loss 8.484962] [D_Loss 0.000492]\n",
      "train [Epoch 1/3] [Batch 528/1173] [Iter 1701] [G_Loss 8.346292] [D_Loss 0.000657]\n",
      "train [Epoch 1/3] [Batch 529/1173] [Iter 1702] [G_Loss 8.388443] [D_Loss 0.000847]\n",
      "train [Epoch 1/3] [Batch 530/1173] [Iter 1703] [G_Loss 8.502530] [D_Loss 0.000408]\n",
      "train [Epoch 1/3] [Batch 531/1173] [Iter 1704] [G_Loss 8.762882] [D_Loss 0.000301]\n",
      "train [Epoch 1/3] [Batch 532/1173] [Iter 1705] [G_Loss 8.969481] [D_Loss 0.000757]\n",
      "train [Epoch 1/3] [Batch 533/1173] [Iter 1706] [G_Loss 9.107842] [D_Loss 0.000521]\n",
      "train [Epoch 1/3] [Batch 534/1173] [Iter 1707] [G_Loss 9.251909] [D_Loss 0.000883]\n",
      "train [Epoch 1/3] [Batch 535/1173] [Iter 1708] [G_Loss 9.360797] [D_Loss 0.000140]\n",
      "train [Epoch 1/3] [Batch 536/1173] [Iter 1709] [G_Loss 9.485395] [D_Loss 0.001540]\n",
      "train [Epoch 1/3] [Batch 537/1173] [Iter 1710] [G_Loss 9.574692] [D_Loss 0.000913]\n",
      "train [Epoch 1/3] [Batch 538/1173] [Iter 1711] [G_Loss 9.642434] [D_Loss 0.001866]\n",
      "train [Epoch 1/3] [Batch 539/1173] [Iter 1712] [G_Loss 9.695392] [D_Loss 0.000337]\n",
      "train [Epoch 1/3] [Batch 540/1173] [Iter 1713] [G_Loss 9.769252] [D_Loss 0.000110]\n",
      "train [Epoch 1/3] [Batch 541/1173] [Iter 1714] [G_Loss 9.844678] [D_Loss 0.000203]\n",
      "train [Epoch 1/3] [Batch 542/1173] [Iter 1715] [G_Loss 9.909933] [D_Loss 0.001440]\n",
      "train [Epoch 1/3] [Batch 543/1173] [Iter 1716] [G_Loss 9.901337] [D_Loss 0.000221]\n",
      "train [Epoch 1/3] [Batch 544/1173] [Iter 1717] [G_Loss 9.892194] [D_Loss 0.000710]\n",
      "train [Epoch 1/3] [Batch 545/1173] [Iter 1718] [G_Loss 9.975496] [D_Loss 0.000161]\n",
      "train [Epoch 1/3] [Batch 546/1173] [Iter 1719] [G_Loss 9.979572] [D_Loss 0.000147]\n",
      "train [Epoch 1/3] [Batch 547/1173] [Iter 1720] [G_Loss 10.051896] [D_Loss 0.000191]\n",
      "train [Epoch 1/3] [Batch 548/1173] [Iter 1721] [G_Loss 10.108159] [D_Loss 0.006442]\n",
      "train [Epoch 1/3] [Batch 549/1173] [Iter 1722] [G_Loss 10.179480] [D_Loss 0.000174]\n",
      "train [Epoch 1/3] [Batch 550/1173] [Iter 1723] [G_Loss 10.234451] [D_Loss 0.000428]\n",
      "train [Epoch 1/3] [Batch 551/1173] [Iter 1724] [G_Loss 10.272292] [D_Loss 0.000647]\n",
      "train [Epoch 1/3] [Batch 552/1173] [Iter 1725] [G_Loss 10.301911] [D_Loss 0.002589]\n",
      "train [Epoch 1/3] [Batch 553/1173] [Iter 1726] [G_Loss 10.350836] [D_Loss 0.000138]\n",
      "train [Epoch 1/3] [Batch 554/1173] [Iter 1727] [G_Loss 10.415797] [D_Loss 0.004862]\n",
      "train [Epoch 1/3] [Batch 555/1173] [Iter 1728] [G_Loss 10.097178] [D_Loss 0.000157]\n",
      "train [Epoch 1/3] [Batch 556/1173] [Iter 1729] [G_Loss 9.981156] [D_Loss 0.000349]\n",
      "train [Epoch 1/3] [Batch 557/1173] [Iter 1730] [G_Loss 9.979083] [D_Loss 0.000376]\n",
      "train [Epoch 1/3] [Batch 558/1173] [Iter 1731] [G_Loss 9.974422] [D_Loss 0.000357]\n",
      "train [Epoch 1/3] [Batch 559/1173] [Iter 1732] [G_Loss 10.041470] [D_Loss 0.000071]\n",
      "train [Epoch 1/3] [Batch 560/1173] [Iter 1733] [G_Loss 10.067448] [D_Loss 0.000097]\n",
      "train [Epoch 1/3] [Batch 561/1173] [Iter 1734] [G_Loss 10.103877] [D_Loss 0.000452]\n",
      "train [Epoch 1/3] [Batch 562/1173] [Iter 1735] [G_Loss 10.158881] [D_Loss 0.000162]\n",
      "train [Epoch 1/3] [Batch 563/1173] [Iter 1736] [G_Loss 10.181904] [D_Loss 0.000194]\n",
      "train [Epoch 1/3] [Batch 564/1173] [Iter 1737] [G_Loss 10.237489] [D_Loss 0.006344]\n",
      "train [Epoch 1/3] [Batch 565/1173] [Iter 1738] [G_Loss 10.271186] [D_Loss 0.001100]\n",
      "train [Epoch 1/3] [Batch 566/1173] [Iter 1739] [G_Loss 10.324365] [D_Loss 0.014582]\n",
      "train [Epoch 1/3] [Batch 567/1173] [Iter 1740] [G_Loss 9.234285] [D_Loss 0.000503]\n",
      "train [Epoch 1/3] [Batch 568/1173] [Iter 1741] [G_Loss 8.774582] [D_Loss 0.000433]\n",
      "train [Epoch 1/3] [Batch 569/1173] [Iter 1742] [G_Loss 8.606478] [D_Loss 0.000238]\n",
      "train [Epoch 1/3] [Batch 570/1173] [Iter 1743] [G_Loss 8.681544] [D_Loss 0.000356]\n",
      "train [Epoch 1/3] [Batch 571/1173] [Iter 1744] [G_Loss 8.806214] [D_Loss 0.000383]\n",
      "train [Epoch 1/3] [Batch 572/1173] [Iter 1745] [G_Loss 8.946007] [D_Loss 0.000192]\n",
      "train [Epoch 1/3] [Batch 573/1173] [Iter 1746] [G_Loss 9.118520] [D_Loss 0.000220]\n",
      "train [Epoch 1/3] [Batch 574/1173] [Iter 1747] [G_Loss 9.286142] [D_Loss 0.000390]\n",
      "train [Epoch 1/3] [Batch 575/1173] [Iter 1748] [G_Loss 9.380603] [D_Loss 0.000188]\n",
      "train [Epoch 1/3] [Batch 576/1173] [Iter 1749] [G_Loss 9.505372] [D_Loss 0.000292]\n",
      "train [Epoch 1/3] [Batch 577/1173] [Iter 1750] [G_Loss 9.672265] [D_Loss 0.000209]\n",
      "train [Epoch 1/3] [Batch 578/1173] [Iter 1751] [G_Loss 9.701527] [D_Loss 0.000201]\n",
      "train [Epoch 1/3] [Batch 579/1173] [Iter 1752] [G_Loss 9.785938] [D_Loss 0.000426]\n",
      "train [Epoch 1/3] [Batch 580/1173] [Iter 1753] [G_Loss 9.854404] [D_Loss 0.000225]\n",
      "train [Epoch 1/3] [Batch 581/1173] [Iter 1754] [G_Loss 9.896904] [D_Loss 0.000344]\n",
      "train [Epoch 1/3] [Batch 582/1173] [Iter 1755] [G_Loss 9.998045] [D_Loss 0.000890]\n",
      "train [Epoch 1/3] [Batch 583/1173] [Iter 1756] [G_Loss 10.054258] [D_Loss 0.000809]\n",
      "train [Epoch 1/3] [Batch 584/1173] [Iter 1757] [G_Loss 10.087095] [D_Loss 0.000165]\n",
      "train [Epoch 1/3] [Batch 585/1173] [Iter 1758] [G_Loss 10.130291] [D_Loss 0.000906]\n",
      "train [Epoch 1/3] [Batch 586/1173] [Iter 1759] [G_Loss 10.191899] [D_Loss 0.000155]\n",
      "train [Epoch 1/3] [Batch 587/1173] [Iter 1760] [G_Loss 10.226361] [D_Loss 0.000178]\n",
      "train [Epoch 1/3] [Batch 588/1173] [Iter 1761] [G_Loss 10.272109] [D_Loss 0.000249]\n",
      "train [Epoch 1/3] [Batch 589/1173] [Iter 1762] [G_Loss 10.290907] [D_Loss 0.000769]\n",
      "train [Epoch 1/3] [Batch 590/1173] [Iter 1763] [G_Loss 10.320845] [D_Loss 0.000077]\n",
      "train [Epoch 1/3] [Batch 591/1173] [Iter 1764] [G_Loss 10.332744] [D_Loss 0.001278]\n",
      "train [Epoch 1/3] [Batch 592/1173] [Iter 1765] [G_Loss 10.336713] [D_Loss 0.000092]\n",
      "train [Epoch 1/3] [Batch 593/1173] [Iter 1766] [G_Loss 10.372754] [D_Loss 0.000218]\n",
      "train [Epoch 1/3] [Batch 594/1173] [Iter 1767] [G_Loss 10.399799] [D_Loss 0.000101]\n",
      "train [Epoch 1/3] [Batch 595/1173] [Iter 1768] [G_Loss 10.430622] [D_Loss 0.000077]\n",
      "train [Epoch 1/3] [Batch 596/1173] [Iter 1769] [G_Loss 10.483918] [D_Loss 0.000427]\n",
      "train [Epoch 1/3] [Batch 597/1173] [Iter 1770] [G_Loss 10.501369] [D_Loss 0.000953]\n",
      "train [Epoch 1/3] [Batch 598/1173] [Iter 1771] [G_Loss 10.507369] [D_Loss 0.000165]\n",
      "train [Epoch 1/3] [Batch 599/1173] [Iter 1772] [G_Loss 10.517447] [D_Loss 0.000125]\n",
      "train [Epoch 1/3] [Batch 600/1173] [Iter 1773] [G_Loss 10.551770] [D_Loss 0.000213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 1/3] [Batch 601/1173] [Iter 1774] [G_Loss 10.578857] [D_Loss 0.000523]\n",
      "train [Epoch 1/3] [Batch 602/1173] [Iter 1775] [G_Loss 10.605552] [D_Loss 0.000860]\n",
      "train [Epoch 1/3] [Batch 603/1173] [Iter 1776] [G_Loss 10.667356] [D_Loss 0.000109]\n",
      "train [Epoch 1/3] [Batch 604/1173] [Iter 1777] [G_Loss 10.699407] [D_Loss 0.000216]\n",
      "train [Epoch 1/3] [Batch 605/1173] [Iter 1778] [G_Loss 10.723032] [D_Loss 0.000387]\n",
      "train [Epoch 1/3] [Batch 606/1173] [Iter 1779] [G_Loss 10.734158] [D_Loss 0.000402]\n",
      "train [Epoch 1/3] [Batch 607/1173] [Iter 1780] [G_Loss 10.713093] [D_Loss 0.000212]\n",
      "train [Epoch 1/3] [Batch 608/1173] [Iter 1781] [G_Loss 10.692339] [D_Loss 0.000396]\n",
      "train [Epoch 1/3] [Batch 609/1173] [Iter 1782] [G_Loss 10.722937] [D_Loss 0.000150]\n",
      "train [Epoch 1/3] [Batch 610/1173] [Iter 1783] [G_Loss 10.732395] [D_Loss 0.000133]\n",
      "train [Epoch 1/3] [Batch 611/1173] [Iter 1784] [G_Loss 10.743134] [D_Loss 0.000523]\n",
      "train [Epoch 1/3] [Batch 612/1173] [Iter 1785] [G_Loss 10.769148] [D_Loss 0.000161]\n",
      "train [Epoch 1/3] [Batch 613/1173] [Iter 1786] [G_Loss 10.813611] [D_Loss 0.000088]\n",
      "train [Epoch 1/3] [Batch 614/1173] [Iter 1787] [G_Loss 10.830036] [D_Loss 0.000105]\n",
      "train [Epoch 1/3] [Batch 615/1173] [Iter 1788] [G_Loss 10.834604] [D_Loss 0.000238]\n",
      "train [Epoch 1/3] [Batch 616/1173] [Iter 1789] [G_Loss 10.859568] [D_Loss 0.000473]\n",
      "train [Epoch 1/3] [Batch 617/1173] [Iter 1790] [G_Loss 10.889997] [D_Loss 0.000244]\n",
      "train [Epoch 1/3] [Batch 618/1173] [Iter 1791] [G_Loss 10.916653] [D_Loss 0.000060]\n",
      "train [Epoch 1/3] [Batch 619/1173] [Iter 1792] [G_Loss 10.918252] [D_Loss 0.000465]\n",
      "train [Epoch 1/3] [Batch 620/1173] [Iter 1793] [G_Loss 10.939640] [D_Loss 0.000586]\n",
      "train [Epoch 1/3] [Batch 621/1173] [Iter 1794] [G_Loss 10.978845] [D_Loss 0.000154]\n",
      "train [Epoch 1/3] [Batch 622/1173] [Iter 1795] [G_Loss 10.986613] [D_Loss 0.000727]\n",
      "train [Epoch 1/3] [Batch 623/1173] [Iter 1796] [G_Loss 10.999885] [D_Loss 0.000452]\n",
      "train [Epoch 1/3] [Batch 624/1173] [Iter 1797] [G_Loss 11.026611] [D_Loss 0.042290]\n",
      "train [Epoch 1/3] [Batch 625/1173] [Iter 1798] [G_Loss 8.842933] [D_Loss 0.000472]\n",
      "train [Epoch 1/3] [Batch 626/1173] [Iter 1799] [G_Loss 7.881202] [D_Loss 0.000576]\n",
      "train [Epoch 1/3] [Batch 627/1173] [Iter 1800] [G_Loss 7.634752] [D_Loss 0.000519]\n",
      "train [Epoch 1/3] [Batch 628/1173] [Iter 1801] [G_Loss 7.826893] [D_Loss 0.000971]\n",
      "train [Epoch 1/3] [Batch 629/1173] [Iter 1802] [G_Loss 8.114224] [D_Loss 0.000371]\n",
      "train [Epoch 1/3] [Batch 630/1173] [Iter 1803] [G_Loss 8.399981] [D_Loss 0.000259]\n",
      "train [Epoch 1/3] [Batch 631/1173] [Iter 1804] [G_Loss 8.452335] [D_Loss 0.001018]\n",
      "train [Epoch 1/3] [Batch 632/1173] [Iter 1805] [G_Loss 8.270041] [D_Loss 0.000644]\n",
      "train [Epoch 1/3] [Batch 633/1173] [Iter 1806] [G_Loss 8.165496] [D_Loss 0.000811]\n",
      "train [Epoch 1/3] [Batch 634/1173] [Iter 1807] [G_Loss 8.034280] [D_Loss 0.000421]\n",
      "train [Epoch 1/3] [Batch 635/1173] [Iter 1808] [G_Loss 8.209672] [D_Loss 0.000405]\n",
      "train [Epoch 1/3] [Batch 636/1173] [Iter 1809] [G_Loss 8.605412] [D_Loss 0.000268]\n",
      "train [Epoch 1/3] [Batch 637/1173] [Iter 1810] [G_Loss 9.031125] [D_Loss 0.000415]\n",
      "train [Epoch 1/3] [Batch 638/1173] [Iter 1811] [G_Loss 9.281498] [D_Loss 0.000374]\n",
      "train [Epoch 1/3] [Batch 639/1173] [Iter 1812] [G_Loss 9.528968] [D_Loss 0.000153]\n",
      "train [Epoch 1/3] [Batch 640/1173] [Iter 1813] [G_Loss 9.777249] [D_Loss 0.000119]\n",
      "train [Epoch 1/3] [Batch 641/1173] [Iter 1814] [G_Loss 9.922478] [D_Loss 0.000143]\n",
      "train [Epoch 1/3] [Batch 642/1173] [Iter 1815] [G_Loss 10.001198] [D_Loss 0.000508]\n",
      "train [Epoch 1/3] [Batch 643/1173] [Iter 1816] [G_Loss 10.066787] [D_Loss 0.000089]\n",
      "train [Epoch 1/3] [Batch 644/1173] [Iter 1817] [G_Loss 10.173976] [D_Loss 0.000329]\n",
      "train [Epoch 1/3] [Batch 645/1173] [Iter 1818] [G_Loss 10.250466] [D_Loss 0.000153]\n",
      "train [Epoch 1/3] [Batch 646/1173] [Iter 1819] [G_Loss 10.354410] [D_Loss 0.000105]\n",
      "train [Epoch 1/3] [Batch 647/1173] [Iter 1820] [G_Loss 10.447149] [D_Loss 0.000142]\n",
      "train [Epoch 1/3] [Batch 648/1173] [Iter 1821] [G_Loss 10.471595] [D_Loss 0.000501]\n",
      "train [Epoch 1/3] [Batch 649/1173] [Iter 1822] [G_Loss 10.516926] [D_Loss 0.000132]\n",
      "train [Epoch 1/3] [Batch 650/1173] [Iter 1823] [G_Loss 10.569460] [D_Loss 0.000526]\n",
      "train [Epoch 1/3] [Batch 651/1173] [Iter 1824] [G_Loss 10.619516] [D_Loss 0.000057]\n",
      "train [Epoch 1/3] [Batch 652/1173] [Iter 1825] [G_Loss 10.633515] [D_Loss 0.000921]\n",
      "train [Epoch 1/3] [Batch 653/1173] [Iter 1826] [G_Loss 10.671645] [D_Loss 0.000079]\n",
      "train [Epoch 1/3] [Batch 654/1173] [Iter 1827] [G_Loss 10.689959] [D_Loss 0.000071]\n",
      "train [Epoch 1/3] [Batch 655/1173] [Iter 1828] [G_Loss 10.812178] [D_Loss 0.000385]\n",
      "train [Epoch 1/3] [Batch 656/1173] [Iter 1829] [G_Loss 10.797209] [D_Loss 0.000147]\n",
      "train [Epoch 1/3] [Batch 657/1173] [Iter 1830] [G_Loss 10.825515] [D_Loss 0.000239]\n",
      "train [Epoch 1/3] [Batch 658/1173] [Iter 1831] [G_Loss 10.904925] [D_Loss 0.000125]\n",
      "train [Epoch 1/3] [Batch 659/1173] [Iter 1832] [G_Loss 10.889221] [D_Loss 0.000128]\n",
      "train [Epoch 1/3] [Batch 660/1173] [Iter 1833] [G_Loss 10.900388] [D_Loss 0.000227]\n",
      "train [Epoch 1/3] [Batch 661/1173] [Iter 1834] [G_Loss 10.984549] [D_Loss 0.000333]\n",
      "train [Epoch 1/3] [Batch 662/1173] [Iter 1835] [G_Loss 10.976032] [D_Loss 0.000048]\n",
      "train [Epoch 1/3] [Batch 663/1173] [Iter 1836] [G_Loss 11.052732] [D_Loss 0.000053]\n",
      "train [Epoch 1/3] [Batch 664/1173] [Iter 1837] [G_Loss 11.016477] [D_Loss 0.000617]\n",
      "train [Epoch 1/3] [Batch 665/1173] [Iter 1838] [G_Loss 11.047810] [D_Loss 0.000128]\n",
      "train [Epoch 1/3] [Batch 666/1173] [Iter 1839] [G_Loss 11.057737] [D_Loss 0.000048]\n",
      "train [Epoch 1/3] [Batch 667/1173] [Iter 1840] [G_Loss 11.087293] [D_Loss 0.000047]\n",
      "train [Epoch 1/3] [Batch 668/1173] [Iter 1841] [G_Loss 11.102147] [D_Loss 0.000244]\n",
      "train [Epoch 1/3] [Batch 669/1173] [Iter 1842] [G_Loss 11.148222] [D_Loss 0.000745]\n",
      "train [Epoch 1/3] [Batch 670/1173] [Iter 1843] [G_Loss 11.105672] [D_Loss 0.000044]\n",
      "train [Epoch 1/3] [Batch 671/1173] [Iter 1844] [G_Loss 11.092190] [D_Loss 0.000487]\n",
      "train [Epoch 1/3] [Batch 672/1173] [Iter 1845] [G_Loss 11.128157] [D_Loss 0.000395]\n",
      "train [Epoch 1/3] [Batch 673/1173] [Iter 1846] [G_Loss 11.049923] [D_Loss 0.000194]\n",
      "train [Epoch 1/3] [Batch 674/1173] [Iter 1847] [G_Loss 11.065603] [D_Loss 0.000077]\n",
      "train [Epoch 1/3] [Batch 675/1173] [Iter 1848] [G_Loss 11.080627] [D_Loss 0.000048]\n",
      "train [Epoch 1/3] [Batch 676/1173] [Iter 1849] [G_Loss 11.124347] [D_Loss 0.000080]\n",
      "train [Epoch 1/3] [Batch 677/1173] [Iter 1850] [G_Loss 11.177392] [D_Loss 0.000062]\n",
      "train [Epoch 1/3] [Batch 678/1173] [Iter 1851] [G_Loss 11.257170] [D_Loss 0.001541]\n",
      "train [Epoch 1/3] [Batch 679/1173] [Iter 1852] [G_Loss 11.176682] [D_Loss 0.000223]\n",
      "train [Epoch 1/3] [Batch 680/1173] [Iter 1853] [G_Loss 11.230186] [D_Loss 0.000052]\n",
      "train [Epoch 1/3] [Batch 681/1173] [Iter 1854] [G_Loss 11.250985] [D_Loss 0.000393]\n",
      "train [Epoch 1/3] [Batch 682/1173] [Iter 1855] [G_Loss 11.262630] [D_Loss 0.000298]\n",
      "train [Epoch 1/3] [Batch 683/1173] [Iter 1856] [G_Loss 11.271912] [D_Loss 0.000057]\n",
      "train [Epoch 1/3] [Batch 684/1173] [Iter 1857] [G_Loss 11.282394] [D_Loss 0.000032]\n",
      "train [Epoch 1/3] [Batch 685/1173] [Iter 1858] [G_Loss 11.304592] [D_Loss 0.000104]\n",
      "train [Epoch 1/3] [Batch 686/1173] [Iter 1859] [G_Loss 11.317045] [D_Loss 0.000029]\n",
      "train [Epoch 1/3] [Batch 687/1173] [Iter 1860] [G_Loss 11.370582] [D_Loss 0.000063]\n",
      "train [Epoch 1/3] [Batch 688/1173] [Iter 1861] [G_Loss 11.393800] [D_Loss 0.000089]\n",
      "train [Epoch 1/3] [Batch 689/1173] [Iter 1862] [G_Loss 11.403049] [D_Loss 0.000101]\n",
      "train [Epoch 1/3] [Batch 690/1173] [Iter 1863] [G_Loss 11.479690] [D_Loss 0.000068]\n",
      "train [Epoch 1/3] [Batch 691/1173] [Iter 1864] [G_Loss 11.505217] [D_Loss 0.000076]\n",
      "train [Epoch 1/3] [Batch 692/1173] [Iter 1865] [G_Loss 11.510434] [D_Loss 0.000084]\n",
      "train [Epoch 1/3] [Batch 693/1173] [Iter 1866] [G_Loss 11.508767] [D_Loss 0.000155]\n",
      "train [Epoch 1/3] [Batch 694/1173] [Iter 1867] [G_Loss 11.516306] [D_Loss 0.000114]\n",
      "train [Epoch 1/3] [Batch 695/1173] [Iter 1868] [G_Loss 11.510944] [D_Loss 0.000221]\n",
      "train [Epoch 1/3] [Batch 696/1173] [Iter 1869] [G_Loss 11.524194] [D_Loss 0.000033]\n",
      "train [Epoch 1/3] [Batch 697/1173] [Iter 1870] [G_Loss 11.553104] [D_Loss 0.000092]\n",
      "train [Epoch 1/3] [Batch 698/1173] [Iter 1871] [G_Loss 11.567873] [D_Loss 0.000129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 1/3] [Batch 699/1173] [Iter 1872] [G_Loss 11.582821] [D_Loss 0.000221]\n",
      "train [Epoch 1/3] [Batch 700/1173] [Iter 1873] [G_Loss 11.582496] [D_Loss 0.000024]\n",
      "train [Epoch 1/3] [Batch 701/1173] [Iter 1874] [G_Loss 11.571067] [D_Loss 0.000055]\n",
      "train [Epoch 1/3] [Batch 702/1173] [Iter 1875] [G_Loss 11.605400] [D_Loss 0.000031]\n",
      "train [Epoch 1/3] [Batch 703/1173] [Iter 1876] [G_Loss 11.607629] [D_Loss 0.000056]\n",
      "train [Epoch 1/3] [Batch 704/1173] [Iter 1877] [G_Loss 11.650537] [D_Loss 0.000158]\n",
      "train [Epoch 1/3] [Batch 705/1173] [Iter 1878] [G_Loss 11.656939] [D_Loss 0.000185]\n",
      "train [Epoch 1/3] [Batch 706/1173] [Iter 1879] [G_Loss 11.680908] [D_Loss 0.000098]\n",
      "train [Epoch 1/3] [Batch 707/1173] [Iter 1880] [G_Loss 11.691020] [D_Loss 0.000471]\n",
      "train [Epoch 1/3] [Batch 708/1173] [Iter 1881] [G_Loss 11.704496] [D_Loss 0.006186]\n",
      "train [Epoch 1/3] [Batch 709/1173] [Iter 1882] [G_Loss 11.166601] [D_Loss 0.000102]\n",
      "train [Epoch 1/3] [Batch 710/1173] [Iter 1883] [G_Loss 10.897225] [D_Loss 0.000069]\n",
      "train [Epoch 1/3] [Batch 711/1173] [Iter 1884] [G_Loss 10.788467] [D_Loss 0.000079]\n",
      "train [Epoch 1/3] [Batch 712/1173] [Iter 1885] [G_Loss 10.827058] [D_Loss 0.000309]\n",
      "train [Epoch 1/3] [Batch 713/1173] [Iter 1886] [G_Loss 11.049111] [D_Loss 0.000082]\n",
      "train [Epoch 1/3] [Batch 714/1173] [Iter 1887] [G_Loss 10.775735] [D_Loss 0.000976]\n",
      "train [Epoch 1/3] [Batch 715/1173] [Iter 1888] [G_Loss 10.937491] [D_Loss 0.000285]\n",
      "train [Epoch 1/3] [Batch 716/1173] [Iter 1889] [G_Loss 11.041616] [D_Loss 0.000049]\n",
      "train [Epoch 1/3] [Batch 717/1173] [Iter 1890] [G_Loss 10.845108] [D_Loss 0.000081]\n",
      "train [Epoch 1/3] [Batch 718/1173] [Iter 1891] [G_Loss 11.042473] [D_Loss 0.001267]\n",
      "train [Epoch 1/3] [Batch 719/1173] [Iter 1892] [G_Loss 10.940994] [D_Loss 0.001987]\n",
      "train [Epoch 1/3] [Batch 720/1173] [Iter 1893] [G_Loss 10.319742] [D_Loss 0.001117]\n",
      "train [Epoch 1/3] [Batch 721/1173] [Iter 1894] [G_Loss 10.327445] [D_Loss 0.000129]\n",
      "train [Epoch 1/3] [Batch 722/1173] [Iter 1895] [G_Loss 10.210269] [D_Loss 0.000349]\n",
      "train [Epoch 1/3] [Batch 723/1173] [Iter 1896] [G_Loss 10.080924] [D_Loss 0.000179]\n",
      "train [Epoch 1/3] [Batch 724/1173] [Iter 1897] [G_Loss 10.167130] [D_Loss 0.000117]\n",
      "train [Epoch 1/3] [Batch 725/1173] [Iter 1898] [G_Loss 10.311438] [D_Loss 0.000172]\n",
      "train [Epoch 1/3] [Batch 726/1173] [Iter 1899] [G_Loss 10.259615] [D_Loss 0.000077]\n",
      "train [Epoch 1/3] [Batch 727/1173] [Iter 1900] [G_Loss 10.321701] [D_Loss 0.000466]\n",
      "train [Epoch 1/3] [Batch 728/1173] [Iter 1901] [G_Loss 10.386304] [D_Loss 0.000167]\n",
      "train [Epoch 1/3] [Batch 729/1173] [Iter 1902] [G_Loss 10.463612] [D_Loss 0.000164]\n",
      "train [Epoch 1/3] [Batch 730/1173] [Iter 1903] [G_Loss 10.517302] [D_Loss 0.000067]\n",
      "train [Epoch 1/3] [Batch 731/1173] [Iter 1904] [G_Loss 10.567021] [D_Loss 0.001440]\n",
      "train [Epoch 1/3] [Batch 732/1173] [Iter 1905] [G_Loss 10.543348] [D_Loss 0.000102]\n",
      "train [Epoch 1/3] [Batch 733/1173] [Iter 1906] [G_Loss 10.554349] [D_Loss 0.000813]\n",
      "train [Epoch 1/3] [Batch 734/1173] [Iter 1907] [G_Loss 10.590591] [D_Loss 0.000454]\n",
      "train [Epoch 1/3] [Batch 735/1173] [Iter 1908] [G_Loss 10.622351] [D_Loss 0.000202]\n",
      "train [Epoch 1/3] [Batch 736/1173] [Iter 1909] [G_Loss 10.664453] [D_Loss 0.000128]\n",
      "train [Epoch 1/3] [Batch 737/1173] [Iter 1910] [G_Loss 10.702323] [D_Loss 0.000174]\n",
      "train [Epoch 1/3] [Batch 738/1173] [Iter 1911] [G_Loss 10.758459] [D_Loss 0.002264]\n",
      "train [Epoch 1/3] [Batch 739/1173] [Iter 1912] [G_Loss 10.579019] [D_Loss 0.000041]\n",
      "train [Epoch 1/3] [Batch 740/1173] [Iter 1913] [G_Loss 10.519442] [D_Loss 0.000114]\n",
      "train [Epoch 1/3] [Batch 741/1173] [Iter 1914] [G_Loss 10.509818] [D_Loss 0.000087]\n",
      "train [Epoch 1/3] [Batch 742/1173] [Iter 1915] [G_Loss 10.535179] [D_Loss 0.000085]\n",
      "train [Epoch 1/3] [Batch 743/1173] [Iter 1916] [G_Loss 10.584882] [D_Loss 0.000135]\n",
      "train [Epoch 1/3] [Batch 744/1173] [Iter 1917] [G_Loss 10.637619] [D_Loss 0.000061]\n",
      "train [Epoch 1/3] [Batch 745/1173] [Iter 1918] [G_Loss 10.677455] [D_Loss 0.000046]\n",
      "train [Epoch 1/3] [Batch 746/1173] [Iter 1919] [G_Loss 10.736662] [D_Loss 0.000056]\n",
      "train [Epoch 1/3] [Batch 747/1173] [Iter 1920] [G_Loss 10.781513] [D_Loss 0.000406]\n",
      "train [Epoch 1/3] [Batch 748/1173] [Iter 1921] [G_Loss 10.813910] [D_Loss 0.000086]\n",
      "train [Epoch 1/3] [Batch 749/1173] [Iter 1922] [G_Loss 10.884579] [D_Loss 0.000058]\n",
      "train [Epoch 1/3] [Batch 750/1173] [Iter 1923] [G_Loss 10.921014] [D_Loss 0.000114]\n",
      "train [Epoch 1/3] [Batch 751/1173] [Iter 1924] [G_Loss 10.952999] [D_Loss 0.000063]\n",
      "train [Epoch 1/3] [Batch 752/1173] [Iter 1925] [G_Loss 10.996613] [D_Loss 0.000042]\n",
      "train [Epoch 1/3] [Batch 753/1173] [Iter 1926] [G_Loss 11.040255] [D_Loss 0.000150]\n",
      "train [Epoch 1/3] [Batch 754/1173] [Iter 1927] [G_Loss 11.044470] [D_Loss 0.000152]\n",
      "train [Epoch 1/3] [Batch 755/1173] [Iter 1928] [G_Loss 11.092108] [D_Loss 0.000026]\n",
      "train [Epoch 1/3] [Batch 756/1173] [Iter 1929] [G_Loss 11.116675] [D_Loss 0.000298]\n",
      "train [Epoch 1/3] [Batch 757/1173] [Iter 1930] [G_Loss 11.140167] [D_Loss 0.000240]\n",
      "train [Epoch 1/3] [Batch 758/1173] [Iter 1931] [G_Loss 11.166916] [D_Loss 0.000027]\n",
      "train [Epoch 1/3] [Batch 759/1173] [Iter 1932] [G_Loss 11.202070] [D_Loss 0.000066]\n",
      "train [Epoch 1/3] [Batch 760/1173] [Iter 1933] [G_Loss 11.231374] [D_Loss 0.000047]\n",
      "train [Epoch 1/3] [Batch 761/1173] [Iter 1934] [G_Loss 11.250067] [D_Loss 0.000020]\n",
      "train [Epoch 1/3] [Batch 762/1173] [Iter 1935] [G_Loss 11.281897] [D_Loss 0.000538]\n",
      "train [Epoch 1/3] [Batch 763/1173] [Iter 1936] [G_Loss 11.316978] [D_Loss 0.000045]\n",
      "train [Epoch 1/3] [Batch 764/1173] [Iter 1937] [G_Loss 11.364901] [D_Loss 0.000022]\n",
      "train [Epoch 1/3] [Batch 765/1173] [Iter 1938] [G_Loss 11.400064] [D_Loss 0.000286]\n",
      "train [Epoch 1/3] [Batch 766/1173] [Iter 1939] [G_Loss 11.387400] [D_Loss 0.000122]\n",
      "train [Epoch 1/3] [Batch 767/1173] [Iter 1940] [G_Loss 11.419287] [D_Loss 0.000236]\n",
      "train [Epoch 1/3] [Batch 768/1173] [Iter 1941] [G_Loss 11.378359] [D_Loss 0.000085]\n",
      "train [Epoch 1/3] [Batch 769/1173] [Iter 1942] [G_Loss 11.423418] [D_Loss 0.000079]\n",
      "train [Epoch 1/3] [Batch 770/1173] [Iter 1943] [G_Loss 11.455848] [D_Loss 0.000056]\n",
      "train [Epoch 1/3] [Batch 771/1173] [Iter 1944] [G_Loss 11.460370] [D_Loss 0.000050]\n",
      "train [Epoch 1/3] [Batch 772/1173] [Iter 1945] [G_Loss 11.504112] [D_Loss 0.002405]\n",
      "train [Epoch 1/3] [Batch 773/1173] [Iter 1946] [G_Loss 11.526113] [D_Loss 0.000213]\n",
      "train [Epoch 1/3] [Batch 774/1173] [Iter 1947] [G_Loss 11.569913] [D_Loss 0.000079]\n",
      "train [Epoch 1/3] [Batch 775/1173] [Iter 1948] [G_Loss 11.575946] [D_Loss 0.000078]\n",
      "train [Epoch 1/3] [Batch 776/1173] [Iter 1949] [G_Loss 11.597412] [D_Loss 0.000649]\n",
      "train [Epoch 1/3] [Batch 777/1173] [Iter 1950] [G_Loss 11.567440] [D_Loss 0.000048]\n",
      "train [Epoch 1/3] [Batch 778/1173] [Iter 1951] [G_Loss 11.573210] [D_Loss 0.000242]\n",
      "train [Epoch 1/3] [Batch 779/1173] [Iter 1952] [G_Loss 11.596830] [D_Loss 0.000027]\n",
      "train [Epoch 1/3] [Batch 780/1173] [Iter 1953] [G_Loss 11.605891] [D_Loss 0.000080]\n",
      "train [Epoch 1/3] [Batch 781/1173] [Iter 1954] [G_Loss 11.615825] [D_Loss 0.002880]\n",
      "train [Epoch 1/3] [Batch 782/1173] [Iter 1955] [G_Loss 11.382021] [D_Loss 0.000042]\n",
      "train [Epoch 1/3] [Batch 783/1173] [Iter 1956] [G_Loss 11.339108] [D_Loss 0.000823]\n",
      "train [Epoch 1/3] [Batch 784/1173] [Iter 1957] [G_Loss 11.236208] [D_Loss 0.000053]\n",
      "train [Epoch 1/3] [Batch 785/1173] [Iter 1958] [G_Loss 11.204563] [D_Loss 0.000067]\n",
      "train [Epoch 1/3] [Batch 786/1173] [Iter 1959] [G_Loss 11.156606] [D_Loss 0.000031]\n",
      "train [Epoch 1/3] [Batch 787/1173] [Iter 1960] [G_Loss 11.173628] [D_Loss 0.000029]\n",
      "train [Epoch 1/3] [Batch 788/1173] [Iter 1961] [G_Loss 11.250158] [D_Loss 0.000072]\n",
      "train [Epoch 1/3] [Batch 789/1173] [Iter 1962] [G_Loss 11.316804] [D_Loss 0.000143]\n",
      "train [Epoch 1/3] [Batch 790/1173] [Iter 1963] [G_Loss 11.285587] [D_Loss 0.000122]\n",
      "train [Epoch 1/3] [Batch 791/1173] [Iter 1964] [G_Loss 11.279990] [D_Loss 0.000021]\n",
      "train [Epoch 1/3] [Batch 792/1173] [Iter 1965] [G_Loss 11.285642] [D_Loss 0.000067]\n",
      "train [Epoch 1/3] [Batch 793/1173] [Iter 1966] [G_Loss 11.318311] [D_Loss 0.000093]\n",
      "train [Epoch 1/3] [Batch 794/1173] [Iter 1967] [G_Loss 11.391458] [D_Loss 0.000056]\n",
      "train [Epoch 1/3] [Batch 795/1173] [Iter 1968] [G_Loss 11.444898] [D_Loss 0.000072]\n",
      "train [Epoch 1/3] [Batch 796/1173] [Iter 1969] [G_Loss 11.384088] [D_Loss 0.000033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 1/3] [Batch 797/1173] [Iter 1970] [G_Loss 11.412600] [D_Loss 0.000228]\n",
      "train [Epoch 1/3] [Batch 798/1173] [Iter 1971] [G_Loss 11.419907] [D_Loss 0.001152]\n",
      "train [Epoch 1/3] [Batch 799/1173] [Iter 1972] [G_Loss 11.277432] [D_Loss 0.000074]\n",
      "train [Epoch 1/3] [Batch 800/1173] [Iter 1973] [G_Loss 11.238069] [D_Loss 0.000017]\n",
      "train [Epoch 1/3] [Batch 801/1173] [Iter 1974] [G_Loss 11.277532] [D_Loss 0.000023]\n",
      "train [Epoch 1/3] [Batch 802/1173] [Iter 1975] [G_Loss 11.235924] [D_Loss 0.000052]\n",
      "train [Epoch 1/3] [Batch 803/1173] [Iter 1976] [G_Loss 11.266617] [D_Loss 0.000028]\n",
      "train [Epoch 1/3] [Batch 804/1173] [Iter 1977] [G_Loss 11.221653] [D_Loss 0.000041]\n",
      "train [Epoch 1/3] [Batch 805/1173] [Iter 1978] [G_Loss 11.266057] [D_Loss 0.000461]\n",
      "train [Epoch 1/3] [Batch 806/1173] [Iter 1979] [G_Loss 11.278640] [D_Loss 0.000031]\n",
      "train [Epoch 1/3] [Batch 807/1173] [Iter 1980] [G_Loss 11.317263] [D_Loss 0.000073]\n",
      "train [Epoch 1/3] [Batch 808/1173] [Iter 1981] [G_Loss 11.343349] [D_Loss 0.000126]\n",
      "train [Epoch 1/3] [Batch 809/1173] [Iter 1982] [G_Loss 11.403453] [D_Loss 0.000143]\n",
      "train [Epoch 1/3] [Batch 810/1173] [Iter 1983] [G_Loss 11.383931] [D_Loss 0.000017]\n",
      "train [Epoch 1/3] [Batch 811/1173] [Iter 1984] [G_Loss 11.399725] [D_Loss 0.002248]\n",
      "train [Epoch 1/3] [Batch 812/1173] [Iter 1985] [G_Loss 11.232113] [D_Loss 0.000053]\n",
      "train [Epoch 1/3] [Batch 813/1173] [Iter 1986] [G_Loss 11.154214] [D_Loss 0.000079]\n",
      "train [Epoch 1/3] [Batch 814/1173] [Iter 1987] [G_Loss 11.161228] [D_Loss 0.000107]\n",
      "train [Epoch 1/3] [Batch 815/1173] [Iter 1988] [G_Loss 11.164431] [D_Loss 0.000030]\n",
      "train [Epoch 1/3] [Batch 816/1173] [Iter 1989] [G_Loss 11.193837] [D_Loss 0.000028]\n",
      "train [Epoch 1/3] [Batch 817/1173] [Iter 1990] [G_Loss 11.203253] [D_Loss 0.000121]\n",
      "train [Epoch 1/3] [Batch 818/1173] [Iter 1991] [G_Loss 11.240096] [D_Loss 0.000316]\n",
      "train [Epoch 1/3] [Batch 819/1173] [Iter 1992] [G_Loss 11.336435] [D_Loss 0.000032]\n",
      "train [Epoch 1/3] [Batch 820/1173] [Iter 1993] [G_Loss 11.307885] [D_Loss 0.000073]\n",
      "train [Epoch 1/3] [Batch 821/1173] [Iter 1994] [G_Loss 11.297215] [D_Loss 0.000024]\n",
      "train [Epoch 1/3] [Batch 822/1173] [Iter 1995] [G_Loss 11.313391] [D_Loss 0.000197]\n",
      "train [Epoch 1/3] [Batch 823/1173] [Iter 1996] [G_Loss 11.391093] [D_Loss 0.000094]\n",
      "train [Epoch 1/3] [Batch 824/1173] [Iter 1997] [G_Loss 11.485730] [D_Loss 0.000038]\n",
      "train [Epoch 1/3] [Batch 825/1173] [Iter 1998] [G_Loss 11.375392] [D_Loss 0.000018]\n",
      "train [Epoch 1/3] [Batch 826/1173] [Iter 1999] [G_Loss 11.446111] [D_Loss 0.000065]\n",
      "train [Epoch 1/3] [Batch 827/1173] [Iter 2000] [G_Loss 11.520864] [D_Loss 0.000068]\n",
      "train [Epoch 1/3] [Batch 828/1173] [Iter 2001] [G_Loss 11.422688] [D_Loss 0.000057]\n",
      "train [Epoch 1/3] [Batch 829/1173] [Iter 2002] [G_Loss 11.404329] [D_Loss 0.000059]\n",
      "train [Epoch 1/3] [Batch 830/1173] [Iter 2003] [G_Loss 11.414204] [D_Loss 0.000026]\n",
      "train [Epoch 1/3] [Batch 831/1173] [Iter 2004] [G_Loss 11.457412] [D_Loss 0.000072]\n",
      "train [Epoch 1/3] [Batch 832/1173] [Iter 2005] [G_Loss 11.547486] [D_Loss 0.000046]\n",
      "train [Epoch 1/3] [Batch 833/1173] [Iter 2006] [G_Loss 11.576359] [D_Loss 0.000092]\n",
      "train [Epoch 1/3] [Batch 834/1173] [Iter 2007] [G_Loss 11.513240] [D_Loss 0.000052]\n",
      "train [Epoch 1/3] [Batch 835/1173] [Iter 2008] [G_Loss 11.525541] [D_Loss 0.000121]\n",
      "train [Epoch 1/3] [Batch 836/1173] [Iter 2009] [G_Loss 11.600910] [D_Loss 0.000176]\n",
      "train [Epoch 1/3] [Batch 837/1173] [Iter 2010] [G_Loss 11.668545] [D_Loss 0.000033]\n",
      "train [Epoch 1/3] [Batch 838/1173] [Iter 2011] [G_Loss 11.637539] [D_Loss 0.000057]\n",
      "train [Epoch 1/3] [Batch 839/1173] [Iter 2012] [G_Loss 11.617196] [D_Loss 0.000043]\n",
      "train [Epoch 1/3] [Batch 840/1173] [Iter 2013] [G_Loss 11.673742] [D_Loss 0.001099]\n",
      "train [Epoch 1/3] [Batch 841/1173] [Iter 2014] [G_Loss 11.603860] [D_Loss 0.000032]\n",
      "train [Epoch 1/3] [Batch 842/1173] [Iter 2015] [G_Loss 11.594038] [D_Loss 0.000180]\n",
      "train [Epoch 1/3] [Batch 843/1173] [Iter 2016] [G_Loss 11.558062] [D_Loss 0.000915]\n",
      "train [Epoch 1/3] [Batch 844/1173] [Iter 2017] [G_Loss 11.469934] [D_Loss 0.000037]\n",
      "train [Epoch 1/3] [Batch 845/1173] [Iter 2018] [G_Loss 11.422353] [D_Loss 0.030231]\n",
      "train [Epoch 1/3] [Batch 846/1173] [Iter 2019] [G_Loss 8.438331] [D_Loss 0.000230]\n",
      "train [Epoch 1/3] [Batch 847/1173] [Iter 2020] [G_Loss 7.279202] [D_Loss 0.000712]\n",
      "train [Epoch 1/3] [Batch 848/1173] [Iter 2021] [G_Loss 7.446461] [D_Loss 0.001155]\n",
      "train [Epoch 1/3] [Batch 849/1173] [Iter 2022] [G_Loss 7.749935] [D_Loss 0.000874]\n",
      "train [Epoch 1/3] [Batch 850/1173] [Iter 2023] [G_Loss 8.512461] [D_Loss 0.000233]\n",
      "train [Epoch 1/3] [Batch 851/1173] [Iter 2024] [G_Loss 8.991491] [D_Loss 0.000168]\n",
      "train [Epoch 1/3] [Batch 852/1173] [Iter 2025] [G_Loss 9.270343] [D_Loss 0.000168]\n",
      "train [Epoch 1/3] [Batch 853/1173] [Iter 2026] [G_Loss 9.561788] [D_Loss 0.000126]\n",
      "train [Epoch 1/3] [Batch 854/1173] [Iter 2027] [G_Loss 9.711423] [D_Loss 0.000809]\n",
      "train [Epoch 1/3] [Batch 855/1173] [Iter 2028] [G_Loss 9.884460] [D_Loss 0.000081]\n",
      "train [Epoch 1/3] [Batch 856/1173] [Iter 2029] [G_Loss 10.038872] [D_Loss 0.000151]\n",
      "train [Epoch 1/3] [Batch 857/1173] [Iter 2030] [G_Loss 10.096354] [D_Loss 0.000442]\n",
      "train [Epoch 1/3] [Batch 858/1173] [Iter 2031] [G_Loss 10.108570] [D_Loss 0.000182]\n",
      "train [Epoch 1/3] [Batch 859/1173] [Iter 2032] [G_Loss 10.166716] [D_Loss 0.000045]\n",
      "train [Epoch 1/3] [Batch 860/1173] [Iter 2033] [G_Loss 10.224289] [D_Loss 0.000106]\n",
      "train [Epoch 1/3] [Batch 861/1173] [Iter 2034] [G_Loss 10.299379] [D_Loss 0.000090]\n",
      "train [Epoch 1/3] [Batch 862/1173] [Iter 2035] [G_Loss 10.372765] [D_Loss 0.000079]\n",
      "train [Epoch 1/3] [Batch 863/1173] [Iter 2036] [G_Loss 10.438952] [D_Loss 0.000081]\n",
      "train [Epoch 1/3] [Batch 864/1173] [Iter 2037] [G_Loss 10.456517] [D_Loss 0.000039]\n",
      "train [Epoch 1/3] [Batch 865/1173] [Iter 2038] [G_Loss 10.510058] [D_Loss 0.000045]\n",
      "train [Epoch 1/3] [Batch 866/1173] [Iter 2039] [G_Loss 10.574125] [D_Loss 0.000040]\n",
      "train [Epoch 1/3] [Batch 867/1173] [Iter 2040] [G_Loss 10.645147] [D_Loss 0.000290]\n",
      "train [Epoch 1/3] [Batch 868/1173] [Iter 2041] [G_Loss 10.641680] [D_Loss 0.000084]\n",
      "train [Epoch 1/3] [Batch 869/1173] [Iter 2042] [G_Loss 10.645183] [D_Loss 0.000028]\n",
      "train [Epoch 1/3] [Batch 870/1173] [Iter 2043] [G_Loss 10.682890] [D_Loss 0.000213]\n",
      "train [Epoch 1/3] [Batch 871/1173] [Iter 2044] [G_Loss 10.719160] [D_Loss 0.000039]\n",
      "train [Epoch 1/3] [Batch 872/1173] [Iter 2045] [G_Loss 10.768591] [D_Loss 0.000563]\n",
      "train [Epoch 1/3] [Batch 873/1173] [Iter 2046] [G_Loss 10.782861] [D_Loss 0.000066]\n",
      "train [Epoch 1/3] [Batch 874/1173] [Iter 2047] [G_Loss 10.780186] [D_Loss 0.000762]\n",
      "train [Epoch 1/3] [Batch 875/1173] [Iter 2048] [G_Loss 10.834709] [D_Loss 0.000219]\n",
      "train [Epoch 1/3] [Batch 876/1173] [Iter 2049] [G_Loss 10.858674] [D_Loss 0.000106]\n",
      "train [Epoch 1/3] [Batch 877/1173] [Iter 2050] [G_Loss 10.892650] [D_Loss 0.000171]\n",
      "train [Epoch 1/3] [Batch 878/1173] [Iter 2051] [G_Loss 10.882524] [D_Loss 0.000141]\n",
      "train [Epoch 1/3] [Batch 879/1173] [Iter 2052] [G_Loss 10.923585] [D_Loss 0.000084]\n",
      "train [Epoch 1/3] [Batch 880/1173] [Iter 2053] [G_Loss 10.948646] [D_Loss 0.000064]\n",
      "train [Epoch 1/3] [Batch 881/1173] [Iter 2054] [G_Loss 10.976575] [D_Loss 0.000103]\n",
      "train [Epoch 1/3] [Batch 882/1173] [Iter 2055] [G_Loss 11.003832] [D_Loss 0.000099]\n",
      "train [Epoch 1/3] [Batch 883/1173] [Iter 2056] [G_Loss 11.013416] [D_Loss 0.000341]\n",
      "train [Epoch 1/3] [Batch 884/1173] [Iter 2057] [G_Loss 11.026677] [D_Loss 0.000042]\n",
      "train [Epoch 1/3] [Batch 885/1173] [Iter 2058] [G_Loss 11.068661] [D_Loss 0.000038]\n",
      "train [Epoch 1/3] [Batch 886/1173] [Iter 2059] [G_Loss 11.086627] [D_Loss 0.000067]\n",
      "train [Epoch 1/3] [Batch 887/1173] [Iter 2060] [G_Loss 11.124569] [D_Loss 0.000043]\n",
      "train [Epoch 1/3] [Batch 888/1173] [Iter 2061] [G_Loss 11.151216] [D_Loss 0.000136]\n",
      "train [Epoch 1/3] [Batch 889/1173] [Iter 2062] [G_Loss 11.180147] [D_Loss 0.000081]\n",
      "train [Epoch 1/3] [Batch 890/1173] [Iter 2063] [G_Loss 11.199053] [D_Loss 0.000253]\n",
      "train [Epoch 1/3] [Batch 891/1173] [Iter 2064] [G_Loss 11.217136] [D_Loss 0.000097]\n",
      "train [Epoch 1/3] [Batch 892/1173] [Iter 2065] [G_Loss 11.236085] [D_Loss 0.000057]\n",
      "train [Epoch 1/3] [Batch 893/1173] [Iter 2066] [G_Loss 11.236421] [D_Loss 0.000080]\n",
      "train [Epoch 1/3] [Batch 894/1173] [Iter 2067] [G_Loss 11.286261] [D_Loss 0.000029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 1/3] [Batch 895/1173] [Iter 2068] [G_Loss 11.332424] [D_Loss 0.000177]\n",
      "train [Epoch 1/3] [Batch 896/1173] [Iter 2069] [G_Loss 11.286084] [D_Loss 0.000019]\n",
      "train [Epoch 1/3] [Batch 897/1173] [Iter 2070] [G_Loss 11.323198] [D_Loss 0.000129]\n",
      "train [Epoch 1/3] [Batch 898/1173] [Iter 2071] [G_Loss 11.311257] [D_Loss 0.000148]\n",
      "train [Epoch 1/3] [Batch 899/1173] [Iter 2072] [G_Loss 11.333354] [D_Loss 0.000041]\n",
      "train [Epoch 1/3] [Batch 900/1173] [Iter 2073] [G_Loss 11.314360] [D_Loss 0.000039]\n",
      "train [Epoch 1/3] [Batch 901/1173] [Iter 2074] [G_Loss 11.335933] [D_Loss 0.000212]\n",
      "train [Epoch 1/3] [Batch 902/1173] [Iter 2075] [G_Loss 11.365798] [D_Loss 0.000132]\n",
      "train [Epoch 1/3] [Batch 903/1173] [Iter 2076] [G_Loss 11.358996] [D_Loss 0.000205]\n",
      "train [Epoch 1/3] [Batch 904/1173] [Iter 2077] [G_Loss 11.382367] [D_Loss 0.000081]\n",
      "train [Epoch 1/3] [Batch 905/1173] [Iter 2078] [G_Loss 11.378015] [D_Loss 0.000095]\n",
      "train [Epoch 1/3] [Batch 906/1173] [Iter 2079] [G_Loss 11.401086] [D_Loss 0.000025]\n",
      "train [Epoch 1/3] [Batch 907/1173] [Iter 2080] [G_Loss 11.420405] [D_Loss 0.000041]\n",
      "train [Epoch 1/3] [Batch 908/1173] [Iter 2081] [G_Loss 11.407761] [D_Loss 0.000467]\n",
      "train [Epoch 1/3] [Batch 909/1173] [Iter 2082] [G_Loss 11.440519] [D_Loss 0.000156]\n",
      "train [Epoch 1/3] [Batch 910/1173] [Iter 2083] [G_Loss 11.421169] [D_Loss 0.000330]\n",
      "train [Epoch 1/3] [Batch 911/1173] [Iter 2084] [G_Loss 11.445048] [D_Loss 0.000040]\n",
      "train [Epoch 1/3] [Batch 912/1173] [Iter 2085] [G_Loss 11.450388] [D_Loss 0.000090]\n",
      "train [Epoch 1/3] [Batch 913/1173] [Iter 2086] [G_Loss 11.468021] [D_Loss 0.001447]\n",
      "train [Epoch 1/3] [Batch 914/1173] [Iter 2087] [G_Loss 11.206388] [D_Loss 0.000054]\n",
      "train [Epoch 1/3] [Batch 915/1173] [Iter 2088] [G_Loss 11.087129] [D_Loss 0.000024]\n",
      "train [Epoch 1/3] [Batch 916/1173] [Iter 2089] [G_Loss 11.022654] [D_Loss 0.000081]\n",
      "train [Epoch 1/3] [Batch 917/1173] [Iter 2090] [G_Loss 11.046011] [D_Loss 0.000078]\n",
      "train [Epoch 1/3] [Batch 918/1173] [Iter 2091] [G_Loss 11.040912] [D_Loss 0.000095]\n",
      "train [Epoch 1/3] [Batch 919/1173] [Iter 2092] [G_Loss 11.075864] [D_Loss 0.000019]\n",
      "train [Epoch 1/3] [Batch 920/1173] [Iter 2093] [G_Loss 11.057204] [D_Loss 0.000040]\n",
      "train [Epoch 1/3] [Batch 921/1173] [Iter 2094] [G_Loss 11.081894] [D_Loss 0.000186]\n",
      "train [Epoch 1/3] [Batch 922/1173] [Iter 2095] [G_Loss 11.113774] [D_Loss 0.000030]\n",
      "train [Epoch 1/3] [Batch 923/1173] [Iter 2096] [G_Loss 11.081008] [D_Loss 0.000045]\n",
      "train [Epoch 1/3] [Batch 924/1173] [Iter 2097] [G_Loss 11.112275] [D_Loss 0.000027]\n",
      "train [Epoch 1/3] [Batch 925/1173] [Iter 2098] [G_Loss 11.199429] [D_Loss 0.000019]\n",
      "train [Epoch 1/3] [Batch 926/1173] [Iter 2099] [G_Loss 11.166860] [D_Loss 0.000022]\n",
      "train [Epoch 1/3] [Batch 927/1173] [Iter 2100] [G_Loss 11.203509] [D_Loss 0.000038]\n",
      "train [Epoch 1/3] [Batch 928/1173] [Iter 2101] [G_Loss 11.185437] [D_Loss 0.000125]\n",
      "train [Epoch 1/3] [Batch 929/1173] [Iter 2102] [G_Loss 11.217176] [D_Loss 0.000119]\n",
      "train [Epoch 1/3] [Batch 930/1173] [Iter 2103] [G_Loss 11.215676] [D_Loss 0.000039]\n",
      "train [Epoch 1/3] [Batch 931/1173] [Iter 2104] [G_Loss 11.216780] [D_Loss 0.000244]\n",
      "train [Epoch 1/3] [Batch 932/1173] [Iter 2105] [G_Loss 11.241878] [D_Loss 0.000047]\n",
      "train [Epoch 1/3] [Batch 933/1173] [Iter 2106] [G_Loss 11.238647] [D_Loss 0.000079]\n",
      "train [Epoch 1/3] [Batch 934/1173] [Iter 2107] [G_Loss 11.245872] [D_Loss 0.000022]\n",
      "train [Epoch 1/3] [Batch 935/1173] [Iter 2108] [G_Loss 11.263481] [D_Loss 0.000184]\n",
      "train [Epoch 1/3] [Batch 936/1173] [Iter 2109] [G_Loss 11.316144] [D_Loss 0.000144]\n",
      "train [Epoch 1/3] [Batch 937/1173] [Iter 2110] [G_Loss 11.279232] [D_Loss 0.000197]\n",
      "train [Epoch 1/3] [Batch 938/1173] [Iter 2111] [G_Loss 11.298183] [D_Loss 0.000189]\n",
      "train [Epoch 1/3] [Batch 939/1173] [Iter 2112] [G_Loss 11.305178] [D_Loss 0.000044]\n",
      "train [Epoch 1/3] [Batch 940/1173] [Iter 2113] [G_Loss 11.258071] [D_Loss 0.000048]\n",
      "train [Epoch 1/3] [Batch 941/1173] [Iter 2114] [G_Loss 11.280461] [D_Loss 0.000027]\n",
      "train [Epoch 1/3] [Batch 942/1173] [Iter 2115] [G_Loss 11.262012] [D_Loss 0.000019]\n",
      "train [Epoch 1/3] [Batch 943/1173] [Iter 2116] [G_Loss 11.231473] [D_Loss 0.000028]\n",
      "train [Epoch 1/3] [Batch 944/1173] [Iter 2117] [G_Loss 11.208567] [D_Loss 0.000519]\n",
      "train [Epoch 1/3] [Batch 945/1173] [Iter 2118] [G_Loss 11.180943] [D_Loss 0.000119]\n",
      "train [Epoch 1/3] [Batch 946/1173] [Iter 2119] [G_Loss 11.176105] [D_Loss 0.000192]\n",
      "train [Epoch 1/3] [Batch 947/1173] [Iter 2120] [G_Loss 11.179519] [D_Loss 0.000023]\n",
      "train [Epoch 1/3] [Batch 948/1173] [Iter 2121] [G_Loss 11.143511] [D_Loss 0.006235]\n",
      "train [Epoch 1/3] [Batch 949/1173] [Iter 2122] [G_Loss 10.156990] [D_Loss 0.000050]\n",
      "train [Epoch 1/3] [Batch 950/1173] [Iter 2123] [G_Loss 9.707463] [D_Loss 0.000328]\n",
      "train [Epoch 1/3] [Batch 951/1173] [Iter 2124] [G_Loss 9.432935] [D_Loss 0.004089]\n",
      "train [Epoch 1/3] [Batch 952/1173] [Iter 2125] [G_Loss 9.341119] [D_Loss 0.000115]\n",
      "train [Epoch 1/3] [Batch 953/1173] [Iter 2126] [G_Loss 9.352390] [D_Loss 0.000103]\n",
      "train [Epoch 1/3] [Batch 954/1173] [Iter 2127] [G_Loss 9.457094] [D_Loss 0.000165]\n",
      "train [Epoch 1/3] [Batch 955/1173] [Iter 2128] [G_Loss 9.474603] [D_Loss 0.000089]\n",
      "train [Epoch 1/3] [Batch 956/1173] [Iter 2129] [G_Loss 9.594543] [D_Loss 0.000095]\n",
      "train [Epoch 1/3] [Batch 957/1173] [Iter 2130] [G_Loss 9.632535] [D_Loss 0.000081]\n",
      "train [Epoch 1/3] [Batch 958/1173] [Iter 2131] [G_Loss 9.679096] [D_Loss 0.000102]\n",
      "train [Epoch 1/3] [Batch 959/1173] [Iter 2132] [G_Loss 9.732139] [D_Loss 0.000066]\n",
      "train [Epoch 1/3] [Batch 960/1173] [Iter 2133] [G_Loss 9.750004] [D_Loss 0.000070]\n",
      "train [Epoch 1/3] [Batch 961/1173] [Iter 2134] [G_Loss 9.784647] [D_Loss 0.000078]\n",
      "train [Epoch 1/3] [Batch 962/1173] [Iter 2135] [G_Loss 9.815255] [D_Loss 0.000094]\n",
      "train [Epoch 1/3] [Batch 963/1173] [Iter 2136] [G_Loss 9.740440] [D_Loss 0.000110]\n",
      "train [Epoch 1/3] [Batch 964/1173] [Iter 2137] [G_Loss 9.791944] [D_Loss 0.000094]\n",
      "train [Epoch 1/3] [Batch 965/1173] [Iter 2138] [G_Loss 9.767792] [D_Loss 0.000618]\n",
      "train [Epoch 1/3] [Batch 966/1173] [Iter 2139] [G_Loss 9.768641] [D_Loss 0.000062]\n",
      "train [Epoch 1/3] [Batch 967/1173] [Iter 2140] [G_Loss 9.810871] [D_Loss 0.000109]\n",
      "train [Epoch 1/3] [Batch 968/1173] [Iter 2141] [G_Loss 9.837159] [D_Loss 0.000147]\n",
      "train [Epoch 1/3] [Batch 969/1173] [Iter 2142] [G_Loss 9.810788] [D_Loss 0.000091]\n",
      "train [Epoch 1/3] [Batch 970/1173] [Iter 2143] [G_Loss 9.855654] [D_Loss 0.000080]\n",
      "train [Epoch 1/3] [Batch 971/1173] [Iter 2144] [G_Loss 9.905718] [D_Loss 0.000056]\n",
      "train [Epoch 1/3] [Batch 972/1173] [Iter 2145] [G_Loss 9.860287] [D_Loss 0.000075]\n",
      "train [Epoch 1/3] [Batch 973/1173] [Iter 2146] [G_Loss 9.850393] [D_Loss 0.000058]\n",
      "train [Epoch 1/3] [Batch 974/1173] [Iter 2147] [G_Loss 9.812155] [D_Loss 0.000084]\n",
      "train [Epoch 1/3] [Batch 975/1173] [Iter 2148] [G_Loss 9.797562] [D_Loss 0.000063]\n",
      "train [Epoch 1/3] [Batch 976/1173] [Iter 2149] [G_Loss 9.787421] [D_Loss 0.000090]\n",
      "train [Epoch 1/3] [Batch 977/1173] [Iter 2150] [G_Loss 9.780412] [D_Loss 0.000066]\n",
      "train [Epoch 1/3] [Batch 978/1173] [Iter 2151] [G_Loss 9.760865] [D_Loss 0.000145]\n",
      "train [Epoch 1/3] [Batch 979/1173] [Iter 2152] [G_Loss 9.778172] [D_Loss 0.000064]\n",
      "train [Epoch 1/3] [Batch 980/1173] [Iter 2153] [G_Loss 9.769642] [D_Loss 0.000059]\n",
      "train [Epoch 1/3] [Batch 981/1173] [Iter 2154] [G_Loss 9.772787] [D_Loss 0.000064]\n",
      "train [Epoch 1/3] [Batch 982/1173] [Iter 2155] [G_Loss 9.759501] [D_Loss 0.000269]\n",
      "train [Epoch 1/3] [Batch 983/1173] [Iter 2156] [G_Loss 9.758220] [D_Loss 0.000177]\n",
      "train [Epoch 1/3] [Batch 984/1173] [Iter 2157] [G_Loss 9.741391] [D_Loss 0.000081]\n",
      "train [Epoch 1/3] [Batch 985/1173] [Iter 2158] [G_Loss 9.742393] [D_Loss 0.000097]\n",
      "train [Epoch 1/3] [Batch 986/1173] [Iter 2159] [G_Loss 9.727578] [D_Loss 0.000082]\n",
      "train [Epoch 1/3] [Batch 987/1173] [Iter 2160] [G_Loss 9.686732] [D_Loss 0.000096]\n",
      "train [Epoch 1/3] [Batch 988/1173] [Iter 2161] [G_Loss 9.693895] [D_Loss 0.000091]\n",
      "train [Epoch 1/3] [Batch 989/1173] [Iter 2162] [G_Loss 9.691299] [D_Loss 0.000151]\n",
      "train [Epoch 1/3] [Batch 990/1173] [Iter 2163] [G_Loss 9.658392] [D_Loss 0.000141]\n",
      "train [Epoch 1/3] [Batch 991/1173] [Iter 2164] [G_Loss 9.649440] [D_Loss 0.000097]\n",
      "train [Epoch 1/3] [Batch 992/1173] [Iter 2165] [G_Loss 9.611144] [D_Loss 0.061865]\n",
      "train [Epoch 1/3] [Batch 993/1173] [Iter 2166] [G_Loss 5.157907] [D_Loss 0.005843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 1/3] [Batch 994/1173] [Iter 2167] [G_Loss 5.223188] [D_Loss 0.005436]\n",
      "train [Epoch 1/3] [Batch 995/1173] [Iter 2168] [G_Loss 7.339851] [D_Loss 0.000656]\n",
      "train [Epoch 1/3] [Batch 996/1173] [Iter 2169] [G_Loss 8.662070] [D_Loss 0.000263]\n",
      "train [Epoch 1/3] [Batch 997/1173] [Iter 2170] [G_Loss 9.309362] [D_Loss 0.000172]\n",
      "train [Epoch 1/3] [Batch 998/1173] [Iter 2171] [G_Loss 9.729731] [D_Loss 0.000087]\n",
      "train [Epoch 1/3] [Batch 999/1173] [Iter 2172] [G_Loss 9.862939] [D_Loss 0.000135]\n",
      "train [Epoch 1/3] [Batch 1000/1173] [Iter 2173] [G_Loss 9.873323] [D_Loss 0.000072]\n",
      "train [Epoch 1/3] [Batch 1001/1173] [Iter 2174] [G_Loss 9.806083] [D_Loss 0.000069]\n",
      "train [Epoch 1/3] [Batch 1002/1173] [Iter 2175] [G_Loss 9.788856] [D_Loss 0.000062]\n",
      "train [Epoch 1/3] [Batch 1003/1173] [Iter 2176] [G_Loss 9.757921] [D_Loss 0.000105]\n",
      "train [Epoch 1/3] [Batch 1004/1173] [Iter 2177] [G_Loss 9.712677] [D_Loss 0.000112]\n",
      "train [Epoch 1/3] [Batch 1005/1173] [Iter 2178] [G_Loss 9.701090] [D_Loss 0.000074]\n",
      "train [Epoch 1/3] [Batch 1006/1173] [Iter 2179] [G_Loss 9.691559] [D_Loss 0.000074]\n",
      "train [Epoch 1/3] [Batch 1007/1173] [Iter 2180] [G_Loss 9.664678] [D_Loss 0.000169]\n",
      "train [Epoch 1/3] [Batch 1008/1173] [Iter 2181] [G_Loss 9.627254] [D_Loss 0.000077]\n",
      "train [Epoch 1/3] [Batch 1009/1173] [Iter 2182] [G_Loss 9.619331] [D_Loss 0.000071]\n",
      "train [Epoch 1/3] [Batch 1010/1173] [Iter 2183] [G_Loss 9.574352] [D_Loss 0.000093]\n",
      "train [Epoch 1/3] [Batch 1011/1173] [Iter 2184] [G_Loss 9.543175] [D_Loss 0.000263]\n",
      "train [Epoch 1/3] [Batch 1012/1173] [Iter 2185] [G_Loss 9.515450] [D_Loss 0.000124]\n",
      "train [Epoch 1/3] [Batch 1013/1173] [Iter 2186] [G_Loss 9.492860] [D_Loss 0.001121]\n",
      "train [Epoch 1/3] [Batch 1014/1173] [Iter 2187] [G_Loss 9.458318] [D_Loss 0.000234]\n",
      "train [Epoch 1/3] [Batch 1015/1173] [Iter 2188] [G_Loss 9.481543] [D_Loss 0.000114]\n",
      "train [Epoch 1/3] [Batch 1016/1173] [Iter 2189] [G_Loss 9.468708] [D_Loss 0.000083]\n",
      "train [Epoch 1/3] [Batch 1017/1173] [Iter 2190] [G_Loss 9.457334] [D_Loss 0.000100]\n",
      "train [Epoch 1/3] [Batch 1018/1173] [Iter 2191] [G_Loss 9.453505] [D_Loss 0.000093]\n",
      "train [Epoch 1/3] [Batch 1019/1173] [Iter 2192] [G_Loss 9.464270] [D_Loss 0.000168]\n",
      "train [Epoch 1/3] [Batch 1020/1173] [Iter 2193] [G_Loss 9.449697] [D_Loss 0.000207]\n",
      "train [Epoch 1/3] [Batch 1021/1173] [Iter 2194] [G_Loss 9.456347] [D_Loss 0.000233]\n",
      "train [Epoch 1/3] [Batch 1022/1173] [Iter 2195] [G_Loss 9.442409] [D_Loss 0.000093]\n",
      "train [Epoch 1/3] [Batch 1023/1173] [Iter 2196] [G_Loss 9.438949] [D_Loss 0.000496]\n",
      "train [Epoch 1/3] [Batch 1024/1173] [Iter 2197] [G_Loss 9.441864] [D_Loss 0.000200]\n",
      "train [Epoch 1/3] [Batch 1025/1173] [Iter 2198] [G_Loss 9.451597] [D_Loss 0.000090]\n",
      "train [Epoch 1/3] [Batch 1026/1173] [Iter 2199] [G_Loss 9.454621] [D_Loss 0.000087]\n",
      "train [Epoch 1/3] [Batch 1027/1173] [Iter 2200] [G_Loss 9.486224] [D_Loss 0.000109]\n",
      "train [Epoch 1/3] [Batch 1028/1173] [Iter 2201] [G_Loss 9.500566] [D_Loss 0.000102]\n",
      "train [Epoch 1/3] [Batch 1029/1173] [Iter 2202] [G_Loss 9.530842] [D_Loss 0.000080]\n",
      "train [Epoch 1/3] [Batch 1030/1173] [Iter 2203] [G_Loss 9.547224] [D_Loss 0.000099]\n",
      "train [Epoch 1/3] [Batch 1031/1173] [Iter 2204] [G_Loss 9.589122] [D_Loss 0.000081]\n",
      "train [Epoch 1/3] [Batch 1032/1173] [Iter 2205] [G_Loss 9.589425] [D_Loss 0.000080]\n",
      "train [Epoch 1/3] [Batch 1033/1173] [Iter 2206] [G_Loss 9.620973] [D_Loss 0.000088]\n",
      "train [Epoch 1/3] [Batch 1034/1173] [Iter 2207] [G_Loss 9.632930] [D_Loss 0.000106]\n",
      "train [Epoch 1/3] [Batch 1035/1173] [Iter 2208] [G_Loss 9.651579] [D_Loss 0.000076]\n",
      "train [Epoch 1/3] [Batch 1036/1173] [Iter 2209] [G_Loss 9.677692] [D_Loss 0.000065]\n",
      "train [Epoch 1/3] [Batch 1037/1173] [Iter 2210] [G_Loss 9.678554] [D_Loss 0.000066]\n",
      "train [Epoch 1/3] [Batch 1038/1173] [Iter 2211] [G_Loss 9.699263] [D_Loss 0.000065]\n",
      "train [Epoch 1/3] [Batch 1039/1173] [Iter 2212] [G_Loss 9.700536] [D_Loss 0.000138]\n",
      "train [Epoch 1/3] [Batch 1040/1173] [Iter 2213] [G_Loss 9.699160] [D_Loss 0.000103]\n",
      "train [Epoch 1/3] [Batch 1041/1173] [Iter 2214] [G_Loss 9.687700] [D_Loss 0.000083]\n",
      "train [Epoch 1/3] [Batch 1042/1173] [Iter 2215] [G_Loss 9.674609] [D_Loss 0.000080]\n",
      "train [Epoch 1/3] [Batch 1043/1173] [Iter 2216] [G_Loss 9.666181] [D_Loss 0.000069]\n",
      "train [Epoch 1/3] [Batch 1044/1173] [Iter 2217] [G_Loss 9.660250] [D_Loss 0.000227]\n",
      "train [Epoch 1/3] [Batch 1045/1173] [Iter 2218] [G_Loss 9.663671] [D_Loss 0.000297]\n",
      "train [Epoch 1/3] [Batch 1046/1173] [Iter 2219] [G_Loss 9.649161] [D_Loss 0.000072]\n",
      "train [Epoch 1/3] [Batch 1047/1173] [Iter 2220] [G_Loss 9.629189] [D_Loss 0.000092]\n",
      "train [Epoch 1/3] [Batch 1048/1173] [Iter 2221] [G_Loss 9.635331] [D_Loss 0.000088]\n",
      "train [Epoch 1/3] [Batch 1049/1173] [Iter 2222] [G_Loss 9.659602] [D_Loss 0.000077]\n",
      "train [Epoch 1/3] [Batch 1050/1173] [Iter 2223] [G_Loss 9.728881] [D_Loss 0.000067]\n",
      "train [Epoch 1/3] [Batch 1051/1173] [Iter 2224] [G_Loss 9.741417] [D_Loss 0.000148]\n",
      "train [Epoch 1/3] [Batch 1052/1173] [Iter 2225] [G_Loss 9.804717] [D_Loss 0.000098]\n",
      "train [Epoch 1/3] [Batch 1053/1173] [Iter 2226] [G_Loss 9.851789] [D_Loss 0.000079]\n",
      "train [Epoch 1/3] [Batch 1054/1173] [Iter 2227] [G_Loss 9.846167] [D_Loss 0.000130]\n",
      "train [Epoch 1/3] [Batch 1055/1173] [Iter 2228] [G_Loss 9.899238] [D_Loss 0.001319]\n",
      "train [Epoch 1/3] [Batch 1056/1173] [Iter 2229] [G_Loss 9.862082] [D_Loss 0.000083]\n",
      "train [Epoch 1/3] [Batch 1057/1173] [Iter 2230] [G_Loss 9.803927] [D_Loss 0.007600]\n",
      "train [Epoch 1/3] [Batch 1058/1173] [Iter 2231] [G_Loss 9.831897] [D_Loss 0.000066]\n",
      "train [Epoch 1/3] [Batch 1059/1173] [Iter 2232] [G_Loss 9.848180] [D_Loss 0.000075]\n",
      "train [Epoch 1/3] [Batch 1060/1173] [Iter 2233] [G_Loss 9.867495] [D_Loss 0.000073]\n",
      "train [Epoch 1/3] [Batch 1061/1173] [Iter 2234] [G_Loss 9.934991] [D_Loss 0.000052]\n",
      "train [Epoch 1/3] [Batch 1062/1173] [Iter 2235] [G_Loss 9.989456] [D_Loss 0.000067]\n",
      "train [Epoch 1/3] [Batch 1063/1173] [Iter 2236] [G_Loss 9.996886] [D_Loss 0.000052]\n",
      "train [Epoch 1/3] [Batch 1064/1173] [Iter 2237] [G_Loss 10.004409] [D_Loss 0.000056]\n",
      "train [Epoch 1/3] [Batch 1065/1173] [Iter 2238] [G_Loss 10.030483] [D_Loss 0.000049]\n",
      "train [Epoch 1/3] [Batch 1066/1173] [Iter 2239] [G_Loss 10.055213] [D_Loss 0.001247]\n",
      "train [Epoch 1/3] [Batch 1067/1173] [Iter 2240] [G_Loss 10.064964] [D_Loss 0.000270]\n",
      "train [Epoch 1/3] [Batch 1068/1173] [Iter 2241] [G_Loss 10.090078] [D_Loss 0.000050]\n",
      "train [Epoch 1/3] [Batch 1069/1173] [Iter 2242] [G_Loss 10.130213] [D_Loss 0.000106]\n",
      "train [Epoch 1/3] [Batch 1070/1173] [Iter 2243] [G_Loss 10.183189] [D_Loss 0.000064]\n",
      "train [Epoch 1/3] [Batch 1071/1173] [Iter 2244] [G_Loss 10.184918] [D_Loss 0.000049]\n",
      "train [Epoch 1/3] [Batch 1072/1173] [Iter 2245] [G_Loss 10.222880] [D_Loss 0.000059]\n",
      "train [Epoch 1/3] [Batch 1073/1173] [Iter 2246] [G_Loss 10.277590] [D_Loss 0.000196]\n",
      "train [Epoch 1/3] [Batch 1074/1173] [Iter 2247] [G_Loss 10.269650] [D_Loss 0.000052]\n",
      "train [Epoch 1/3] [Batch 1075/1173] [Iter 2248] [G_Loss 10.283626] [D_Loss 0.000125]\n",
      "train [Epoch 1/3] [Batch 1076/1173] [Iter 2249] [G_Loss 10.327668] [D_Loss 0.000041]\n",
      "train [Epoch 1/3] [Batch 1077/1173] [Iter 2250] [G_Loss 10.305211] [D_Loss 0.000042]\n",
      "train [Epoch 1/3] [Batch 1078/1173] [Iter 2251] [G_Loss 10.381047] [D_Loss 0.000052]\n",
      "train [Epoch 1/3] [Batch 1079/1173] [Iter 2252] [G_Loss 10.394328] [D_Loss 0.000032]\n",
      "train [Epoch 1/3] [Batch 1080/1173] [Iter 2253] [G_Loss 10.416492] [D_Loss 0.000038]\n",
      "train [Epoch 1/3] [Batch 1081/1173] [Iter 2254] [G_Loss 10.410297] [D_Loss 0.000046]\n",
      "train [Epoch 1/3] [Batch 1082/1173] [Iter 2255] [G_Loss 10.447001] [D_Loss 0.000042]\n",
      "train [Epoch 1/3] [Batch 1083/1173] [Iter 2256] [G_Loss 10.480841] [D_Loss 0.000130]\n",
      "train [Epoch 1/3] [Batch 1084/1173] [Iter 2257] [G_Loss 10.463791] [D_Loss 0.000035]\n",
      "train [Epoch 1/3] [Batch 1085/1173] [Iter 2258] [G_Loss 10.505078] [D_Loss 0.000067]\n",
      "train [Epoch 1/3] [Batch 1086/1173] [Iter 2259] [G_Loss 10.491323] [D_Loss 0.000195]\n",
      "train [Epoch 1/3] [Batch 1087/1173] [Iter 2260] [G_Loss 10.495172] [D_Loss 0.000059]\n",
      "train [Epoch 1/3] [Batch 1088/1173] [Iter 2261] [G_Loss 10.494127] [D_Loss 0.000059]\n",
      "train [Epoch 1/3] [Batch 1089/1173] [Iter 2262] [G_Loss 10.538693] [D_Loss 0.000041]\n",
      "train [Epoch 1/3] [Batch 1090/1173] [Iter 2263] [G_Loss 10.573444] [D_Loss 0.000058]\n",
      "train [Epoch 1/3] [Batch 1091/1173] [Iter 2264] [G_Loss 10.556001] [D_Loss 0.000028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 1/3] [Batch 1092/1173] [Iter 2265] [G_Loss 10.589442] [D_Loss 0.000295]\n",
      "train [Epoch 1/3] [Batch 1093/1173] [Iter 2266] [G_Loss 10.606748] [D_Loss 0.000223]\n",
      "train [Epoch 1/3] [Batch 1094/1173] [Iter 2267] [G_Loss 10.615192] [D_Loss 0.000149]\n",
      "train [Epoch 1/3] [Batch 1095/1173] [Iter 2268] [G_Loss 10.606319] [D_Loss 0.000064]\n",
      "train [Epoch 1/3] [Batch 1096/1173] [Iter 2269] [G_Loss 10.633642] [D_Loss 0.000052]\n",
      "train [Epoch 1/3] [Batch 1097/1173] [Iter 2270] [G_Loss 10.671684] [D_Loss 0.000080]\n",
      "train [Epoch 1/3] [Batch 1098/1173] [Iter 2271] [G_Loss 10.643364] [D_Loss 0.000607]\n",
      "train [Epoch 1/3] [Batch 1099/1173] [Iter 2272] [G_Loss 10.653602] [D_Loss 0.000032]\n",
      "train [Epoch 1/3] [Batch 1100/1173] [Iter 2273] [G_Loss 10.643132] [D_Loss 0.000050]\n",
      "train [Epoch 1/3] [Batch 1101/1173] [Iter 2274] [G_Loss 10.683167] [D_Loss 0.000261]\n",
      "train [Epoch 1/3] [Batch 1102/1173] [Iter 2275] [G_Loss 10.641891] [D_Loss 0.000082]\n",
      "train [Epoch 1/3] [Batch 1103/1173] [Iter 2276] [G_Loss 10.640592] [D_Loss 0.000037]\n",
      "train [Epoch 1/3] [Batch 1104/1173] [Iter 2277] [G_Loss 10.683412] [D_Loss 0.000057]\n",
      "train [Epoch 1/3] [Batch 1105/1173] [Iter 2278] [G_Loss 10.736931] [D_Loss 0.004994]\n",
      "train [Epoch 1/3] [Batch 1106/1173] [Iter 2279] [G_Loss 10.058387] [D_Loss 0.000097]\n",
      "train [Epoch 1/3] [Batch 1107/1173] [Iter 2280] [G_Loss 9.834478] [D_Loss 0.000073]\n",
      "train [Epoch 1/3] [Batch 1108/1173] [Iter 2281] [G_Loss 9.744674] [D_Loss 0.000076]\n",
      "train [Epoch 1/3] [Batch 1109/1173] [Iter 2282] [G_Loss 9.639399] [D_Loss 0.000068]\n",
      "train [Epoch 1/3] [Batch 1110/1173] [Iter 2283] [G_Loss 9.596395] [D_Loss 0.000104]\n",
      "train [Epoch 1/3] [Batch 1111/1173] [Iter 2284] [G_Loss 9.635990] [D_Loss 0.000100]\n",
      "train [Epoch 1/3] [Batch 1112/1173] [Iter 2285] [G_Loss 9.668703] [D_Loss 0.000078]\n",
      "train [Epoch 1/3] [Batch 1113/1173] [Iter 2286] [G_Loss 9.664371] [D_Loss 0.000074]\n",
      "train [Epoch 1/3] [Batch 1114/1173] [Iter 2287] [G_Loss 9.705385] [D_Loss 0.000071]\n",
      "train [Epoch 1/3] [Batch 1115/1173] [Iter 2288] [G_Loss 9.789081] [D_Loss 0.000066]\n",
      "train [Epoch 1/3] [Batch 1116/1173] [Iter 2289] [G_Loss 9.764782] [D_Loss 0.000059]\n",
      "train [Epoch 1/3] [Batch 1117/1173] [Iter 2290] [G_Loss 9.828440] [D_Loss 0.000108]\n",
      "train [Epoch 1/3] [Batch 1118/1173] [Iter 2291] [G_Loss 9.863036] [D_Loss 0.000064]\n",
      "train [Epoch 1/3] [Batch 1119/1173] [Iter 2292] [G_Loss 9.889680] [D_Loss 0.000061]\n",
      "train [Epoch 1/3] [Batch 1120/1173] [Iter 2293] [G_Loss 9.921968] [D_Loss 0.000056]\n",
      "train [Epoch 1/3] [Batch 1121/1173] [Iter 2294] [G_Loss 9.987650] [D_Loss 0.000057]\n",
      "train [Epoch 1/3] [Batch 1122/1173] [Iter 2295] [G_Loss 9.950302] [D_Loss 0.000072]\n",
      "train [Epoch 1/3] [Batch 1123/1173] [Iter 2296] [G_Loss 10.066905] [D_Loss 0.000044]\n",
      "train [Epoch 1/3] [Batch 1124/1173] [Iter 2297] [G_Loss 10.192471] [D_Loss 0.000052]\n",
      "train [Epoch 1/3] [Batch 1125/1173] [Iter 2298] [G_Loss 10.167937] [D_Loss 0.000059]\n",
      "train [Epoch 1/3] [Batch 1126/1173] [Iter 2299] [G_Loss 10.107695] [D_Loss 0.000045]\n",
      "train [Epoch 1/3] [Batch 1127/1173] [Iter 2300] [G_Loss 10.190348] [D_Loss 0.000202]\n",
      "train [Epoch 1/3] [Batch 1128/1173] [Iter 2301] [G_Loss 10.286997] [D_Loss 0.000035]\n",
      "train [Epoch 1/3] [Batch 1129/1173] [Iter 2302] [G_Loss 10.244607] [D_Loss 0.000059]\n",
      "train [Epoch 1/3] [Batch 1130/1173] [Iter 2303] [G_Loss 10.207900] [D_Loss 0.000488]\n",
      "train [Epoch 1/3] [Batch 1131/1173] [Iter 2304] [G_Loss 10.323438] [D_Loss 0.000048]\n",
      "train [Epoch 1/3] [Batch 1132/1173] [Iter 2305] [G_Loss 10.361991] [D_Loss 0.000142]\n",
      "train [Epoch 1/3] [Batch 1133/1173] [Iter 2306] [G_Loss 10.339931] [D_Loss 0.000047]\n",
      "train [Epoch 1/3] [Batch 1134/1173] [Iter 2307] [G_Loss 10.329386] [D_Loss 0.000041]\n",
      "train [Epoch 1/3] [Batch 1135/1173] [Iter 2308] [G_Loss 10.351876] [D_Loss 0.000049]\n",
      "train [Epoch 1/3] [Batch 1136/1173] [Iter 2309] [G_Loss 10.367877] [D_Loss 0.000038]\n",
      "train [Epoch 1/3] [Batch 1137/1173] [Iter 2310] [G_Loss 10.331116] [D_Loss 0.000086]\n",
      "train [Epoch 1/3] [Batch 1138/1173] [Iter 2311] [G_Loss 10.391981] [D_Loss 0.005815]\n",
      "train [Epoch 1/3] [Batch 1139/1173] [Iter 2312] [G_Loss 9.470902] [D_Loss 0.000085]\n",
      "train [Epoch 1/3] [Batch 1140/1173] [Iter 2313] [G_Loss 9.056976] [D_Loss 0.000119]\n",
      "train [Epoch 1/3] [Batch 1141/1173] [Iter 2314] [G_Loss 8.915843] [D_Loss 0.000179]\n",
      "train [Epoch 1/3] [Batch 1142/1173] [Iter 2315] [G_Loss 8.879636] [D_Loss 0.000171]\n",
      "train [Epoch 1/3] [Batch 1143/1173] [Iter 2316] [G_Loss 8.908390] [D_Loss 0.000153]\n",
      "train [Epoch 1/3] [Batch 1144/1173] [Iter 2317] [G_Loss 8.963309] [D_Loss 0.000130]\n",
      "train [Epoch 1/3] [Batch 1145/1173] [Iter 2318] [G_Loss 9.029025] [D_Loss 0.000131]\n",
      "train [Epoch 1/3] [Batch 1146/1173] [Iter 2319] [G_Loss 9.122460] [D_Loss 0.000134]\n",
      "train [Epoch 1/3] [Batch 1147/1173] [Iter 2320] [G_Loss 9.182390] [D_Loss 0.000106]\n",
      "train [Epoch 1/3] [Batch 1148/1173] [Iter 2321] [G_Loss 9.270417] [D_Loss 0.000142]\n",
      "train [Epoch 1/3] [Batch 1149/1173] [Iter 2322] [G_Loss 9.339634] [D_Loss 0.000095]\n",
      "train [Epoch 1/3] [Batch 1150/1173] [Iter 2323] [G_Loss 9.387609] [D_Loss 0.000139]\n",
      "train [Epoch 1/3] [Batch 1151/1173] [Iter 2324] [G_Loss 9.419261] [D_Loss 0.000085]\n",
      "train [Epoch 1/3] [Batch 1152/1173] [Iter 2325] [G_Loss 9.493905] [D_Loss 0.001107]\n",
      "train [Epoch 1/3] [Batch 1153/1173] [Iter 2326] [G_Loss 9.301229] [D_Loss 0.011137]\n",
      "train [Epoch 1/3] [Batch 1154/1173] [Iter 2327] [G_Loss 9.270438] [D_Loss 0.000109]\n",
      "train [Epoch 1/3] [Batch 1155/1173] [Iter 2328] [G_Loss 9.258348] [D_Loss 0.000140]\n",
      "train [Epoch 1/3] [Batch 1156/1173] [Iter 2329] [G_Loss 9.313019] [D_Loss 0.000113]\n",
      "train [Epoch 1/3] [Batch 1157/1173] [Iter 2330] [G_Loss 9.346702] [D_Loss 0.000121]\n",
      "train [Epoch 1/3] [Batch 1158/1173] [Iter 2331] [G_Loss 9.389981] [D_Loss 0.000104]\n",
      "train [Epoch 1/3] [Batch 1159/1173] [Iter 2332] [G_Loss 9.430503] [D_Loss 0.000090]\n",
      "train [Epoch 1/3] [Batch 1160/1173] [Iter 2333] [G_Loss 9.492014] [D_Loss 0.000089]\n",
      "train [Epoch 1/3] [Batch 1161/1173] [Iter 2334] [G_Loss 9.519816] [D_Loss 0.000136]\n",
      "train [Epoch 1/3] [Batch 1162/1173] [Iter 2335] [G_Loss 9.602814] [D_Loss 0.000074]\n",
      "train [Epoch 1/3] [Batch 1163/1173] [Iter 2336] [G_Loss 9.596279] [D_Loss 0.000143]\n",
      "train [Epoch 1/3] [Batch 1164/1173] [Iter 2337] [G_Loss 9.680785] [D_Loss 0.000086]\n",
      "train [Epoch 1/3] [Batch 1165/1173] [Iter 2338] [G_Loss 9.705841] [D_Loss 0.000066]\n",
      "train [Epoch 1/3] [Batch 1166/1173] [Iter 2339] [G_Loss 9.743358] [D_Loss 0.000088]\n",
      "train [Epoch 1/3] [Batch 1167/1173] [Iter 2340] [G_Loss 9.756222] [D_Loss 0.000061]\n",
      "train [Epoch 1/3] [Batch 1168/1173] [Iter 2341] [G_Loss 9.834036] [D_Loss 0.000080]\n",
      "train [Epoch 1/3] [Batch 1169/1173] [Iter 2342] [G_Loss 9.853919] [D_Loss 0.000055]\n",
      "train [Epoch 1/3] [Batch 1170/1173] [Iter 2343] [G_Loss 9.948312] [D_Loss 0.000064]\n",
      "train [Epoch 1/3] [Batch 1171/1173] [Iter 2344] [G_Loss 9.918686] [D_Loss 0.000066]\n",
      "train [Epoch 1/3] [Batch 1172/1173] [Iter 2345] [G_Loss 9.937645] [D_Loss 0.000062]\n",
      "val [Epoch 1/3] [Batch 0/31] [Iter 31] [G_Loss 10.002075] [D_Loss 0.000054]\n",
      "val [Epoch 1/3] [Batch 1/31] [Iter 32] [G_Loss 9.965855] [D_Loss 0.000065]\n",
      "val [Epoch 1/3] [Batch 2/31] [Iter 33] [G_Loss 10.003845] [D_Loss 0.000140]\n",
      "val [Epoch 1/3] [Batch 3/31] [Iter 34] [G_Loss 9.989968] [D_Loss 0.000048]\n",
      "val [Epoch 1/3] [Batch 4/31] [Iter 35] [G_Loss 9.949782] [D_Loss 0.000058]\n",
      "val [Epoch 1/3] [Batch 5/31] [Iter 36] [G_Loss 9.974925] [D_Loss 0.000052]\n",
      "val [Epoch 1/3] [Batch 6/31] [Iter 37] [G_Loss 9.944498] [D_Loss 0.000057]\n",
      "val [Epoch 1/3] [Batch 7/31] [Iter 38] [G_Loss 9.956419] [D_Loss 0.000067]\n",
      "val [Epoch 1/3] [Batch 8/31] [Iter 39] [G_Loss 9.998055] [D_Loss 0.000068]\n",
      "val [Epoch 1/3] [Batch 9/31] [Iter 40] [G_Loss 9.981877] [D_Loss 0.000052]\n",
      "val [Epoch 1/3] [Batch 10/31] [Iter 41] [G_Loss 9.984249] [D_Loss 0.000047]\n",
      "val [Epoch 1/3] [Batch 11/31] [Iter 42] [G_Loss 9.999871] [D_Loss 0.000075]\n",
      "val [Epoch 1/3] [Batch 12/31] [Iter 43] [G_Loss 9.974040] [D_Loss 0.000149]\n",
      "val [Epoch 1/3] [Batch 13/31] [Iter 44] [G_Loss 9.965060] [D_Loss 0.000076]\n",
      "val [Epoch 1/3] [Batch 14/31] [Iter 45] [G_Loss 9.977963] [D_Loss 0.000050]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 490.00 MiB (GPU 0; 15.90 GiB total capacity; 13.27 GiB already allocated; 425.81 MiB free; 1.57 GiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a152329c3349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_gan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/cs231n-proj/src/scripts/train_gan.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(generator, discriminator, save)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalidation_rate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mval_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0msave_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../../outputs/gan/val_losses.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs231n-proj/src/scripts/train_gan.py\u001b[0m in \u001b[0;36mtrain_batches\u001b[0;34m(epoch, generator, discriminator, dataloader, train, save)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# Generate images using the generator, find loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs231n-proj/src/scripts/train_gan.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleakyRelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleakyRelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleakyRelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 490.00 MiB (GPU 0; 15.90 GiB total capacity; 13.27 GiB already allocated; 425.81 MiB free; 1.57 GiB cached)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_gan import *\n",
    "\n",
    "train_model(generator, discriminator, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.33s)\n",
      "creating index...\n",
      "index created!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "train [Epoch 0/1] [Batch 0/1172] [Iter 0] [G_Loss 1.397669] [D_Loss 1.125656]\n",
      "train [Epoch 0/1] [Batch 1/1172] [Iter 1] [G_Loss 16.180323] [D_Loss 0.451497]\n",
      "train [Epoch 0/1] [Batch 2/1172] [Iter 2] [G_Loss 11.407255] [D_Loss 0.147787]\n",
      "train [Epoch 0/1] [Batch 3/1172] [Iter 3] [G_Loss 8.657849] [D_Loss 0.244820]\n",
      "train [Epoch 0/1] [Batch 4/1172] [Iter 4] [G_Loss 9.909332] [D_Loss 0.278587]\n",
      "train [Epoch 0/1] [Batch 5/1172] [Iter 5] [G_Loss 9.758183] [D_Loss 0.287825]\n",
      "train [Epoch 0/1] [Batch 6/1172] [Iter 6] [G_Loss 8.782508] [D_Loss 0.230669]\n",
      "train [Epoch 0/1] [Batch 7/1172] [Iter 7] [G_Loss 5.963762] [D_Loss 0.282871]\n",
      "train [Epoch 0/1] [Batch 8/1172] [Iter 8] [G_Loss 5.055948] [D_Loss 0.272283]\n",
      "train [Epoch 0/1] [Batch 9/1172] [Iter 9] [G_Loss 5.423881] [D_Loss 0.346383]\n",
      "train [Epoch 0/1] [Batch 10/1172] [Iter 10] [G_Loss 3.378678] [D_Loss 0.325677]\n",
      "train [Epoch 0/1] [Batch 11/1172] [Iter 11] [G_Loss 4.813469] [D_Loss 0.398779]\n",
      "train [Epoch 0/1] [Batch 12/1172] [Iter 12] [G_Loss 2.308883] [D_Loss 0.516824]\n",
      "train [Epoch 0/1] [Batch 13/1172] [Iter 13] [G_Loss 4.711646] [D_Loss 0.562106]\n",
      "train [Epoch 0/1] [Batch 14/1172] [Iter 14] [G_Loss 2.236791] [D_Loss 0.444174]\n",
      "train [Epoch 0/1] [Batch 15/1172] [Iter 15] [G_Loss 2.145907] [D_Loss 0.374163]\n",
      "train [Epoch 0/1] [Batch 16/1172] [Iter 16] [G_Loss 3.316895] [D_Loss 0.352136]\n",
      "train [Epoch 0/1] [Batch 17/1172] [Iter 17] [G_Loss 2.499424] [D_Loss 0.325050]\n",
      "train [Epoch 0/1] [Batch 18/1172] [Iter 18] [G_Loss 3.007478] [D_Loss 0.529615]\n",
      "train [Epoch 0/1] [Batch 19/1172] [Iter 19] [G_Loss 2.924825] [D_Loss 0.480989]\n",
      "train [Epoch 0/1] [Batch 20/1172] [Iter 20] [G_Loss 1.922325] [D_Loss 0.496903]\n",
      "train [Epoch 0/1] [Batch 21/1172] [Iter 21] [G_Loss 7.490127] [D_Loss 2.364472]\n",
      "train [Epoch 0/1] [Batch 22/1172] [Iter 22] [G_Loss 0.464144] [D_Loss 1.625406]\n",
      "train [Epoch 0/1] [Batch 23/1172] [Iter 23] [G_Loss 1.876053] [D_Loss 0.398793]\n",
      "train [Epoch 0/1] [Batch 24/1172] [Iter 24] [G_Loss 3.433034] [D_Loss 0.450303]\n",
      "train [Epoch 0/1] [Batch 25/1172] [Iter 25] [G_Loss 3.470065] [D_Loss 0.367744]\n",
      "train [Epoch 0/1] [Batch 26/1172] [Iter 26] [G_Loss 2.828980] [D_Loss 0.287867]\n",
      "train [Epoch 0/1] [Batch 27/1172] [Iter 27] [G_Loss 2.789396] [D_Loss 0.304225]\n",
      "train [Epoch 0/1] [Batch 28/1172] [Iter 28] [G_Loss 3.643425] [D_Loss 0.204566]\n",
      "train [Epoch 0/1] [Batch 29/1172] [Iter 29] [G_Loss 3.874151] [D_Loss 0.177076]\n",
      "train [Epoch 0/1] [Batch 30/1172] [Iter 30] [G_Loss 3.908235] [D_Loss 0.170808]\n",
      "train [Epoch 0/1] [Batch 31/1172] [Iter 31] [G_Loss 3.502557] [D_Loss 0.126337]\n",
      "train [Epoch 0/1] [Batch 32/1172] [Iter 32] [G_Loss 3.167695] [D_Loss 0.199516]\n",
      "train [Epoch 0/1] [Batch 33/1172] [Iter 33] [G_Loss 4.606936] [D_Loss 0.333940]\n",
      "train [Epoch 0/1] [Batch 34/1172] [Iter 34] [G_Loss 2.107981] [D_Loss 0.369692]\n",
      "train [Epoch 0/1] [Batch 35/1172] [Iter 35] [G_Loss 5.019051] [D_Loss 0.423001]\n",
      "train [Epoch 0/1] [Batch 36/1172] [Iter 36] [G_Loss 2.983041] [D_Loss 0.181871]\n",
      "train [Epoch 0/1] [Batch 37/1172] [Iter 37] [G_Loss 3.010458] [D_Loss 0.249721]\n",
      "train [Epoch 0/1] [Batch 38/1172] [Iter 38] [G_Loss 4.184815] [D_Loss 0.210320]\n",
      "train [Epoch 0/1] [Batch 39/1172] [Iter 39] [G_Loss 3.893400] [D_Loss 0.133134]\n",
      "train [Epoch 0/1] [Batch 40/1172] [Iter 40] [G_Loss 3.571790] [D_Loss 0.142197]\n",
      "train [Epoch 0/1] [Batch 41/1172] [Iter 41] [G_Loss 3.793527] [D_Loss 0.146628]\n",
      "train [Epoch 0/1] [Batch 42/1172] [Iter 42] [G_Loss 3.340211] [D_Loss 0.161319]\n",
      "train [Epoch 0/1] [Batch 43/1172] [Iter 43] [G_Loss 3.291760] [D_Loss 0.126429]\n",
      "train [Epoch 0/1] [Batch 44/1172] [Iter 44] [G_Loss 4.098942] [D_Loss 0.105308]\n",
      "train [Epoch 0/1] [Batch 45/1172] [Iter 45] [G_Loss 3.730048] [D_Loss 0.184023]\n",
      "train [Epoch 0/1] [Batch 46/1172] [Iter 46] [G_Loss 3.015490] [D_Loss 0.158475]\n",
      "train [Epoch 0/1] [Batch 47/1172] [Iter 47] [G_Loss 5.943796] [D_Loss 0.618623]\n",
      "train [Epoch 0/1] [Batch 48/1172] [Iter 48] [G_Loss 0.500228] [D_Loss 3.759826]\n",
      "train [Epoch 0/1] [Batch 49/1172] [Iter 49] [G_Loss 9.636898] [D_Loss 2.525715]\n",
      "train [Epoch 0/1] [Batch 50/1172] [Iter 50] [G_Loss 4.195799] [D_Loss 0.408391]\n",
      "train [Epoch 0/1] [Batch 51/1172] [Iter 51] [G_Loss 1.452562] [D_Loss 0.442746]\n",
      "train [Epoch 0/1] [Batch 52/1172] [Iter 52] [G_Loss 1.583948] [D_Loss 0.397478]\n",
      "train [Epoch 0/1] [Batch 53/1172] [Iter 53] [G_Loss 2.432977] [D_Loss 0.238032]\n",
      "train [Epoch 0/1] [Batch 54/1172] [Iter 54] [G_Loss 2.881207] [D_Loss 0.317523]\n",
      "train [Epoch 0/1] [Batch 55/1172] [Iter 55] [G_Loss 2.951979] [D_Loss 0.258286]\n",
      "train [Epoch 0/1] [Batch 56/1172] [Iter 56] [G_Loss 2.712007] [D_Loss 0.266950]\n",
      "train [Epoch 0/1] [Batch 57/1172] [Iter 57] [G_Loss 2.654711] [D_Loss 0.311365]\n",
      "train [Epoch 0/1] [Batch 58/1172] [Iter 58] [G_Loss 2.986857] [D_Loss 0.279782]\n",
      "train [Epoch 0/1] [Batch 59/1172] [Iter 59] [G_Loss 2.884526] [D_Loss 0.260317]\n",
      "train [Epoch 0/1] [Batch 60/1172] [Iter 60] [G_Loss 2.731525] [D_Loss 0.252166]\n",
      "train [Epoch 0/1] [Batch 61/1172] [Iter 61] [G_Loss 3.018155] [D_Loss 0.298021]\n",
      "train [Epoch 0/1] [Batch 62/1172] [Iter 62] [G_Loss 2.645102] [D_Loss 0.328825]\n",
      "train [Epoch 0/1] [Batch 63/1172] [Iter 63] [G_Loss 2.948883] [D_Loss 0.368129]\n",
      "train [Epoch 0/1] [Batch 64/1172] [Iter 64] [G_Loss 3.077277] [D_Loss 0.284269]\n",
      "train [Epoch 0/1] [Batch 65/1172] [Iter 65] [G_Loss 2.774985] [D_Loss 0.445142]\n",
      "train [Epoch 0/1] [Batch 66/1172] [Iter 66] [G_Loss 2.729777] [D_Loss 0.369682]\n",
      "train [Epoch 0/1] [Batch 67/1172] [Iter 67] [G_Loss 3.257939] [D_Loss 0.352486]\n",
      "train [Epoch 0/1] [Batch 68/1172] [Iter 68] [G_Loss 2.137472] [D_Loss 0.539181]\n",
      "train [Epoch 0/1] [Batch 69/1172] [Iter 69] [G_Loss 5.491587] [D_Loss 0.880436]\n",
      "train [Epoch 0/1] [Batch 70/1172] [Iter 70] [G_Loss 0.910885] [D_Loss 1.645288]\n",
      "train [Epoch 0/1] [Batch 71/1172] [Iter 71] [G_Loss 3.676456] [D_Loss 0.513381]\n",
      "train [Epoch 0/1] [Batch 72/1172] [Iter 72] [G_Loss 4.367104] [D_Loss 1.041448]\n",
      "train [Epoch 0/1] [Batch 73/1172] [Iter 73] [G_Loss 2.754667] [D_Loss 0.604634]\n",
      "train [Epoch 0/1] [Batch 74/1172] [Iter 74] [G_Loss 2.152112] [D_Loss 0.490796]\n",
      "train [Epoch 0/1] [Batch 75/1172] [Iter 75] [G_Loss 3.142982] [D_Loss 0.361841]\n",
      "train [Epoch 0/1] [Batch 76/1172] [Iter 76] [G_Loss 3.859286] [D_Loss 0.343085]\n",
      "train [Epoch 0/1] [Batch 77/1172] [Iter 77] [G_Loss 2.576388] [D_Loss 0.351083]\n",
      "train [Epoch 0/1] [Batch 78/1172] [Iter 78] [G_Loss 3.048281] [D_Loss 0.387410]\n",
      "train [Epoch 0/1] [Batch 79/1172] [Iter 79] [G_Loss 3.096267] [D_Loss 0.375179]\n",
      "train [Epoch 0/1] [Batch 80/1172] [Iter 80] [G_Loss 2.508803] [D_Loss 0.441594]\n",
      "train [Epoch 0/1] [Batch 81/1172] [Iter 81] [G_Loss 3.104078] [D_Loss 0.612990]\n",
      "train [Epoch 0/1] [Batch 82/1172] [Iter 82] [G_Loss 1.673700] [D_Loss 1.223639]\n"
     ]
    }
   ],
   "source": [
    "#Training the sad gan\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_gan import *\n",
    "\n",
    "train_model(generator, discriminator, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.80s)\n",
      "creating index...\n",
      "index created!\n",
      "train [Epoch 0/1] [Batch 0/1172] [Iter 1172] [G_Loss 1.201494] [D_Loss 1.411299]\n",
      "train [Epoch 0/1] [Batch 1/1172] [Iter 1173] [G_Loss 1.207312] [D_Loss 1.333341]\n",
      "train [Epoch 0/1] [Batch 2/1172] [Iter 1174] [G_Loss 1.107260] [D_Loss 1.309551]\n",
      "train [Epoch 0/1] [Batch 3/1172] [Iter 1175] [G_Loss 1.137965] [D_Loss 1.275378]\n",
      "train [Epoch 0/1] [Batch 4/1172] [Iter 1176] [G_Loss 1.193196] [D_Loss 1.383388]\n",
      "train [Epoch 0/1] [Batch 5/1172] [Iter 1177] [G_Loss 1.190458] [D_Loss 1.402689]\n",
      "train [Epoch 0/1] [Batch 6/1172] [Iter 1178] [G_Loss 1.247942] [D_Loss 1.360892]\n",
      "train [Epoch 0/1] [Batch 7/1172] [Iter 1179] [G_Loss 1.182512] [D_Loss 1.333456]\n",
      "train [Epoch 0/1] [Batch 8/1172] [Iter 1180] [G_Loss 1.071972] [D_Loss 1.348914]\n",
      "train [Epoch 0/1] [Batch 9/1172] [Iter 1181] [G_Loss 1.148698] [D_Loss 1.294565]\n",
      "train [Epoch 0/1] [Batch 10/1172] [Iter 1182] [G_Loss 1.152362] [D_Loss 1.310598]\n",
      "train [Epoch 0/1] [Batch 11/1172] [Iter 1183] [G_Loss 1.189002] [D_Loss 1.434941]\n",
      "train [Epoch 0/1] [Batch 12/1172] [Iter 1184] [G_Loss 1.170219] [D_Loss 1.284821]\n",
      "train [Epoch 0/1] [Batch 13/1172] [Iter 1185] [G_Loss 1.166212] [D_Loss 1.343578]\n",
      "train [Epoch 0/1] [Batch 14/1172] [Iter 1186] [G_Loss 1.134976] [D_Loss 1.297070]\n",
      "train [Epoch 0/1] [Batch 15/1172] [Iter 1187] [G_Loss 1.156829] [D_Loss 1.354418]\n",
      "train [Epoch 0/1] [Batch 16/1172] [Iter 1188] [G_Loss 1.179046] [D_Loss 1.319779]\n",
      "train [Epoch 0/1] [Batch 17/1172] [Iter 1189] [G_Loss 1.135549] [D_Loss 1.395945]\n",
      "train [Epoch 0/1] [Batch 18/1172] [Iter 1190] [G_Loss 1.150398] [D_Loss 1.356773]\n",
      "train [Epoch 0/1] [Batch 19/1172] [Iter 1191] [G_Loss 1.121152] [D_Loss 1.330286]\n",
      "train [Epoch 0/1] [Batch 20/1172] [Iter 1192] [G_Loss 1.186566] [D_Loss 1.315350]\n",
      "train [Epoch 0/1] [Batch 21/1172] [Iter 1193] [G_Loss 1.228628] [D_Loss 1.354664]\n",
      "train [Epoch 0/1] [Batch 22/1172] [Iter 1194] [G_Loss 1.183078] [D_Loss 1.376262]\n",
      "train [Epoch 0/1] [Batch 23/1172] [Iter 1195] [G_Loss 1.110574] [D_Loss 1.341850]\n",
      "train [Epoch 0/1] [Batch 24/1172] [Iter 1196] [G_Loss 1.215539] [D_Loss 1.362678]\n",
      "train [Epoch 0/1] [Batch 25/1172] [Iter 1197] [G_Loss 1.137548] [D_Loss 1.301582]\n",
      "train [Epoch 0/1] [Batch 26/1172] [Iter 1198] [G_Loss 1.194642] [D_Loss 1.294379]\n",
      "train [Epoch 0/1] [Batch 27/1172] [Iter 1199] [G_Loss 1.113693] [D_Loss 1.389474]\n",
      "train [Epoch 0/1] [Batch 28/1172] [Iter 1200] [G_Loss 1.163027] [D_Loss 1.364835]\n",
      "train [Epoch 0/1] [Batch 29/1172] [Iter 1201] [G_Loss 1.189607] [D_Loss 1.393879]\n",
      "train [Epoch 0/1] [Batch 30/1172] [Iter 1202] [G_Loss 1.110552] [D_Loss 1.313916]\n",
      "train [Epoch 0/1] [Batch 31/1172] [Iter 1203] [G_Loss 1.166677] [D_Loss 1.320996]\n",
      "train [Epoch 0/1] [Batch 32/1172] [Iter 1204] [G_Loss 1.181154] [D_Loss 1.348884]\n",
      "train [Epoch 0/1] [Batch 33/1172] [Iter 1205] [G_Loss 1.208467] [D_Loss 1.318652]\n",
      "train [Epoch 0/1] [Batch 34/1172] [Iter 1206] [G_Loss 1.155723] [D_Loss 1.280981]\n",
      "train [Epoch 0/1] [Batch 35/1172] [Iter 1207] [G_Loss 1.085046] [D_Loss 1.325760]\n",
      "train [Epoch 0/1] [Batch 36/1172] [Iter 1208] [G_Loss 1.193186] [D_Loss 1.287996]\n",
      "train [Epoch 0/1] [Batch 37/1172] [Iter 1209] [G_Loss 1.237405] [D_Loss 1.364761]\n",
      "train [Epoch 0/1] [Batch 38/1172] [Iter 1210] [G_Loss 1.160238] [D_Loss 1.329365]\n",
      "train [Epoch 0/1] [Batch 39/1172] [Iter 1211] [G_Loss 1.205735] [D_Loss 1.301407]\n",
      "train [Epoch 0/1] [Batch 40/1172] [Iter 1212] [G_Loss 1.178208] [D_Loss 1.388880]\n",
      "train [Epoch 0/1] [Batch 41/1172] [Iter 1213] [G_Loss 1.128929] [D_Loss 1.363358]\n",
      "train [Epoch 0/1] [Batch 42/1172] [Iter 1214] [G_Loss 1.202526] [D_Loss 1.388788]\n",
      "train [Epoch 0/1] [Batch 43/1172] [Iter 1215] [G_Loss 1.112314] [D_Loss 1.332847]\n",
      "train [Epoch 0/1] [Batch 44/1172] [Iter 1216] [G_Loss 1.129616] [D_Loss 1.327067]\n",
      "train [Epoch 0/1] [Batch 45/1172] [Iter 1217] [G_Loss 1.105710] [D_Loss 1.302907]\n",
      "train [Epoch 0/1] [Batch 46/1172] [Iter 1218] [G_Loss 1.215186] [D_Loss 1.301699]\n",
      "train [Epoch 0/1] [Batch 47/1172] [Iter 1219] [G_Loss 1.157741] [D_Loss 1.360267]\n",
      "train [Epoch 0/1] [Batch 48/1172] [Iter 1220] [G_Loss 1.110544] [D_Loss 1.289639]\n",
      "train [Epoch 0/1] [Batch 49/1172] [Iter 1221] [G_Loss 1.251789] [D_Loss 1.352679]\n",
      "train [Epoch 0/1] [Batch 50/1172] [Iter 1222] [G_Loss 1.206930] [D_Loss 1.383897]\n",
      "train [Epoch 0/1] [Batch 51/1172] [Iter 1223] [G_Loss 1.197407] [D_Loss 1.328228]\n",
      "train [Epoch 0/1] [Batch 52/1172] [Iter 1224] [G_Loss 1.118076] [D_Loss 1.336813]\n",
      "train [Epoch 0/1] [Batch 53/1172] [Iter 1225] [G_Loss 1.206026] [D_Loss 1.296831]\n",
      "train [Epoch 0/1] [Batch 54/1172] [Iter 1226] [G_Loss 1.133842] [D_Loss 1.325571]\n",
      "train [Epoch 0/1] [Batch 55/1172] [Iter 1227] [G_Loss 1.198022] [D_Loss 1.316916]\n",
      "train [Epoch 0/1] [Batch 56/1172] [Iter 1228] [G_Loss 1.204722] [D_Loss 1.359449]\n",
      "train [Epoch 0/1] [Batch 57/1172] [Iter 1229] [G_Loss 1.086193] [D_Loss 1.295037]\n",
      "train [Epoch 0/1] [Batch 58/1172] [Iter 1230] [G_Loss 1.120327] [D_Loss 1.301359]\n",
      "train [Epoch 0/1] [Batch 59/1172] [Iter 1231] [G_Loss 1.147188] [D_Loss 1.311401]\n",
      "train [Epoch 0/1] [Batch 60/1172] [Iter 1232] [G_Loss 1.139533] [D_Loss 1.320217]\n",
      "train [Epoch 0/1] [Batch 61/1172] [Iter 1233] [G_Loss 1.133309] [D_Loss 1.297132]\n",
      "train [Epoch 0/1] [Batch 62/1172] [Iter 1234] [G_Loss 1.107112] [D_Loss 1.388115]\n",
      "train [Epoch 0/1] [Batch 63/1172] [Iter 1235] [G_Loss 1.174109] [D_Loss 1.381038]\n",
      "train [Epoch 0/1] [Batch 64/1172] [Iter 1236] [G_Loss 1.199998] [D_Loss 1.399049]\n",
      "train [Epoch 0/1] [Batch 65/1172] [Iter 1237] [G_Loss 1.145762] [D_Loss 1.349316]\n",
      "train [Epoch 0/1] [Batch 66/1172] [Iter 1238] [G_Loss 1.125224] [D_Loss 1.335863]\n",
      "train [Epoch 0/1] [Batch 67/1172] [Iter 1239] [G_Loss 1.196368] [D_Loss 1.358508]\n",
      "train [Epoch 0/1] [Batch 68/1172] [Iter 1240] [G_Loss 1.128598] [D_Loss 1.255195]\n",
      "train [Epoch 0/1] [Batch 69/1172] [Iter 1241] [G_Loss 1.069774] [D_Loss 1.342194]\n",
      "train [Epoch 0/1] [Batch 70/1172] [Iter 1242] [G_Loss 1.202369] [D_Loss 1.348747]\n",
      "train [Epoch 0/1] [Batch 71/1172] [Iter 1243] [G_Loss 1.160593] [D_Loss 1.330871]\n",
      "train [Epoch 0/1] [Batch 72/1172] [Iter 1244] [G_Loss 1.176216] [D_Loss 1.320272]\n",
      "train [Epoch 0/1] [Batch 73/1172] [Iter 1245] [G_Loss 1.281795] [D_Loss 1.319423]\n",
      "train [Epoch 0/1] [Batch 74/1172] [Iter 1246] [G_Loss 1.179445] [D_Loss 1.344761]\n",
      "train [Epoch 0/1] [Batch 75/1172] [Iter 1247] [G_Loss 1.186570] [D_Loss 1.343803]\n",
      "train [Epoch 0/1] [Batch 76/1172] [Iter 1248] [G_Loss 1.125291] [D_Loss 1.353903]\n",
      "train [Epoch 0/1] [Batch 77/1172] [Iter 1249] [G_Loss 1.109687] [D_Loss 1.347053]\n",
      "train [Epoch 0/1] [Batch 78/1172] [Iter 1250] [G_Loss 1.284652] [D_Loss 1.332425]\n",
      "train [Epoch 0/1] [Batch 79/1172] [Iter 1251] [G_Loss 1.099257] [D_Loss 1.360284]\n",
      "train [Epoch 0/1] [Batch 80/1172] [Iter 1252] [G_Loss 1.174608] [D_Loss 1.321691]\n",
      "train [Epoch 0/1] [Batch 81/1172] [Iter 1253] [G_Loss 1.115978] [D_Loss 1.337629]\n",
      "train [Epoch 0/1] [Batch 82/1172] [Iter 1254] [G_Loss 1.198567] [D_Loss 1.311406]\n",
      "train [Epoch 0/1] [Batch 86/1172] [Iter 1258] [G_Loss 1.110142] [D_Loss 1.358327]\n",
      "train [Epoch 0/1] [Batch 87/1172] [Iter 1259] [G_Loss 1.184696] [D_Loss 1.309771]\n",
      "train [Epoch 0/1] [Batch 88/1172] [Iter 1260] [G_Loss 1.206699] [D_Loss 1.353329]\n",
      "train [Epoch 0/1] [Batch 89/1172] [Iter 1261] [G_Loss 1.142690] [D_Loss 1.316524]\n",
      "train [Epoch 0/1] [Batch 90/1172] [Iter 1262] [G_Loss 1.243687] [D_Loss 1.327142]\n",
      "train [Epoch 0/1] [Batch 91/1172] [Iter 1263] [G_Loss 1.118766] [D_Loss 1.312191]\n",
      "train [Epoch 0/1] [Batch 92/1172] [Iter 1264] [G_Loss 1.132610] [D_Loss 1.281072]\n",
      "train [Epoch 0/1] [Batch 93/1172] [Iter 1265] [G_Loss 1.029454] [D_Loss 1.356062]\n",
      "train [Epoch 0/1] [Batch 94/1172] [Iter 1266] [G_Loss 1.150042] [D_Loss 1.288222]\n",
      "train [Epoch 0/1] [Batch 95/1172] [Iter 1267] [G_Loss 1.135659] [D_Loss 1.383845]\n",
      "train [Epoch 0/1] [Batch 96/1172] [Iter 1268] [G_Loss 1.122255] [D_Loss 1.414628]\n",
      "train [Epoch 0/1] [Batch 97/1172] [Iter 1269] [G_Loss 1.141786] [D_Loss 1.360297]\n",
      "train [Epoch 0/1] [Batch 98/1172] [Iter 1270] [G_Loss 1.157539] [D_Loss 1.345837]\n",
      "train [Epoch 0/1] [Batch 99/1172] [Iter 1271] [G_Loss 1.102435] [D_Loss 1.356273]\n",
      "train [Epoch 0/1] [Batch 100/1172] [Iter 1272] [G_Loss 1.174310] [D_Loss 1.349185]\n",
      "train [Epoch 0/1] [Batch 101/1172] [Iter 1273] [G_Loss 1.139976] [D_Loss 1.352605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/1] [Batch 102/1172] [Iter 1274] [G_Loss 1.167247] [D_Loss 1.286676]\n",
      "train [Epoch 0/1] [Batch 103/1172] [Iter 1275] [G_Loss 1.196280] [D_Loss 1.292514]\n",
      "train [Epoch 0/1] [Batch 104/1172] [Iter 1276] [G_Loss 1.112189] [D_Loss 1.335901]\n",
      "train [Epoch 0/1] [Batch 105/1172] [Iter 1277] [G_Loss 1.153679] [D_Loss 1.284129]\n",
      "train [Epoch 0/1] [Batch 106/1172] [Iter 1278] [G_Loss 1.144404] [D_Loss 1.315334]\n",
      "train [Epoch 0/1] [Batch 107/1172] [Iter 1279] [G_Loss 1.172991] [D_Loss 1.319594]\n",
      "train [Epoch 0/1] [Batch 108/1172] [Iter 1280] [G_Loss 1.193559] [D_Loss 1.356135]\n",
      "train [Epoch 0/1] [Batch 109/1172] [Iter 1281] [G_Loss 1.098208] [D_Loss 1.328902]\n",
      "train [Epoch 0/1] [Batch 110/1172] [Iter 1282] [G_Loss 1.217780] [D_Loss 1.279540]\n",
      "train [Epoch 0/1] [Batch 111/1172] [Iter 1283] [G_Loss 1.229866] [D_Loss 1.309379]\n",
      "train [Epoch 0/1] [Batch 112/1172] [Iter 1284] [G_Loss 1.208821] [D_Loss 1.358906]\n",
      "train [Epoch 0/1] [Batch 113/1172] [Iter 1285] [G_Loss 1.133522] [D_Loss 1.302742]\n",
      "train [Epoch 0/1] [Batch 114/1172] [Iter 1286] [G_Loss 1.128276] [D_Loss 1.348547]\n",
      "train [Epoch 0/1] [Batch 115/1172] [Iter 1287] [G_Loss 1.124874] [D_Loss 1.370446]\n",
      "train [Epoch 0/1] [Batch 116/1172] [Iter 1288] [G_Loss 1.135386] [D_Loss 1.353248]\n",
      "train [Epoch 0/1] [Batch 117/1172] [Iter 1289] [G_Loss 1.186042] [D_Loss 1.316031]\n",
      "train [Epoch 0/1] [Batch 118/1172] [Iter 1290] [G_Loss 1.115307] [D_Loss 1.417907]\n",
      "train [Epoch 0/1] [Batch 119/1172] [Iter 1291] [G_Loss 1.182585] [D_Loss 1.334388]\n",
      "train [Epoch 0/1] [Batch 120/1172] [Iter 1292] [G_Loss 1.230247] [D_Loss 1.299733]\n",
      "train [Epoch 0/1] [Batch 121/1172] [Iter 1293] [G_Loss 1.145448] [D_Loss 1.280236]\n",
      "train [Epoch 0/1] [Batch 122/1172] [Iter 1294] [G_Loss 1.156563] [D_Loss 1.315303]\n",
      "train [Epoch 0/1] [Batch 123/1172] [Iter 1295] [G_Loss 1.143880] [D_Loss 1.344869]\n",
      "train [Epoch 0/1] [Batch 124/1172] [Iter 1296] [G_Loss 1.262415] [D_Loss 1.318028]\n",
      "train [Epoch 0/1] [Batch 125/1172] [Iter 1297] [G_Loss 1.240482] [D_Loss 1.322701]\n",
      "train [Epoch 0/1] [Batch 126/1172] [Iter 1298] [G_Loss 1.152876] [D_Loss 1.403980]\n",
      "train [Epoch 0/1] [Batch 127/1172] [Iter 1299] [G_Loss 1.234629] [D_Loss 1.339369]\n",
      "train [Epoch 0/1] [Batch 128/1172] [Iter 1300] [G_Loss 1.050405] [D_Loss 1.358081]\n",
      "train [Epoch 0/1] [Batch 129/1172] [Iter 1301] [G_Loss 1.210122] [D_Loss 1.405952]\n",
      "train [Epoch 0/1] [Batch 130/1172] [Iter 1302] [G_Loss 1.073298] [D_Loss 1.343678]\n",
      "train [Epoch 0/1] [Batch 131/1172] [Iter 1303] [G_Loss 1.128374] [D_Loss 1.374749]\n",
      "train [Epoch 0/1] [Batch 132/1172] [Iter 1304] [G_Loss 1.168800] [D_Loss 1.290624]\n",
      "train [Epoch 0/1] [Batch 133/1172] [Iter 1305] [G_Loss 1.102363] [D_Loss 1.278628]\n",
      "train [Epoch 0/1] [Batch 134/1172] [Iter 1306] [G_Loss 1.315200] [D_Loss 1.337866]\n",
      "train [Epoch 0/1] [Batch 135/1172] [Iter 1307] [G_Loss 1.331218] [D_Loss 1.370350]\n",
      "train [Epoch 0/1] [Batch 136/1172] [Iter 1308] [G_Loss 1.226552] [D_Loss 1.356929]\n",
      "train [Epoch 0/1] [Batch 137/1172] [Iter 1309] [G_Loss 1.108020] [D_Loss 1.340784]\n",
      "train [Epoch 0/1] [Batch 138/1172] [Iter 1310] [G_Loss 1.170313] [D_Loss 1.377293]\n",
      "train [Epoch 0/1] [Batch 139/1172] [Iter 1311] [G_Loss 1.173226] [D_Loss 1.369496]\n",
      "train [Epoch 0/1] [Batch 140/1172] [Iter 1312] [G_Loss 1.206854] [D_Loss 1.400818]\n",
      "train [Epoch 0/1] [Batch 141/1172] [Iter 1313] [G_Loss 1.139222] [D_Loss 1.388153]\n",
      "train [Epoch 0/1] [Batch 142/1172] [Iter 1314] [G_Loss 1.211123] [D_Loss 1.339000]\n",
      "train [Epoch 0/1] [Batch 143/1172] [Iter 1315] [G_Loss 1.143212] [D_Loss 1.341039]\n",
      "train [Epoch 0/1] [Batch 144/1172] [Iter 1316] [G_Loss 1.235947] [D_Loss 1.307865]\n",
      "train [Epoch 0/1] [Batch 145/1172] [Iter 1317] [G_Loss 1.201000] [D_Loss 1.302099]\n",
      "train [Epoch 0/1] [Batch 146/1172] [Iter 1318] [G_Loss 1.246256] [D_Loss 1.321390]\n",
      "train [Epoch 0/1] [Batch 147/1172] [Iter 1319] [G_Loss 1.123100] [D_Loss 1.300115]\n",
      "train [Epoch 0/1] [Batch 148/1172] [Iter 1320] [G_Loss 1.201770] [D_Loss 1.325356]\n",
      "train [Epoch 0/1] [Batch 149/1172] [Iter 1321] [G_Loss 1.236171] [D_Loss 1.330164]\n",
      "train [Epoch 0/1] [Batch 150/1172] [Iter 1322] [G_Loss 1.155505] [D_Loss 1.312172]\n",
      "train [Epoch 0/1] [Batch 151/1172] [Iter 1323] [G_Loss 1.109698] [D_Loss 1.318955]\n",
      "train [Epoch 0/1] [Batch 152/1172] [Iter 1324] [G_Loss 1.110607] [D_Loss 1.388900]\n",
      "train [Epoch 0/1] [Batch 153/1172] [Iter 1325] [G_Loss 1.228743] [D_Loss 1.283165]\n",
      "train [Epoch 0/1] [Batch 154/1172] [Iter 1326] [G_Loss 1.167622] [D_Loss 1.359683]\n",
      "train [Epoch 0/1] [Batch 155/1172] [Iter 1327] [G_Loss 1.132001] [D_Loss 1.347466]\n",
      "train [Epoch 0/1] [Batch 156/1172] [Iter 1328] [G_Loss 1.171035] [D_Loss 1.357294]\n",
      "train [Epoch 0/1] [Batch 157/1172] [Iter 1329] [G_Loss 1.078406] [D_Loss 1.367385]\n",
      "train [Epoch 0/1] [Batch 158/1172] [Iter 1330] [G_Loss 1.151299] [D_Loss 1.234641]\n",
      "train [Epoch 0/1] [Batch 159/1172] [Iter 1331] [G_Loss 1.179230] [D_Loss 1.363076]\n",
      "train [Epoch 0/1] [Batch 160/1172] [Iter 1332] [G_Loss 1.242321] [D_Loss 1.364976]\n",
      "train [Epoch 0/1] [Batch 161/1172] [Iter 1333] [G_Loss 1.194638] [D_Loss 1.373740]\n",
      "train [Epoch 0/1] [Batch 162/1172] [Iter 1334] [G_Loss 1.102153] [D_Loss 1.378323]\n",
      "train [Epoch 0/1] [Batch 163/1172] [Iter 1335] [G_Loss 1.171780] [D_Loss 1.351989]\n",
      "train [Epoch 0/1] [Batch 164/1172] [Iter 1336] [G_Loss 1.196372] [D_Loss 1.313079]\n",
      "train [Epoch 0/1] [Batch 165/1172] [Iter 1337] [G_Loss 1.162488] [D_Loss 1.330082]\n",
      "train [Epoch 0/1] [Batch 166/1172] [Iter 1338] [G_Loss 1.211481] [D_Loss 1.333846]\n",
      "train [Epoch 0/1] [Batch 167/1172] [Iter 1339] [G_Loss 1.159123] [D_Loss 1.344950]\n",
      "train [Epoch 0/1] [Batch 168/1172] [Iter 1340] [G_Loss 1.094600] [D_Loss 1.361725]\n",
      "train [Epoch 0/1] [Batch 169/1172] [Iter 1341] [G_Loss 1.179960] [D_Loss 1.363996]\n",
      "train [Epoch 0/1] [Batch 170/1172] [Iter 1342] [G_Loss 1.133219] [D_Loss 1.325597]\n",
      "train [Epoch 0/1] [Batch 171/1172] [Iter 1343] [G_Loss 1.185598] [D_Loss 1.365903]\n",
      "train [Epoch 0/1] [Batch 172/1172] [Iter 1344] [G_Loss 1.065822] [D_Loss 1.373227]\n",
      "train [Epoch 0/1] [Batch 173/1172] [Iter 1345] [G_Loss 1.125695] [D_Loss 1.389117]\n",
      "train [Epoch 0/1] [Batch 174/1172] [Iter 1346] [G_Loss 1.120557] [D_Loss 1.347103]\n",
      "train [Epoch 0/1] [Batch 175/1172] [Iter 1347] [G_Loss 1.192233] [D_Loss 1.318097]\n",
      "train [Epoch 0/1] [Batch 176/1172] [Iter 1348] [G_Loss 1.222021] [D_Loss 1.387730]\n",
      "train [Epoch 0/1] [Batch 177/1172] [Iter 1349] [G_Loss 1.179187] [D_Loss 1.332440]\n",
      "train [Epoch 0/1] [Batch 178/1172] [Iter 1350] [G_Loss 1.183067] [D_Loss 1.350789]\n",
      "train [Epoch 0/1] [Batch 179/1172] [Iter 1351] [G_Loss 1.140714] [D_Loss 1.313545]\n",
      "train [Epoch 0/1] [Batch 180/1172] [Iter 1352] [G_Loss 1.149697] [D_Loss 1.351970]\n",
      "train [Epoch 0/1] [Batch 181/1172] [Iter 1353] [G_Loss 1.214980] [D_Loss 1.319005]\n",
      "train [Epoch 0/1] [Batch 182/1172] [Iter 1354] [G_Loss 1.149505] [D_Loss 1.317336]\n",
      "train [Epoch 0/1] [Batch 183/1172] [Iter 1355] [G_Loss 1.129222] [D_Loss 1.325772]\n",
      "train [Epoch 0/1] [Batch 184/1172] [Iter 1356] [G_Loss 1.156775] [D_Loss 1.269865]\n",
      "train [Epoch 0/1] [Batch 185/1172] [Iter 1357] [G_Loss 1.190593] [D_Loss 1.357807]\n",
      "train [Epoch 0/1] [Batch 186/1172] [Iter 1358] [G_Loss 1.073186] [D_Loss 1.332579]\n",
      "train [Epoch 0/1] [Batch 187/1172] [Iter 1359] [G_Loss 1.149446] [D_Loss 1.280742]\n",
      "train [Epoch 0/1] [Batch 188/1172] [Iter 1360] [G_Loss 1.127077] [D_Loss 1.361421]\n",
      "train [Epoch 0/1] [Batch 189/1172] [Iter 1361] [G_Loss 1.106321] [D_Loss 1.359459]\n",
      "train [Epoch 0/1] [Batch 190/1172] [Iter 1362] [G_Loss 1.089644] [D_Loss 1.390053]\n",
      "train [Epoch 0/1] [Batch 191/1172] [Iter 1363] [G_Loss 1.105737] [D_Loss 1.365940]\n",
      "train [Epoch 0/1] [Batch 192/1172] [Iter 1364] [G_Loss 1.245203] [D_Loss 1.318569]\n",
      "train [Epoch 0/1] [Batch 193/1172] [Iter 1365] [G_Loss 1.182664] [D_Loss 1.335492]\n",
      "train [Epoch 0/1] [Batch 194/1172] [Iter 1366] [G_Loss 1.187777] [D_Loss 1.315740]\n",
      "train [Epoch 0/1] [Batch 195/1172] [Iter 1367] [G_Loss 1.168484] [D_Loss 1.360627]\n",
      "train [Epoch 0/1] [Batch 196/1172] [Iter 1368] [G_Loss 1.099308] [D_Loss 1.358273]\n",
      "train [Epoch 0/1] [Batch 197/1172] [Iter 1369] [G_Loss 1.235710] [D_Loss 1.305173]\n",
      "train [Epoch 0/1] [Batch 198/1172] [Iter 1370] [G_Loss 1.213278] [D_Loss 1.331839]\n",
      "train [Epoch 0/1] [Batch 199/1172] [Iter 1371] [G_Loss 1.163246] [D_Loss 1.306303]\n",
      "train [Epoch 0/1] [Batch 200/1172] [Iter 1372] [G_Loss 1.145151] [D_Loss 1.348934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/1] [Batch 201/1172] [Iter 1373] [G_Loss 1.155777] [D_Loss 1.352304]\n",
      "train [Epoch 0/1] [Batch 202/1172] [Iter 1374] [G_Loss 1.166341] [D_Loss 1.374236]\n",
      "train [Epoch 0/1] [Batch 203/1172] [Iter 1375] [G_Loss 1.112269] [D_Loss 1.303325]\n",
      "train [Epoch 0/1] [Batch 204/1172] [Iter 1376] [G_Loss 1.177415] [D_Loss 1.298182]\n",
      "train [Epoch 0/1] [Batch 205/1172] [Iter 1377] [G_Loss 1.142467] [D_Loss 1.392092]\n",
      "train [Epoch 0/1] [Batch 206/1172] [Iter 1378] [G_Loss 1.250828] [D_Loss 1.332336]\n",
      "train [Epoch 0/1] [Batch 207/1172] [Iter 1379] [G_Loss 1.207902] [D_Loss 1.281610]\n",
      "train [Epoch 0/1] [Batch 208/1172] [Iter 1380] [G_Loss 1.168185] [D_Loss 1.309127]\n",
      "train [Epoch 0/1] [Batch 209/1172] [Iter 1381] [G_Loss 1.159319] [D_Loss 1.346452]\n",
      "train [Epoch 0/1] [Batch 210/1172] [Iter 1382] [G_Loss 1.112962] [D_Loss 1.337546]\n",
      "train [Epoch 0/1] [Batch 211/1172] [Iter 1383] [G_Loss 1.197924] [D_Loss 1.380982]\n",
      "train [Epoch 0/1] [Batch 212/1172] [Iter 1384] [G_Loss 1.113899] [D_Loss 1.379261]\n",
      "train [Epoch 0/1] [Batch 213/1172] [Iter 1385] [G_Loss 1.173568] [D_Loss 1.373230]\n",
      "train [Epoch 0/1] [Batch 214/1172] [Iter 1386] [G_Loss 1.178433] [D_Loss 1.333346]\n",
      "train [Epoch 0/1] [Batch 215/1172] [Iter 1387] [G_Loss 1.146283] [D_Loss 1.317852]\n",
      "train [Epoch 0/1] [Batch 216/1172] [Iter 1388] [G_Loss 1.152046] [D_Loss 1.349338]\n",
      "train [Epoch 0/1] [Batch 217/1172] [Iter 1389] [G_Loss 1.181671] [D_Loss 1.321569]\n",
      "train [Epoch 0/1] [Batch 218/1172] [Iter 1390] [G_Loss 1.164125] [D_Loss 1.430836]\n",
      "train [Epoch 0/1] [Batch 219/1172] [Iter 1391] [G_Loss 1.209086] [D_Loss 1.306929]\n",
      "train [Epoch 0/1] [Batch 220/1172] [Iter 1392] [G_Loss 1.126080] [D_Loss 1.278006]\n",
      "train [Epoch 0/1] [Batch 221/1172] [Iter 1393] [G_Loss 1.156206] [D_Loss 1.340165]\n",
      "train [Epoch 0/1] [Batch 222/1172] [Iter 1394] [G_Loss 1.142870] [D_Loss 1.301331]\n",
      "train [Epoch 0/1] [Batch 223/1172] [Iter 1395] [G_Loss 1.151273] [D_Loss 1.312404]\n",
      "train [Epoch 0/1] [Batch 224/1172] [Iter 1396] [G_Loss 1.137033] [D_Loss 1.301955]\n",
      "train [Epoch 0/1] [Batch 225/1172] [Iter 1397] [G_Loss 1.156501] [D_Loss 1.281000]\n",
      "train [Epoch 0/1] [Batch 226/1172] [Iter 1398] [G_Loss 1.215406] [D_Loss 1.336983]\n",
      "train [Epoch 0/1] [Batch 227/1172] [Iter 1399] [G_Loss 1.144893] [D_Loss 1.276319]\n",
      "train [Epoch 0/1] [Batch 228/1172] [Iter 1400] [G_Loss 1.078595] [D_Loss 1.321861]\n",
      "train [Epoch 0/1] [Batch 229/1172] [Iter 1401] [G_Loss 1.116138] [D_Loss 1.265851]\n",
      "train [Epoch 0/1] [Batch 230/1172] [Iter 1402] [G_Loss 1.167896] [D_Loss 1.295102]\n",
      "train [Epoch 0/1] [Batch 231/1172] [Iter 1403] [G_Loss 1.154301] [D_Loss 1.296780]\n",
      "train [Epoch 0/1] [Batch 232/1172] [Iter 1404] [G_Loss 1.065158] [D_Loss 1.327537]\n",
      "train [Epoch 0/1] [Batch 233/1172] [Iter 1405] [G_Loss 1.130926] [D_Loss 1.373204]\n",
      "train [Epoch 0/1] [Batch 234/1172] [Iter 1406] [G_Loss 1.208104] [D_Loss 1.311290]\n",
      "train [Epoch 0/1] [Batch 235/1172] [Iter 1407] [G_Loss 1.242087] [D_Loss 1.323543]\n",
      "train [Epoch 0/1] [Batch 236/1172] [Iter 1408] [G_Loss 1.126943] [D_Loss 1.287897]\n",
      "train [Epoch 0/1] [Batch 237/1172] [Iter 1409] [G_Loss 1.137889] [D_Loss 1.382414]\n",
      "train [Epoch 0/1] [Batch 238/1172] [Iter 1410] [G_Loss 1.163089] [D_Loss 1.314409]\n",
      "train [Epoch 0/1] [Batch 239/1172] [Iter 1411] [G_Loss 1.011907] [D_Loss 1.412128]\n",
      "train [Epoch 0/1] [Batch 240/1172] [Iter 1412] [G_Loss 1.117332] [D_Loss 1.264299]\n",
      "train [Epoch 0/1] [Batch 241/1172] [Iter 1413] [G_Loss 1.144518] [D_Loss 1.327753]\n",
      "train [Epoch 0/1] [Batch 242/1172] [Iter 1414] [G_Loss 1.120286] [D_Loss 1.365135]\n",
      "train [Epoch 0/1] [Batch 243/1172] [Iter 1415] [G_Loss 1.115882] [D_Loss 1.307113]\n",
      "train [Epoch 0/1] [Batch 244/1172] [Iter 1416] [G_Loss 1.180808] [D_Loss 1.318892]\n",
      "train [Epoch 0/1] [Batch 245/1172] [Iter 1417] [G_Loss 1.098587] [D_Loss 1.334905]\n",
      "train [Epoch 0/1] [Batch 246/1172] [Iter 1418] [G_Loss 1.152892] [D_Loss 1.404655]\n",
      "train [Epoch 0/1] [Batch 247/1172] [Iter 1419] [G_Loss 1.064088] [D_Loss 1.382432]\n",
      "train [Epoch 0/1] [Batch 248/1172] [Iter 1420] [G_Loss 1.181968] [D_Loss 1.313747]\n",
      "train [Epoch 0/1] [Batch 249/1172] [Iter 1421] [G_Loss 1.166448] [D_Loss 1.317111]\n",
      "train [Epoch 0/1] [Batch 250/1172] [Iter 1422] [G_Loss 1.166169] [D_Loss 1.269575]\n",
      "train [Epoch 0/1] [Batch 251/1172] [Iter 1423] [G_Loss 1.107493] [D_Loss 1.348726]\n",
      "train [Epoch 0/1] [Batch 252/1172] [Iter 1424] [G_Loss 1.049807] [D_Loss 1.298646]\n",
      "train [Epoch 0/1] [Batch 253/1172] [Iter 1425] [G_Loss 1.260897] [D_Loss 1.346266]\n",
      "train [Epoch 0/1] [Batch 254/1172] [Iter 1426] [G_Loss 1.203143] [D_Loss 1.353769]\n",
      "train [Epoch 0/1] [Batch 255/1172] [Iter 1427] [G_Loss 1.138334] [D_Loss 1.326293]\n",
      "train [Epoch 0/1] [Batch 256/1172] [Iter 1428] [G_Loss 1.133887] [D_Loss 1.355135]\n",
      "train [Epoch 0/1] [Batch 257/1172] [Iter 1429] [G_Loss 1.115391] [D_Loss 1.397589]\n",
      "train [Epoch 0/1] [Batch 258/1172] [Iter 1430] [G_Loss 1.244076] [D_Loss 1.317419]\n",
      "train [Epoch 0/1] [Batch 259/1172] [Iter 1431] [G_Loss 1.230028] [D_Loss 1.348581]\n",
      "train [Epoch 0/1] [Batch 260/1172] [Iter 1432] [G_Loss 1.158962] [D_Loss 1.333734]\n",
      "train [Epoch 0/1] [Batch 261/1172] [Iter 1433] [G_Loss 1.273900] [D_Loss 1.348621]\n",
      "train [Epoch 0/1] [Batch 262/1172] [Iter 1434] [G_Loss 1.180897] [D_Loss 1.390059]\n",
      "train [Epoch 0/1] [Batch 263/1172] [Iter 1435] [G_Loss 1.267852] [D_Loss 1.341724]\n",
      "train [Epoch 0/1] [Batch 264/1172] [Iter 1436] [G_Loss 1.188441] [D_Loss 1.324212]\n",
      "train [Epoch 0/1] [Batch 265/1172] [Iter 1437] [G_Loss 1.171409] [D_Loss 1.362771]\n",
      "train [Epoch 0/1] [Batch 266/1172] [Iter 1438] [G_Loss 1.171597] [D_Loss 1.400091]\n",
      "train [Epoch 0/1] [Batch 267/1172] [Iter 1439] [G_Loss 1.105513] [D_Loss 1.316386]\n",
      "train [Epoch 0/1] [Batch 268/1172] [Iter 1440] [G_Loss 1.226186] [D_Loss 1.360603]\n",
      "train [Epoch 0/1] [Batch 269/1172] [Iter 1441] [G_Loss 1.139328] [D_Loss 1.306208]\n",
      "train [Epoch 0/1] [Batch 270/1172] [Iter 1442] [G_Loss 1.256940] [D_Loss 1.386194]\n",
      "train [Epoch 0/1] [Batch 271/1172] [Iter 1443] [G_Loss 1.188667] [D_Loss 1.305948]\n",
      "train [Epoch 0/1] [Batch 272/1172] [Iter 1444] [G_Loss 1.149606] [D_Loss 1.283897]\n",
      "train [Epoch 0/1] [Batch 273/1172] [Iter 1445] [G_Loss 1.190834] [D_Loss 1.316762]\n",
      "train [Epoch 0/1] [Batch 274/1172] [Iter 1446] [G_Loss 1.183582] [D_Loss 1.309686]\n",
      "train [Epoch 0/1] [Batch 275/1172] [Iter 1447] [G_Loss 1.207050] [D_Loss 1.319014]\n",
      "train [Epoch 0/1] [Batch 276/1172] [Iter 1448] [G_Loss 1.159344] [D_Loss 1.285000]\n",
      "train [Epoch 0/1] [Batch 277/1172] [Iter 1449] [G_Loss 1.174380] [D_Loss 1.337545]\n",
      "train [Epoch 0/1] [Batch 278/1172] [Iter 1450] [G_Loss 1.163823] [D_Loss 1.412937]\n",
      "train [Epoch 0/1] [Batch 279/1172] [Iter 1451] [G_Loss 1.136790] [D_Loss 1.365145]\n",
      "train [Epoch 0/1] [Batch 280/1172] [Iter 1452] [G_Loss 1.176120] [D_Loss 1.386653]\n",
      "train [Epoch 0/1] [Batch 281/1172] [Iter 1453] [G_Loss 1.102168] [D_Loss 1.377342]\n",
      "train [Epoch 0/1] [Batch 282/1172] [Iter 1454] [G_Loss 1.212881] [D_Loss 1.297435]\n",
      "train [Epoch 0/1] [Batch 283/1172] [Iter 1455] [G_Loss 1.132284] [D_Loss 1.291392]\n",
      "train [Epoch 0/1] [Batch 284/1172] [Iter 1456] [G_Loss 1.188496] [D_Loss 1.330553]\n",
      "train [Epoch 0/1] [Batch 285/1172] [Iter 1457] [G_Loss 1.194709] [D_Loss 1.354837]\n",
      "train [Epoch 0/1] [Batch 286/1172] [Iter 1458] [G_Loss 1.117896] [D_Loss 1.326092]\n",
      "train [Epoch 0/1] [Batch 287/1172] [Iter 1459] [G_Loss 1.150307] [D_Loss 1.387896]\n",
      "train [Epoch 0/1] [Batch 288/1172] [Iter 1460] [G_Loss 1.162511] [D_Loss 1.321853]\n",
      "train [Epoch 0/1] [Batch 289/1172] [Iter 1461] [G_Loss 1.247313] [D_Loss 1.273317]\n",
      "train [Epoch 0/1] [Batch 290/1172] [Iter 1462] [G_Loss 1.183754] [D_Loss 1.255300]\n",
      "train [Epoch 0/1] [Batch 291/1172] [Iter 1463] [G_Loss 1.119079] [D_Loss 1.310730]\n",
      "train [Epoch 0/1] [Batch 292/1172] [Iter 1464] [G_Loss 1.248748] [D_Loss 1.330048]\n",
      "train [Epoch 0/1] [Batch 293/1172] [Iter 1465] [G_Loss 1.193457] [D_Loss 1.316757]\n",
      "train [Epoch 0/1] [Batch 294/1172] [Iter 1466] [G_Loss 1.171876] [D_Loss 1.317295]\n",
      "train [Epoch 0/1] [Batch 295/1172] [Iter 1467] [G_Loss 1.130484] [D_Loss 1.377372]\n",
      "train [Epoch 0/1] [Batch 296/1172] [Iter 1468] [G_Loss 1.127462] [D_Loss 1.300822]\n",
      "train [Epoch 0/1] [Batch 297/1172] [Iter 1469] [G_Loss 1.136692] [D_Loss 1.307708]\n",
      "train [Epoch 0/1] [Batch 298/1172] [Iter 1470] [G_Loss 1.163537] [D_Loss 1.294827]\n",
      "train [Epoch 0/1] [Batch 299/1172] [Iter 1471] [G_Loss 1.139462] [D_Loss 1.280119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [Epoch 0/1] [Batch 300/1172] [Iter 1472] [G_Loss 1.121453] [D_Loss 1.300598]\n",
      "train [Epoch 0/1] [Batch 301/1172] [Iter 1473] [G_Loss 1.129806] [D_Loss 1.365225]\n",
      "train [Epoch 0/1] [Batch 302/1172] [Iter 1474] [G_Loss 1.034476] [D_Loss 1.364886]\n",
      "train [Epoch 0/1] [Batch 303/1172] [Iter 1475] [G_Loss 1.217722] [D_Loss 1.366216]\n",
      "train [Epoch 0/1] [Batch 304/1172] [Iter 1476] [G_Loss 1.149333] [D_Loss 1.334410]\n",
      "train [Epoch 0/1] [Batch 305/1172] [Iter 1477] [G_Loss 1.138937] [D_Loss 1.373381]\n",
      "train [Epoch 0/1] [Batch 306/1172] [Iter 1478] [G_Loss 1.166403] [D_Loss 1.350775]\n",
      "train [Epoch 0/1] [Batch 307/1172] [Iter 1479] [G_Loss 1.124724] [D_Loss 1.315841]\n",
      "train [Epoch 0/1] [Batch 308/1172] [Iter 1480] [G_Loss 1.210159] [D_Loss 1.301386]\n",
      "train [Epoch 0/1] [Batch 309/1172] [Iter 1481] [G_Loss 1.181960] [D_Loss 1.358202]\n",
      "train [Epoch 0/1] [Batch 310/1172] [Iter 1482] [G_Loss 1.152872] [D_Loss 1.382135]\n",
      "train [Epoch 0/1] [Batch 311/1172] [Iter 1483] [G_Loss 1.278432] [D_Loss 1.418485]\n",
      "train [Epoch 0/1] [Batch 312/1172] [Iter 1484] [G_Loss 1.122777] [D_Loss 1.280993]\n",
      "train [Epoch 0/1] [Batch 313/1172] [Iter 1485] [G_Loss 1.194508] [D_Loss 1.364099]\n",
      "train [Epoch 0/1] [Batch 314/1172] [Iter 1486] [G_Loss 1.206309] [D_Loss 1.269459]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_gan import *\n",
    "\n",
    "gen = torch.load(\"../../outputs/gan_1/generator.pt\")\n",
    "dis = torch.load(\"../../outputs/gan_1/discriminator.pt\")\n",
    "gen.eval()\n",
    "dis.eval()\n",
    "\n",
    "train_model(gen, dis, True, it=1172)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the sad gan\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_unet import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
